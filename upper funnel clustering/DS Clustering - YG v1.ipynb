{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "573c76bf-f2e3-4946-a6ca-0c2a5c9596fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks permissions diagnostic script (Attach you own cluster instead of serverless for this)\n",
    "\n",
    "# Check current user\n",
    "print(\"Current User:\")\n",
    "print(spark.sql(\"SELECT current_user()\").collect()[0][0])\n",
    "\n",
    "# List available writable locations\n",
    "#print(\"\\nPossible writable locations:\")\n",
    "#print(\"1. /tmp directory:\")\n",
    "#display(dbutils.fs.ls(\"/tmp\"))\n",
    "\n",
    "# print(\"\\n2. Your home directory:\")\n",
    "# home_dir = f\"/user/{spark.sql('SELECT current_user()').collect()[0][0]}\"\n",
    "# display(dbutils.fs.ls(home_dir))\n",
    "\n",
    "# Attempt to write to a temporary location\n",
    "try:\n",
    "    test_path = \"/tmp/permissions_test.txt\"\n",
    "    dbutils.fs.put(test_path, \"Permissions test\", overwrite=True)\n",
    "    print(f\"\\nSuccessfully wrote to {test_path}\")\n",
    "    \n",
    "    # Read back the file\n",
    "    print(\"File contents:\")\n",
    "    print(dbutils.fs.head(test_path))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nFailed to write to temporary location: {e}\")\n",
    "\n",
    "# Additional diagnostic information\n",
    "print(\"\\nCurrent Spark Configuration:\")\n",
    "for key, value in spark.conf.getAll.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "9146e26c-749a-4ab6-8762-eaf20af4ae31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# CELL 2: Enable Databricks disk caching (Attach you own cluster instead of serverless for this)\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import sys\n",
    "\n",
    "# Get the current Spark session\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Try to enable Databricks disk caching with error handling\n",
    "try:\n",
    "    # Attempt to enable disk caching\n",
    "    spark.conf.set(\"spark.databricks.io.cache.enabled\", \"true\")\n",
    "    spark.conf.set(\"spark.databricks.io.cache.compression.enabled\", \"true\")\n",
    "    \n",
    "    # Verify if setting was accepted\n",
    "    cache_enabled = spark.conf.get(\"spark.databricks.io.cache.enabled\")\n",
    "    print(f\"✅ Databricks disk cache successfully enabled: {cache_enabled}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Unable to enable Databricks disk cache: {str(e)}\")\n",
    "    print(\"The pipeline will continue without disk caching\")\n",
    "    \n",
    "    # Check runtime type to provide better guidance\n",
    "    if \"spark.databricks.compute\" in [conf.key for conf in spark.sparkContext.getConf().getAll()]:\n",
    "        compute_type = spark.sparkContext.getConf().get(\"spark.databricks.compute\", \"unknown\")\n",
    "        if \"serverless\" in compute_type.lower():\n",
    "            print(\"NOTE: You appear to be using Serverless compute which has limited configuration options.\")\n",
    "            print(\"Consider switching to a Standard All-Purpose cluster if you need disk caching.\")\n",
    "\n",
    "print(\"\\n=== Continuing with pipeline execution ===\")\n",
    "\n",
    "# The rest of your code continues here\n",
    "# You can now proceed with your data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "a3547394-cebc-4a53-8ef4-2a697c467c8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#  Cell 3: Helper Functions\n",
    "def log_status(message, print_timestamp=True):\n",
    "    \"\"\"Helper function to print status updates with consistent formatting and display in Databricks\"\"\"\n",
    "    timestamp = f\"[{time.strftime('%Y-%m-%d %H:%M:%S')}] \" if print_timestamp else \"\"\n",
    "    formatted_message = f\"STATUS: {timestamp}{message}\"\n",
    "    \n",
    "    # Print to standard output\n",
    "    print(formatted_message)\n",
    "    \n",
    "    # If in Databricks, also display with HTML formatting for better visibility\n",
    "    if IN_DATABRICKS:\n",
    "        try:\n",
    "            if message.startswith(\"ERROR\") or message.startswith(\"CRITICAL ERROR\"):\n",
    "                display(HTML(f\"<div style='color:red; font-weight:bold;'>{formatted_message}</div>\"))\n",
    "            elif message.startswith(\"WARNING\"):\n",
    "                display(HTML(f\"<div style='color:orange; font-weight:bold;'>{formatted_message}</div>\"))\n",
    "            elif message.startswith(\"===== \"):\n",
    "                display(HTML(f\"<div style='color:blue; font-weight:bold; font-size:1.1em;'>{formatted_message}</div>\"))\n",
    "            else:\n",
    "                display(HTML(f\"<div>{formatted_message}</div>\"))\n",
    "        except:\n",
    "            # Fallback is the standard print already done above\n",
    "            pass\n",
    "\n",
    "print(\"\\n=== CELL: Helper Functions completed ===\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "94bcfda2-4b6d-41a1-8c27-924feca50071",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# CELL 4: Enhanced Utility Functions\n",
    "# Position: After Original Cell 4 (Visualization Functions)\n",
    "# Purpose: Adds improved logging, configuration, and resource management utilities\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "import traceback\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import functools\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Define IN_DATABRICKS variable\n",
    "IN_DATABRICKS = True\n",
    "\n",
    "# Configuration management for better parameter organization\n",
    "class PipelineConfig:\n",
    "    def __init__(self, sample_size=10000, output_prefix=\"/dbfs/FileStore/acxiom_clustering/vehicle_segmentation\",\n",
    "                clustering_methods=None, max_clusters=15):\n",
    "        self.sample_size = sample_size\n",
    "        self.output_prefix = output_prefix\n",
    "        self.clustering_methods = clustering_methods or [\"kmeans\"]\n",
    "        self.max_clusters = max_clusters\n",
    "        self.debug_mode = False\n",
    "        \n",
    "    def enable_debug(self):\n",
    "        \"\"\"Enable debug mode for more verbose logging\"\"\"\n",
    "        self.debug_mode = True\n",
    "        return self\n",
    "        \n",
    "    def to_dict(self):\n",
    "        \"\"\"Convert config to dictionary for logging\"\"\"\n",
    "        return {\n",
    "            \"sample_size\": self.sample_size,\n",
    "            \"output_prefix\": self.output_prefix,\n",
    "            \"clustering_methods\": self.clustering_methods,\n",
    "            \"max_clusters\": self.max_clusters,\n",
    "            \"debug_mode\": self.debug_mode\n",
    "        }\n",
    "\n",
    "def log_status(message, print_timestamp=True):\n",
    "    \"\"\"Helper function to print status updates with consistent formatting and display in Databricks\"\"\"\n",
    "    timestamp = f\"[{time.strftime('%Y-%m-%d %H:%M:%S')}] \" if print_timestamp else \"\"\n",
    "    formatted_message = f\"STATUS: {timestamp}{message}\"\n",
    "    \n",
    "    # Print to standard output\n",
    "    print(formatted_message)\n",
    "    \n",
    "    # If in Databricks, also display with HTML formatting for better visibility\n",
    "    if IN_DATABRICKS:\n",
    "        try:\n",
    "            if message.startswith(\"ERROR\") or message.startswith(\"CRITICAL ERROR\"):\n",
    "                display(HTML(f\"<div style='color:red; font-weight:bold;'>{formatted_message}</div>\"))\n",
    "            elif message.startswith(\"WARNING\"):\n",
    "                display(HTML(f\"<div style='color:orange; font-weight:bold;'>{formatted_message}</div>\"))\n",
    "            elif message.startswith(\"===== \"):\n",
    "                display(HTML(f\"<div style='color:blue; font-weight:bold; font-size:1.1em;'>{formatted_message}</div>\"))\n",
    "            else:\n",
    "                display(HTML(f\"<div>{formatted_message}</div>\"))\n",
    "        except:\n",
    "            # Fallback is the standard print already done above\n",
    "            pass\n",
    "\n",
    "def log_progress(step, total_steps, message):\n",
    "    \"\"\"\n",
    "    Log progress with a visual progress bar\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    step : int\n",
    "        Current step number\n",
    "    total_steps : int\n",
    "        Total number of steps\n",
    "    message : str\n",
    "        Message to display with progress\n",
    "    \"\"\"\n",
    "    percentage = (step / total_steps) * 100\n",
    "    progress_bar = \"▓\" * int(percentage // 5) + \"░\" * (20 - int(percentage // 5))\n",
    "    log_status(f\"[{progress_bar}] ({percentage:.1f}%) {message}\")\n",
    "\n",
    "def release_resources(spark_df=None, pandas_dfs=None):\n",
    "    \"\"\"\n",
    "    Release memory resources to prevent OOM errors\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    spark_df : SparkDataFrame or None\n",
    "        Spark DataFrame to unpersist\n",
    "    pandas_dfs : list or None\n",
    "        List of pandas DataFrames to delete\n",
    "    \"\"\"\n",
    "    # Release Spark DataFrame\n",
    "    if spark_df is not None:\n",
    "        try:\n",
    "            spark_df.unpersist()\n",
    "            log_status(\"Released Spark DataFrame from memory\")\n",
    "        except Exception as e:\n",
    "            log_status(f\"Warning: Could not release Spark DataFrame: {str(e)}\")\n",
    "    \n",
    "    # Release pandas DataFrames\n",
    "    if pandas_dfs is not None:\n",
    "        import gc\n",
    "        try:\n",
    "            for i, df in enumerate(pandas_dfs):\n",
    "                del df\n",
    "                log_status(f\"Deleted pandas DataFrame {i+1} of {len(pandas_dfs)}\")\n",
    "            gc.collect()\n",
    "            log_status(\"Garbage collection completed\")\n",
    "        except Exception as e:\n",
    "            log_status(f\"Warning: Error during memory cleanup: {str(e)}\")\n",
    "\n",
    "def profile_execution(func):\n",
    "    \"\"\"\n",
    "    Decorator to profile function execution time and memory usage\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    func : function\n",
    "        Function to profile\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    function\n",
    "        Wrapped function with profiling\n",
    "    \"\"\"\n",
    "    @functools.wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        # Record start time\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Try to get memory info if psutil is available\n",
    "        try:\n",
    "            import psutil\n",
    "            start_mem = psutil.Process().memory_info().rss / (1024 * 1024)\n",
    "            memory_available = True\n",
    "        except ImportError:\n",
    "            memory_available = False\n",
    "            \n",
    "        # Run function with all arguments\n",
    "        result = func(*args, **kwargs)\n",
    "        \n",
    "        # Record end time\n",
    "        end_time = time.time()\n",
    "        execution_time = end_time - start_time\n",
    "        \n",
    "        # Log execution time\n",
    "        log_status(f\"Function {func.__name__} completed in {execution_time:.2f} seconds\")\n",
    "        \n",
    "        # Log memory usage if available\n",
    "        if memory_available:\n",
    "            try:\n",
    "                import psutil\n",
    "                end_mem = psutil.Process().memory_info().rss / (1024 * 1024)\n",
    "                memory_diff = end_mem - start_mem\n",
    "                log_status(f\"Memory change: {memory_diff:.2f} MB (Current: {end_mem:.2f} MB)\")\n",
    "            except Exception as memory_error:\n",
    "                log_status(f\"Could not measure memory usage: {str(memory_error)}\")\n",
    "        \n",
    "        # Run garbage collection\n",
    "        gc.collect()\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    return wrapper\n",
    "\n",
    "def verify_data_quality(df, stage_name):\n",
    "    \"\"\"\n",
    "    Verify data quality at various pipeline stages\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame to verify\n",
    "    stage_name : str\n",
    "        Name of the current pipeline stage\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    bool\n",
    "        True if data quality is acceptable, False otherwise\n",
    "    \"\"\"\n",
    "    issues = []\n",
    "    \n",
    "    # Skip empty dataframes\n",
    "    if df is None or len(df) == 0:\n",
    "        log_status(f\"⚠️ Empty DataFrame at {stage_name}\")\n",
    "        return False\n",
    "    \n",
    "    # Check for missing values\n",
    "    missing_counts = df.isna().sum()\n",
    "    high_missing_cols = missing_counts[missing_counts > len(df) * 0.5].index.tolist()\n",
    "    if high_missing_cols:\n",
    "        issues.append(f\"High missing values in columns: {high_missing_cols[:5]}...\")\n",
    "    \n",
    "    # Check for constant columns\n",
    "    constant_cols = [col for col in df.columns if df[col].nunique() <= 1]\n",
    "    if constant_cols:\n",
    "        issues.append(f\"Constant columns found: {constant_cols[:5]}...\")\n",
    "    \n",
    "    # Check for extreme outliers in numeric columns\n",
    "    numeric_cols = df.select_dtypes(include=['number']).columns\n",
    "    for col in numeric_cols[:5]:  # Check just first 5 numeric columns for speed\n",
    "        try:\n",
    "            q1 = df[col].quantile(0.25)\n",
    "            q3 = df[col].quantile(0.75)\n",
    "            iqr = q3 - q1\n",
    "            if iqr > 0:  # Avoid division by zero\n",
    "                extreme_count = ((df[col] < q1 - 3*iqr) | (df[col] > q3 + 3*iqr)).sum()\n",
    "                if extreme_count > len(df) * 0.05:\n",
    "                    issues.append(f\"Column {col} has {extreme_count} extreme outliers\")\n",
    "        except Exception:\n",
    "            # Skip columns that can't be analyzed\n",
    "            continue\n",
    "    \n",
    "    # Log issues or confirm quality\n",
    "    if issues:\n",
    "        log_status(f\"⚠️ Data quality issues at {stage_name}:\")\n",
    "        for issue in issues:\n",
    "            log_status(f\"  - {issue}\")\n",
    "        \n",
    "        # Sample problematic records\n",
    "        if high_missing_cols and IN_DATABRICKS:\n",
    "            try:\n",
    "                sample_col = high_missing_cols[0]\n",
    "                problem_samples = df[df[sample_col].isna()].head(3)\n",
    "                log_status(\"Sample records with missing values:\")\n",
    "                display(HTML(f\"<h5>Sample problematic records for column {sample_col}</h5>\"))\n",
    "                display(problem_samples)\n",
    "            except Exception as e:\n",
    "                log_status(f\"Could not display sample records: {str(e)}\")\n",
    "        \n",
    "        return False\n",
    "    else:\n",
    "        log_status(f\"✓ Data quality verified at {stage_name}\")\n",
    "        return True\n",
    "\n",
    "def create_detailed_log(config, pipeline_stages):\n",
    "    \"\"\"\n",
    "    Create a detailed log file with all stages and parameters\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    config : PipelineConfig\n",
    "        Configuration object\n",
    "    pipeline_stages : dict\n",
    "        Dictionary of pipeline stages and their details\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        Path to the created log file\n",
    "    \"\"\"\n",
    "    log_content = f\"Vehicle Segmentation Pipeline Execution Log\\n\"\n",
    "    log_content += f\"Date: {time.strftime('%Y-%m-%d %H:%M:%S')}\\n\"\n",
    "    log_content += \"=\" * 50 + \"\\n\\n\"\n",
    "    \n",
    "    # Add configuration\n",
    "    log_content += \"Configuration:\\n\"\n",
    "    log_content += \"-\" * 20 + \"\\n\"\n",
    "    for key, value in config.to_dict().items():\n",
    "        log_content += f\"  {key}: {value}\\n\"\n",
    "    \n",
    "    # Add pipeline stages\n",
    "    for stage, details in pipeline_stages.items():\n",
    "        log_content += f\"\\n[{stage}]\\n\"\n",
    "        log_content += \"-\" * 20 + \"\\n\"\n",
    "        for key, value in details.items():\n",
    "            if isinstance(value, dict):\n",
    "                log_content += f\"  {key}:\\n\"\n",
    "                for sub_key, sub_value in value.items():\n",
    "                    log_content += f\"    {sub_key}: {sub_value}\\n\"\n",
    "            else:\n",
    "                log_content += f\"  {key}: {value}\\n\"\n",
    "    \n",
    "    # Write log file\n",
    "    log_path = f\"{config.output_prefix}_execution_log.txt\"\n",
    "    with open(log_path, \"w\") as f:\n",
    "        f.write(log_content)\n",
    "        \n",
    "    log_status(f\"Created detailed execution log at {log_path}\")\n",
    "    return log_path\n",
    "\n",
    "print(\"\\n=== CELL 4: Enhanced Utility Functions loaded ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "5caa4432-8c6e-4def-84ed-1a627e8f83a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# CELL 5: Testing Functions (not being used so skip this cell)\n",
    "# Purpose: Adds isolated testing capabilities for different pipeline components\n",
    "\n",
    "# def test_pipeline():\n",
    "#     \"\"\"\n",
    "#     Run pipeline on tiny dataset to verify functionality\n",
    "    \n",
    "#     Returns:\n",
    "#     --------\n",
    "#     bool\n",
    "#         True if test was successful, False otherwise\n",
    "#     \"\"\"\n",
    "#     print(\"=\" * 50)\n",
    "#     print(\"RUNNING PIPELINE IN TEST MODE\")\n",
    "#     print(\"=\" * 50)\n",
    "    \n",
    "#     config = PipelineConfig(\n",
    "#         sample_size=100,  # Small sample \n",
    "#         output_prefix=\"/dbfs/FileStore/acxiom_clustering/vehicle_segmentation_test\",\n",
    "#         clustering_methods=[\"kmeans\"]  # Simplified\n",
    "#     ).enable_debug()\n",
    "    \n",
    "#     try:\n",
    "#         success = enhanced_vehicle_segmentation_pipeline(\n",
    "#             sample_size=config.sample_size,\n",
    "#             output_prefix=config.output_prefix,\n",
    "#             clustering_methods=config.clustering_methods\n",
    "#         )\n",
    "        \n",
    "#         if success:\n",
    "#             print(\"✅ TEST MODE: Pipeline executed successfully on small sample\")\n",
    "#         else:\n",
    "#             print(\"❌ TEST MODE: Pipeline failed on small sample\")\n",
    "            \n",
    "#         return success\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"❌ TEST MODE ERROR: {str(e)}\")\n",
    "#         print(traceback.format_exc())\n",
    "#         return False\n",
    "        \n",
    "# def test_data_extraction(sample_size=100):\n",
    "#     \"\"\"\n",
    "#     Test just the data extraction portion\n",
    "    \n",
    "#     Parameters:\n",
    "#     -----------\n",
    "#     sample_size : int\n",
    "#         Number of rows to sample\n",
    "        \n",
    "#     Returns:\n",
    "#     --------\n",
    "#     dict or None\n",
    "#         Extracted data if successful, None otherwise\n",
    "#     \"\"\"\n",
    "#     print(\"Testing data extraction...\")\n",
    "    \n",
    "#     column_definitions = {\n",
    "#         \"id_column\": \"GM_PERSON_REALID\",\n",
    "#         \"vehicle_columns\": {\n",
    "#             \"primary\": [\"AP004561\", \"AP006816\", \"AP004542\"],\n",
    "#             \"secondary\": [\"AP001711\", \"AP001425\"]\n",
    "#         },\n",
    "#         \"propensity_columns\": {\n",
    "#             \"primary\": [\"AP008945\", \"AP008215\"],\n",
    "#             \"secondary\": [\"AP005119\", \"AP004119\"]\n",
    "#         },\n",
    "#         \"demographic_columns\": {\n",
    "#             \"primary\": [\"AP003015\", \"AP003004\"],\n",
    "#             \"secondary\": [\"AP001119\", \"AP003711\"]\n",
    "#         }\n",
    "#     }\n",
    "    \n",
    "#     try:\n",
    "#         extracted_data = extract_acxiom_data(column_definitions, sample_size)\n",
    "#         if extracted_data:\n",
    "#             print(f\"✅ Successfully extracted {extracted_data['row_count']} rows\")\n",
    "#             return extracted_data\n",
    "#         else:\n",
    "#             print(\"❌ Data extraction returned None\")\n",
    "#             return None\n",
    "#     except Exception as e:\n",
    "#         print(f\"❌ Error in data extraction test: {str(e)}\")\n",
    "#         print(traceback.format_exc())\n",
    "#         return None\n",
    "        \n",
    "# def test_mca_only(prepared_data):\n",
    "#     \"\"\"\n",
    "#     Test just the MCA analysis\n",
    "    \n",
    "#     Parameters:\n",
    "#     -----------\n",
    "#     prepared_data : dict\n",
    "#         Prepared data for MCA\n",
    "        \n",
    "#     Returns:\n",
    "#     --------\n",
    "#     dict or None\n",
    "#         MCA results if successful, None otherwise\n",
    "#     \"\"\"\n",
    "#     print(\"Testing MCA analysis...\")\n",
    "    \n",
    "#     try:\n",
    "#         mca_results = run_mca_analysis(prepared_data)\n",
    "#         if mca_results:\n",
    "#             print(f\"✅ MCA analysis successful - identified {mca_results['n_dims']} dimensions\")\n",
    "#             return mca_results\n",
    "#         else:\n",
    "#             print(\"❌ MCA analysis returned None\")\n",
    "#             return None\n",
    "#     except Exception as e:\n",
    "#         print(f\"❌ Error in MCA test: {str(e)}\")\n",
    "#         print(traceback.format_exc())\n",
    "#         return None\n",
    "\n",
    "# def sample_problematic_records(df, condition, n=5):\n",
    "#     \"\"\"\n",
    "#     Extract sample records that meet a problematic condition\n",
    "    \n",
    "#     Parameters:\n",
    "#     -----------\n",
    "#     df : pandas.DataFrame\n",
    "#         DataFrame to sample from\n",
    "#     condition : pandas.Series of booleans\n",
    "#         Condition to identify problematic records\n",
    "#     n : int\n",
    "#         Number of samples to return\n",
    "        \n",
    "#     Returns:\n",
    "#     --------\n",
    "#     pandas.DataFrame\n",
    "#         Sample of problematic records\n",
    "#     \"\"\"\n",
    "#     problem_samples = df[condition].sample(min(n, condition.sum()))\n",
    "#     return problem_samples\n",
    "\n",
    "# print(\"\\n=== CELL 5: Testing Functions loaded ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "2f97a1a5-46e6-46db-ba2c-5e38e613e5a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# CELL 6: Report Generation Function (Python-friendly version)\n",
    "import os\n",
    "import time\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def create_enhanced_mca_analysis_report(mca_analysis, prepared_data, report_path):\n",
    "    \"\"\"\n",
    "    Create a comprehensive analysis report for MCA results with category insights\n",
    "    Using Python file handling instead of R's sink()\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    mca_analysis : dict\n",
    "        Dictionary containing MCA results from robust_run_mca_analysis()\n",
    "    prepared_data : dict\n",
    "        Dictionary containing prepared data and metadata\n",
    "    report_path : str\n",
    "        Path to save the report\n",
    "    \"\"\"\n",
    "    log_status(\"Creating enhanced MCA analysis report with category insights\")\n",
    "    \n",
    "    # Create report content as a string\n",
    "    report_content = \"===== ENHANCED ACXIOM VEHICLE SEGMENTATION MCA ANALYSIS REPORT =====\\n\\n\"\n",
    "    report_content += f\"Report generated: {time.strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\"\n",
    "    \n",
    "    report_content += \"DATA SUMMARY:\\n\"\n",
    "    report_content += \"-------------\\n\"\n",
    "    \n",
    "    # Categorized feature summary\n",
    "    if 'column_categories' in prepared_data:\n",
    "        categories = prepared_data['column_categories']\n",
    "        category_counts = {}\n",
    "        for category in set(categories.values()):\n",
    "            category_counts[category] = sum(1 for c in categories.values() if c == category)\n",
    "        \n",
    "        report_content += \"Features by category:\\n\"\n",
    "        for category, count in category_counts.items():\n",
    "            report_content += f\"- {category}: {count} features\\n\"\n",
    "    else:\n",
    "        report_content += f\"Total feature columns: {len(prepared_data['feature_cols'])}\\n\"\n",
    "    \n",
    "    report_content += f\"Categorical variables used: {len(prepared_data['categorical_cols'])}\\n\\n\"\n",
    "    \n",
    "    report_content += \"MCA RESULTS:\\n\"\n",
    "    report_content += \"-----------\\n\"\n",
    "    report_content += f\"Total dimensions: {len(mca_analysis['eigenvalues'])}\\n\"\n",
    "    report_content += f\"Retained dimensions: {mca_analysis['n_dims']}\\n\\n\"\n",
    "    \n",
    "    report_content += \"VARIANCE EXPLAINED:\\n\"\n",
    "    report_content += \"------------------\\n\"\n",
    "    report_content += f\"Dimension 1: {mca_analysis['var_explained'][0]:.2%}\\n\"\n",
    "    if len(mca_analysis['var_explained']) > 1:\n",
    "        report_content += f\"Dimension 2: {mca_analysis['var_explained'][1]:.2%}\\n\"\n",
    "    if len(mca_analysis['var_explained']) > 2:\n",
    "        report_content += f\"Dimension 3: {mca_analysis['var_explained'][2]:.2%}\\n\"\n",
    "    report_content += f\"Cumulative (all retained dimensions): {mca_analysis['cum_var'][mca_analysis['n_dims']-1]:.2%}\\n\\n\"\n",
    "    \n",
    "    report_content += \"CUSTOMER SEGMENT INTERPRETATION GUIDE:\\n\"\n",
    "    report_content += \"-------------------------------------\\n\"\n",
    "    report_content += \"Based on the MCA analysis, we can identify several potential customer segments:\\n\\n\"\n",
    "    \n",
    "    report_content += \"1. Luxury Vehicle Enthusiasts\\n\"\n",
    "    report_content += \"   - Likely found in positive values of Dimension 1\\n\"\n",
    "    report_content += \"   - Associated with premium vehicle preferences (Luxury SUVs, Performance models)\\n\"\n",
    "    report_content += \"   - Higher income demographics and luxury spending patterns\\n\"\n",
    "    report_content += \"   - Financing via leasing more common than traditional loans\\n\\n\"\n",
    "    \n",
    "    report_content += \"2. Practical Family Segments\\n\"\n",
    "    report_content += \"   - Likely found in negative values of Dimension 1, positive values of Dimension 2\\n\"\n",
    "    report_content += \"   - Associated with SUVs, minivans, and family-oriented vehicles\\n\" \n",
    "    report_content += \"   - Middle income brackets with family-focused buying patterns\\n\"\n",
    "    report_content += \"   - Traditional financing and value considerations\\n\\n\"\n",
    "    \n",
    "    report_content += \"3. Economy/Value Segments\\n\"\n",
    "    report_content += \"   - Likely found in negative regions of both dimensions\\n\"\n",
    "    report_content += \"   - Strong association with compact and economy vehicles\\n\"\n",
    "    report_content += \"   - Price sensitivity and practical considerations dominant\\n\"\n",
    "    report_content += \"   - Various financing options including longer-term loans\\n\\n\"\n",
    "    \n",
    "    report_content += \"4. Technology Early Adopters\\n\"\n",
    "    report_content += \"   - Distinctive position in positive Dimension 2 space\\n\"\n",
    "    report_content += \"   - Interest in alternative fuel vehicles and latest features\\n\"\n",
    "    report_content += \"   - Higher technology adoption scores across categories\\n\"\n",
    "    report_content += \"   - More diverse demographic profile than other segments\\n\\n\"\n",
    "    \n",
    "    report_content += \"NEXT STEPS:\\n\"\n",
    "    report_content += \"-----------\\n\"\n",
    "    report_content += \"1. Use the MCA coordinates for k-means clustering to identify distinct consumer segments\\n\"\n",
    "    report_content += \"2. Validate clusters against known customer behaviors and outcomes\\n\"\n",
    "    report_content += \"3. Develop targeted marketing strategies for each identified segment\\n\"\n",
    "    report_content += \"4. Create actionable audience profiles for CDP implementation\\n\"\n",
    "    report_content += \"5. Apply cluster assignments to full customer dataset to enable personalization\\n\\n\"\n",
    "    \n",
    "    report_content += \"===== END OF REPORT =====\\n\"\n",
    "    \n",
    "    # Create directory if it doesn't exist\n",
    "    try:\n",
    "        os.makedirs(os.path.dirname(report_path), exist_ok=True)\n",
    "    except Exception as e:\n",
    "        log_status(f\"Warning: Could not create directory for report: {str(e)}\")\n",
    "    \n",
    "    # Write to file using Python's file handling (replaces R's sink)\n",
    "    try:\n",
    "        with open(report_path, \"w\") as f:\n",
    "            f.write(report_content)\n",
    "    except Exception as e:\n",
    "        log_status(f\"ERROR: Could not write report to {report_path}: {str(e)}\")\n",
    "        return\n",
    "    \n",
    "    # Display report summary in Databricks\n",
    "    if IN_DATABRICKS:\n",
    "        try:\n",
    "            display(HTML(f\"<h4>Enhanced MCA Analysis Report Created</h4><pre>{report_content[:500]}...</pre><p><i>Full report saved to {report_path}</i></p>\"))\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    log_status(f\"Enhanced vehicle segmentation MCA analysis report saved to {report_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "8de06a01-4b07-45bb-9f0b-1165f76a58cd",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Generate Cluster-Based Segment Names with Profiles"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 7: Generate Cluster-Based Segment Names (not being used so skip this cell)\n",
    "\n",
    "# def generate_data_driven_segment_names(cluster_data, cluster_col, prepared_data, mca_analysis, column_map=None):\n",
    "#     \"\"\"\n",
    "#     Generate data-driven segment names based on cluster characteristics\n",
    "    \n",
    "#     Parameters:\n",
    "#     -----------\n",
    "#     cluster_data : pandas.DataFrame\n",
    "#         DataFrame containing cluster assignments\n",
    "#     cluster_col : str\n",
    "#         Name of the column containing cluster assignments\n",
    "#     prepared_data : dict\n",
    "#         Dictionary with prepared data and column information\n",
    "#     mca_analysis : dict\n",
    "#         Dictionary with MCA analysis results\n",
    "#     column_map : dict, optional\n",
    "#         Mapping of column categories (vehicle, demographic, etc.)\n",
    "        \n",
    "#     Returns:\n",
    "#     --------\n",
    "#     dict\n",
    "#         Dictionary mapping cluster IDs to generated names\n",
    "#     \"\"\"\n",
    "#     log_status(\"Generating data-driven segment names based on cluster characteristics...\")\n",
    "    \n",
    "#     # Get number of clusters\n",
    "#     num_clusters = cluster_data[cluster_col].nunique()\n",
    "    \n",
    "#     # Initialize names dictionary\n",
    "#     segment_names = {}\n",
    "    \n",
    "#     # If we have the original feature data\n",
    "#     if 'features' in prepared_data and 'column_categories' in prepared_data:\n",
    "#         original_data = prepared_data['features']\n",
    "#         column_categories = prepared_data['column_categories']\n",
    "        \n",
    "#         # Join cluster assignments to original data\n",
    "#         if cluster_col not in original_data.columns:\n",
    "#             analysis_data = original_data.copy()\n",
    "#             analysis_data[cluster_col] = cluster_data[cluster_col].values\n",
    "#         else:\n",
    "#             analysis_data = original_data\n",
    "        \n",
    "#         # For each cluster\n",
    "#         for cluster_id in range(num_clusters):\n",
    "#             # Isolate cluster data\n",
    "#             cluster_mask = analysis_data[cluster_col] == cluster_id\n",
    "#             cluster_segment = analysis_data[cluster_mask]\n",
    "            \n",
    "#             # Create a profile based on original features\n",
    "#             # First categorize features\n",
    "#             vehicle_cols = [col for col, cat in column_categories.items() \n",
    "#                          if cat.lower() == 'vehicle']\n",
    "#             demo_cols = [col for col, cat in column_categories.items() \n",
    "#                        if cat.lower() == 'demographic']\n",
    "#             propensity_cols = [col for col, cat in column_categories.items() \n",
    "#                              if cat.lower() == 'propensity']\n",
    "            \n",
    "#             # Find distinctive vehicle preferences\n",
    "#             # This approach looks for columns where the cluster has notably higher values\n",
    "#             dominant_vehicle_type = None\n",
    "#             max_vehicle_diff = 0\n",
    "            \n",
    "#             for col in vehicle_cols:\n",
    "#                 if col in cluster_segment.columns:\n",
    "#                     cluster_mean = cluster_segment[col].mean()\n",
    "#                     overall_mean = analysis_data[col].mean()\n",
    "#                     diff = cluster_mean - overall_mean\n",
    "                    \n",
    "#                     if diff > max_vehicle_diff:\n",
    "#                         max_vehicle_diff = diff\n",
    "#                         # Extract vehicle type from column name or value\n",
    "#                         # This will depend on your specific column naming conventions\n",
    "#                         if \"luxury\" in col.lower():\n",
    "#                             dominant_vehicle_type = \"Luxury\"\n",
    "#                         elif \"suv\" in col.lower() or \"pickup\" in col.lower():\n",
    "#                             dominant_vehicle_type = \"SUV/Truck\"\n",
    "#                         elif \"compact\" in col.lower() or \"economy\" in col.lower():\n",
    "#                             dominant_vehicle_type = \"Economy\"\n",
    "#                         elif \"electric\" in col.lower() or \"hybrid\" in col.lower():\n",
    "#                             dominant_vehicle_type = \"Alternative Fuel\"\n",
    "#                         elif \"sports\" in col.lower() or \"performance\" in col.lower():\n",
    "#                             dominant_vehicle_type = \"Performance\"\n",
    "#                         elif \"family\" in col.lower() or \"minivan\" in col.lower():\n",
    "#                             dominant_vehicle_type = \"Family\"\n",
    "#                         else:\n",
    "#                             dominant_vehicle_type = \"Standard\"\n",
    "            \n",
    "#             # Find distinctive demographics\n",
    "#             income_level = None\n",
    "#             age_group = None\n",
    "            \n",
    "#             for col in demo_cols:\n",
    "#                 if col in cluster_segment.columns:\n",
    "#                     # Check income-related columns\n",
    "#                     if \"income\" in col.lower():\n",
    "#                         cluster_mean = cluster_segment[col].mean()\n",
    "#                         overall_mean = analysis_data[col].mean()\n",
    "                        \n",
    "#                         if cluster_mean > overall_mean * 1.2:  # 20% higher\n",
    "#                             if \"high\" in col.lower() or \"200k\" in col.lower():\n",
    "#                                 income_level = \"Affluent\"\n",
    "#                             elif \"mid\" in col.lower() or \"100k\" in col.lower():\n",
    "#                                 income_level = \"Middle-Income\"\n",
    "#                             elif \"low\" in col.lower() or \"50k\" in col.lower():\n",
    "#                                 income_level = \"Budget-Conscious\"\n",
    "                    \n",
    "#                     # Check age-related columns\n",
    "#                     if \"age\" in col.lower():\n",
    "#                         cluster_mean = cluster_segment[col].mean()\n",
    "#                         overall_mean = analysis_data[col].mean()\n",
    "                        \n",
    "#                         if cluster_mean > overall_mean * 1.2:  # 20% higher\n",
    "#                             if \"young\" in col.lower() or \"18-34\" in col.lower():\n",
    "#                                 age_group = \"Young\"\n",
    "#                             elif \"senior\" in col.lower() or \"65+\" in col.lower():\n",
    "#                                 age_group = \"Senior\"\n",
    "#                             elif \"middle\" in col.lower() or \"35-64\" in col.lower():\n",
    "#                                 age_group = \"Middle-Aged\"\n",
    "            \n",
    "#             # Find distinctive buying behavior\n",
    "#             buying_behavior = None\n",
    "#             for col in propensity_cols:\n",
    "#                 if col in cluster_segment.columns:\n",
    "#                     cluster_mean = cluster_segment[col].mean()\n",
    "#                     overall_mean = analysis_data[col].mean()\n",
    "                    \n",
    "#                     if cluster_mean > overall_mean * 1.2:  # 20% higher\n",
    "#                         if \"tech\" in col.lower() or \"early\" in col.lower():\n",
    "#                             buying_behavior = \"Tech-Forward\"\n",
    "#                         elif \"value\" in col.lower() or \"price\" in col.lower():\n",
    "#                             buying_behavior = \"Value-Conscious\"\n",
    "#                         elif \"luxury\" in col.lower() or \"premium\" in col.lower():\n",
    "#                             buying_behavior = \"Premium\"\n",
    "#                         elif \"eco\" in col.lower() or \"environment\" in col.lower():\n",
    "#                             buying_behavior = \"Eco-Conscious\"\n",
    "#                         elif \"family\" in col.lower() or \"practical\" in col.lower():\n",
    "#                             buying_behavior = \"Practical\"\n",
    "                            \n",
    "#             # Generate segment name based on findings\n",
    "#             name_parts = []\n",
    "            \n",
    "#             if income_level:\n",
    "#                 name_parts.append(income_level)\n",
    "            \n",
    "#             if age_group:\n",
    "#                 name_parts.append(age_group)\n",
    "            \n",
    "#             if buying_behavior:\n",
    "#                 name_parts.append(buying_behavior)\n",
    "                \n",
    "#             if dominant_vehicle_type:\n",
    "#                 if buying_behavior:\n",
    "#                     name_parts.append(f\"{dominant_vehicle_type} Vehicle Buyers\")\n",
    "#                 else:\n",
    "#                     name_parts.append(f\"{dominant_vehicle_type} Vehicle Enthusiasts\")\n",
    "            \n",
    "#             # If we couldn't determine characteristics, use cluster dimensions\n",
    "#             if not name_parts and 'mca_coords' in mca_analysis:\n",
    "#                 # Get cluster center in MCA space\n",
    "#                 dim_cols = [f'MCA_dim{i+1}' for i in range(min(3, mca_analysis['n_dims']))]\n",
    "#                 center = cluster_data[cluster_data[cluster_col] == cluster_id][dim_cols].mean()\n",
    "                \n",
    "#                 # Determine main characteristics from MCA dimensions\n",
    "#                 # This is highly dependent on how the MCA dimensions are interpreted\n",
    "#                 # Usually requires domain knowledge or post-hoc analysis\n",
    "                \n",
    "#                 if center['MCA_dim1'] > 0.5:\n",
    "#                     name_parts.append(\"Premium Vehicle\")\n",
    "#                 elif center['MCA_dim1'] < -0.5:\n",
    "#                     name_parts.append(\"Economy Vehicle\")\n",
    "                    \n",
    "#                 if len(dim_cols) > 1:\n",
    "#                     if center['MCA_dim2'] > 0.5:\n",
    "#                         name_parts.append(\"Tech-Savvy\")\n",
    "#                     elif center['MCA_dim2'] < -0.5:\n",
    "#                         name_parts.append(\"Traditional\")\n",
    "                \n",
    "#                 if not name_parts:\n",
    "#                     name_parts = [\"Mainstream\"] \n",
    "                \n",
    "#                 name_parts.append(\"Buyers\")\n",
    "            \n",
    "#             # Final fallback if we still don't have a name\n",
    "#             if not name_parts:\n",
    "#                 name_parts = [f\"Segment {cluster_id + 1}\"]\n",
    "            \n",
    "#             # Combine parts into a full name\n",
    "#             segment_names[cluster_id] = \" \".join(name_parts)\n",
    "    \n",
    "#     # If we don't have original features or failed to generate names,\n",
    "#     # fallback to generic names with MCA dimensions information\n",
    "#     if not segment_names:\n",
    "#         log_status(\"Using MCA dimensions to generate generic segment names\")\n",
    "        \n",
    "#         # Extract MCA dimensions data\n",
    "#         dim_cols = [f'MCA_dim{i+1}' for i in range(min(3, mca_analysis['n_dims']))]\n",
    "        \n",
    "#         # For each cluster\n",
    "#         for cluster_id in range(num_clusters):\n",
    "#             # Get cluster center in MCA space\n",
    "#             center = cluster_data[cluster_data[cluster_col] == cluster_id][dim_cols].mean()\n",
    "            \n",
    "#             # Use the dominant dimensions to name the cluster\n",
    "#             primary_dim = None\n",
    "#             primary_value = 0\n",
    "            \n",
    "#             for dim in dim_cols:\n",
    "#                 if abs(center[dim]) > abs(primary_value):\n",
    "#                     primary_value = center[dim]\n",
    "#                     primary_dim = dim\n",
    "            \n",
    "#             if primary_dim and abs(primary_value) > 0.2:\n",
    "#                 direction = \"High\" if primary_value > 0 else \"Low\"\n",
    "#                 segment_names[cluster_id] = f\"{direction} {primary_dim} Vehicle Segment\"\n",
    "#             else:\n",
    "#                 segment_names[cluster_id] = f\"Average Vehicle Segment {cluster_id + 1}\"\n",
    "    \n",
    "#     log_status(f\"Generated {len(segment_names)} data-driven segment names\")\n",
    "#     return segment_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eeeded4b-1fde-4ff3-9064-03ed31d5f534",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# CELL 8: Global Configuration\n",
    "GLOBAL_CONFIG = {\n",
    "    # Database and table settings\n",
    "    'acxiom_table': \"dataproducts_dev.bronze_acxiom.gm_consumer_list\",\n",
    "    'id_column': \"GM_PERSON_REALID\",\n",
    "    \n",
    "    # File paths and prefixes\n",
    "    'base_output_path': \"/dbfs/FileStore/acxiom_clustering/\",\n",
    "    'mca_output_prefix': \"/dbfs/FileStore/acxiom_clustering/mca_results\",\n",
    "    'clustering_output_prefix': \"/dbfs/FileStore/acxiom_clustering/clustering_results\",\n",
    "    \n",
    "    # Sample sizes\n",
    "    'mca_sample_size': 50000,  # Larger sample for MCA\n",
    "    'clustering_sample_size': 40000,  # Smaller sample for clustering\n",
    "    \n",
    "    # Clustering parameters\n",
    "    'kmeans_clusters': [8, 10, 12],\n",
    "    'hierarchical_clusters': [8, 10, 12],\n",
    "    'max_clusters': 15\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "40dae68c-86be-4331-b2b5-a83e27f04dbb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# CELL 9: Column Definitions\n",
    "# Define columns for each category\n",
    "def define_columns():\n",
    "    # Define the ID column\n",
    "    id_column = \"GM_PERSON_REALID\"\n",
    "    \n",
    "    # 1. VEHICLE PREFERENCE COLUMNS - Expanded\n",
    "    vehicle_columns = [\n",
    "        # Luxury vehicle interest (already in your code)\n",
    "        \"AP004561\",  # Purchase a New Luxury CUV (Financial)\n",
    "        \"AP004542\",  # Purchase a New Luxury SUV (Financial)\n",
    "        \"AP004563\",  # Purchase a New Luxury Car (Financial)\n",
    "        \n",
    "        # Economy/Mainstream vehicle interest (already in your code)\n",
    "        \"AP004559\",  # Purchase a New Compact Car (Financial)\n",
    "        \"AP004564\",  # Purchase a New Mid-Sized Car (Financial)\n",
    "        \"AP004560\",  # Purchase a New Full-Sized Car (Financial)\n",
    "        \n",
    "        # Specialty vehicle interest (already in your code)\n",
    "        \"AP004562\",  # Purchase a New Sports Car (Financial)\n",
    "        \"AP004540\",  # Purchase a New Full-Sized Pickup (Financial)\n",
    "        \"AP004541\",  # Purchase a New Minivan (Financial)\n",
    "        \"AP004543\",  # Purchase a New Economy SUV (Financial)\n",
    "        \n",
    "        # Current ownership (already in your code)\n",
    "        \"AP007105\",  # Owns a Luxury SUV Body Style Vehicle\n",
    "        \"AP007113\",  # Owns an all electric SUV\n",
    "        \n",
    "        # General vehicle interest (already in your code)\n",
    "        \"AP006816\",  # In market for a vehicle\n",
    "        \"AP001711\",  # Alternative to AP007119 for fuel type preference\n",
    "        \"AP001425\",  # Household vehicle ownership count\n",
    "        \n",
    "        # NEW: Additional vehicle body type ownership\n",
    "        \"AP007106\",  # Owns a Sedan Body Style\n",
    "        \"AP007107\",  # Owns a Compact Vehicle\n",
    "        \"AP007108\",  # Owns a Pickup Truck\n",
    "        \"AP007109\",  # Owns a Minivan\n",
    "        \"AP007114\",  # Owns hybrid vehicle\n",
    "        \"AP007115\",  # Interest in electric vehicles\n",
    "        \n",
    "        # NEW: Vehicle usage patterns\n",
    "        \"AP005201\",  # Daily commuter usage\n",
    "        \"AP005202\",  # Weekend leisure driving\n",
    "        \"AP005203\"   # Long-distance travel\n",
    "    ]\n",
    "    \n",
    "    # 2. PROPENSITY & PURCHASE BEHAVIOR COLUMNS - Expanded\n",
    "    propensity_columns = [\n",
    "        # Purchase behavior (already in your code)\n",
    "        \"AP005983\",  # Likely to buy vehicle with alternative fuel\n",
    "        \"AP006832\",  # Shops for auto insurance\n",
    "        \n",
    "        # Financing behavior (already in your code)\n",
    "        \"AP004553\",  # Auto Loan - New Vehicle (Financial)\n",
    "        \"AP004554\",  # Auto Loan - Used Vehicle (Financial)\n",
    "        \"AP004570\",  # Auto Lease (Financial)\n",
    "        \n",
    "        # Propensity indicators (already in your code)\n",
    "        \"AP004118\",  # Automotive - General Interest\n",
    "        \"AP008214\",  # Automobile ownership\n",
    "        \"AP008945\",  # New vehicle purchase propensity\n",
    "        \"AP008215\",  # Vehicle upgrade timeframe\n",
    "        \"AP008219\",  # Price sensitivity\n",
    "        \"AP007821\",  # Technology adoption\n",
    "        \"AP007903\",  # Finance vs cash purchase\n",
    "        \n",
    "        # NEW: Decision-making process \n",
    "        \"AP008301\",  # Research-intensive buyer\n",
    "        \"AP008302\",  # Brand-loyal customer\n",
    "        \"AP008303\",  # Deal-seeking behavior\n",
    "        \"AP004581\",  # Preferred loan duration\n",
    "        \n",
    "        # NEW: Shopping patterns\n",
    "        \"AP005270\",  # High-end shopper\n",
    "        \"AP005271\"   # Value shopper\n",
    "    ]\n",
    "    \n",
    "    # 3. DEMOGRAPHIC COLUMNS - Expanded\n",
    "    demographic_columns = [\n",
    "        # Income brackets (already in your code)\n",
    "        \"AP001200\",  # Household Income <$15K\n",
    "        \"AP001203\",  # Household Income $35-49K\n",
    "        \"AP001204\",  # Household Income $50-74K\n",
    "        \"AP001205\",  # Household Income $150-174K\n",
    "        \"AP001206\",  # Household Income $175-199K\n",
    "        \"AP001207\",  # Household Income $200-249K\n",
    "        \"AP001208\",  # Household Income $250K+\n",
    "        \n",
    "        # Age brackets (already in your code)\n",
    "        \"AP001106\",  # Age 65-74\n",
    "        \"AP001107\",  # Age 75+\n",
    "        \n",
    "        # Other key demographics (already in your code)\n",
    "        \"AP003015\",  # Household income (general)\n",
    "        \"AP003004\",  # Age (general)\n",
    "        \"AP003035\",  # Geographic location (urban/suburban/rural)\n",
    "        \"AP003061\",  # Home ownership\n",
    "        \"AP001119\",  # Alternative demographic column\n",
    "        \"AP003711\",  # Alternative demographic column\n",
    "        \n",
    "        # NEW: Family status\n",
    "        \"AP001500\",  # Number of children in household\n",
    "        \"AP001501\",  # Presence of teenagers\n",
    "        \"AP001502\",  # Multi-generational household\n",
    "        \"AP001505\",  # Recent baby/new parent\n",
    "        \n",
    "        # NEW: Additional location indicators\n",
    "        \"AP003036\",  # Commute length\n",
    "        \"AP003037\"   # Public transit usage\n",
    "    ]\n",
    "    \n",
    "    # 4. LIFESTYLE/USAGE COLUMNS - Expanded\n",
    "    lifestyle_columns = [\n",
    "        # Outdoor activities (already in your code)\n",
    "        \"AP003921\",  # Outdoor enthusiast - hunting/fishing\n",
    "        \"AP005265\",  # Interest in camping and hiking\n",
    "        \"AP003965\",  # Off-road racing enthusiast\n",
    "        \n",
    "        # Travel & business (already in your code)\n",
    "        \"AP004081\",  # Business traveler\n",
    "        \"AP003726\",  # Travel enthusiast\n",
    "        \n",
    "        # Luxury & culture (already in your code)\n",
    "        \"AP003895\",  # Luxury product buyer\n",
    "        \"AP003935\",  # Cultural/arts enthusiast\n",
    "        \n",
    "        # Shopping behavior (already in your code)\n",
    "        \"AP008218\",  # Technology interest level\n",
    "        \"AP005270\",  # High-end shopper\n",
    "        \"AP005271\",  # Value shopper\n",
    "        \"AP008124\",  # Social media user\n",
    "        \n",
    "        # NEW: Additional lifestyle indicators\n",
    "        \"AP003980\",  # Home improvement enthusiast\n",
    "        \"AP003981\",  # DIY car maintenance\n",
    "        \"AP003982\",  # Environmental concerns\n",
    "        \"AP003983\"   # Urban vs rural lifestyle preference\n",
    "    ]\n",
    "    \n",
    "    # 5. FINANCIAL BEHAVIOR COLUMNS - Expanded\n",
    "    financial_columns = [\n",
    "        # Credit card behavior (already in your code)\n",
    "        \"AP004504\",  # Credit card - Premium/Upscale\n",
    "        \"AP004508\",  # Credit card - Cash-back reward\n",
    "        \"AP004510\",  # Credit card - Airline miles reward\n",
    "        \"AP004512\",  # Credit card - Points reward\n",
    "        \"AP004505\",  # Credit card - Standard\n",
    "        \"AP004520\",  # Credit card - Frequent user\n",
    "        \"AP004531\",  # Credit card - Balance carrier\n",
    "        \n",
    "        # Investment behavior (already in your code)\n",
    "        \"AP005550\",  # Financial investment - Active Stock Trader\n",
    "        \"AP004580\",  # Bank account - Premium banking\n",
    "        \"AP007904\",  # Financial outlook indicator\n",
    "        \n",
    "        # NEW: Economic indicators\n",
    "        \"AP003580\",  # Household income tier\n",
    "        \"AP003581\",  # Credit card usage frequency\n",
    "        \"AP003582\",  # Savings behavior\n",
    "        \"AP003583\",  # Risk tolerance\n",
    "        \"AP003584\"   # Financial planning preference\n",
    "    ]\n",
    "    \n",
    "    return id_column, vehicle_columns, propensity_columns, demographic_columns, lifestyle_columns, financial_columns\n",
    "\n",
    "print(\"\\n=== CELL: Column Definitions defined ===\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "9df32bcc-bc4e-4d1c-b066-e1fb61d43dd5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# CELL 10: Data Extraction Function - Modified to use Python-friendly formats\n",
    "def direct_extract_acxiom_data(sample_size=GLOBAL_CONFIG.get('mca_sample_size', 10000)):\n",
    "    \"\"\"\n",
    "    Extract Acxiom data directly using SQL without complex stratification\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    sample_size : int\n",
    "        Number of rows to sample\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary containing extracted data and metadata\n",
    "    \"\"\"\n",
    "    extraction_start = time.time()\n",
    "    log_status(f\"Starting direct Acxiom data extraction with sample size: {sample_size}\")\n",
    "    \n",
    "    try:\n",
    "        # Get column definitions\n",
    "        id_column, vehicle_columns, propensity_columns, demographic_columns, lifestyle_columns, financial_columns = define_columns()\n",
    "        \n",
    "        # Combine all columns to extract\n",
    "        all_columns = [id_column] + vehicle_columns + propensity_columns + demographic_columns + lifestyle_columns + financial_columns\n",
    "        \n",
    "        # Remove any duplicates while preserving order\n",
    "        all_columns = list(dict.fromkeys(all_columns))\n",
    "        \n",
    "        # Create a column mapping for return\n",
    "        column_map = {\n",
    "            \"id\": id_column,\n",
    "            \"vehicle\": vehicle_columns,\n",
    "            \"propensity\": propensity_columns,\n",
    "            \"demographic\": demographic_columns,\n",
    "            \"lifestyle\": lifestyle_columns,\n",
    "            \"financial\": financial_columns\n",
    "        }\n",
    "        \n",
    "        # Specify the correct table name\n",
    "        acxiom_table = \"dataproducts_dev.bronze_acxiom.gm_consumer_list\"\n",
    "        \n",
    "        # Verify table access\n",
    "        log_status(\"Verifying database access...\")\n",
    "        try:\n",
    "            spark.sql(f\"SELECT 1 FROM {acxiom_table} LIMIT 1\")\n",
    "            log_status(\"Database access verified\")\n",
    "        except Exception as e:\n",
    "            log_status(f\"FATAL ERROR: Cannot access Acxiom table: {str(e)}\")\n",
    "            raise RuntimeError(f\"Cannot access {acxiom_table}. This is a fatal error.\")\n",
    "        \n",
    "        # Verify column existence (get a sample row)\n",
    "        sample_df = spark.sql(f\"SELECT * FROM {acxiom_table} LIMIT 1\")\n",
    "        available_columns = set(sample_df.columns)\n",
    "        \n",
    "        # Filter for only available columns\n",
    "        valid_columns = [col for col in all_columns if col in available_columns]\n",
    "        \n",
    "        # Report on missing columns\n",
    "        missing_columns = set(all_columns) - set(valid_columns)\n",
    "        if missing_columns:\n",
    "            log_status(f\"WARNING: {len(missing_columns)} columns not found in dataset\")\n",
    "            # Only log first 10 missing columns to avoid clutter\n",
    "            if len(missing_columns) > 10:\n",
    "                log_status(f\"First 10 missing columns: {list(missing_columns)[:10]}\")\n",
    "            else:\n",
    "                log_status(f\"Missing columns: {list(missing_columns)}\")\n",
    "        \n",
    "        # Ensure we have enough columns\n",
    "        if len(valid_columns) < 10:\n",
    "            log_status(f\"FATAL ERROR: Not enough valid columns found (only {len(valid_columns)}). Need at least 10 columns for meaningful analysis.\")\n",
    "            raise RuntimeError(\"Not enough valid columns found for analysis. This is a fatal error.\")\n",
    "        \n",
    "        # Build column list for query\n",
    "        column_list = \", \".join(valid_columns)\n",
    "        \n",
    "        # Create simple query - just use random sampling\n",
    "        log_status(\"Executing simple random sampling...\")\n",
    "        simple_sql_query = f\"\"\"\n",
    "        SELECT {column_list}\n",
    "        FROM {acxiom_table}\n",
    "        WHERE {id_column} IS NOT NULL\n",
    "        ORDER BY rand()\n",
    "        LIMIT {sample_size}\n",
    "        \"\"\"\n",
    "        \n",
    "        # Execute the query\n",
    "        acxiom_df = spark.sql(simple_sql_query)\n",
    "        \n",
    "        # Check if we got enough data\n",
    "        row_count = acxiom_df.count()\n",
    "        if row_count == 0:\n",
    "            log_status(\"FATAL ERROR: No rows returned from Acxiom query\")\n",
    "            raise RuntimeError(\"No data returned from Acxiom database query. This is a fatal error.\")\n",
    "        \n",
    "        # Cache the result for faster subsequent operations\n",
    "        acxiom_df.cache()\n",
    "        \n",
    "        # Display sample in Databricks\n",
    "        if IN_DATABRICKS:\n",
    "            try:\n",
    "                display(HTML(\"<h4>Data Sample (first 5 rows)</h4>\"))\n",
    "                display(acxiom_df.limit(5))\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Prepare result dictionary\n",
    "        col_count = len(acxiom_df.columns)\n",
    "        extract_time = time.time() - extraction_start\n",
    "        log_status(f\"Successfully extracted {row_count} rows and {col_count} columns in {extract_time:.2f} seconds\")\n",
    "        \n",
    "        result = {\n",
    "            'spark_df': acxiom_df,\n",
    "            'id_column': id_column,\n",
    "            'column_map': column_map,\n",
    "            'row_count': row_count,\n",
    "            'extract_time': extract_time\n",
    "        }\n",
    "        \n",
    "        # Save extraction metadata (Python-friendly format)\n",
    "        import json\n",
    "        metadata_path = f\"/dbfs/FileStore/acxiom_clustering/extraction_metadata.json\"\n",
    "        metadata = {\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'sample_size': sample_size,\n",
    "            'rows_extracted': row_count,\n",
    "            'columns_extracted': col_count,\n",
    "            'execution_time_seconds': extract_time\n",
    "        }\n",
    "        \n",
    "        # Create directory if it doesn't exist\n",
    "        import os\n",
    "        os.makedirs(os.path.dirname(metadata_path), exist_ok=True)\n",
    "        \n",
    "        # Now save the metadata\n",
    "        with open(metadata_path, 'w') as f:\n",
    "            json.dump(metadata, f, indent=2)\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        log_status(f\"ERROR in direct data extraction: {str(e)}\")\n",
    "        log_status(f\"Error details: {traceback.format_exc()}\")\n",
    "        raise\n",
    "        \n",
    "        # Verify column existence (get a sample row)\n",
    "        sample_df = spark.sql(f\"SELECT * FROM {acxiom_table} LIMIT 1\")\n",
    "        available_columns = set(sample_df.columns)\n",
    "        \n",
    "        # Filter for only available columns\n",
    "        valid_columns = [col for col in all_columns if col in available_columns]\n",
    "        \n",
    "        # Report on missing columns\n",
    "        missing_columns = set(all_columns) - set(valid_columns)\n",
    "        if missing_columns:\n",
    "            log_status(f\"WARNING: {len(missing_columns)} columns not found in dataset\")\n",
    "            # Only log first 10 missing columns to avoid clutter\n",
    "            if len(missing_columns) > 10:\n",
    "                log_status(f\"First 10 missing columns: {list(missing_columns)[:10]}\")\n",
    "            else:\n",
    "                log_status(f\"Missing columns: {list(missing_columns)}\")\n",
    "        \n",
    "        # Ensure we have enough columns\n",
    "        if len(valid_columns) < 10:\n",
    "            log_status(f\"FATAL ERROR: Not enough valid columns found (only {len(valid_columns)}). Need at least 10 columns for meaningful analysis.\")\n",
    "            raise RuntimeError(\"Not enough valid columns found for analysis. This is a fatal error.\")\n",
    "        \n",
    "        # Build column list for query\n",
    "        column_list = \", \".join(valid_columns)\n",
    "        \n",
    "        # Create simple query - just use random sampling\n",
    "        log_status(\"Executing simple random sampling...\")\n",
    "        simple_sql_query = f\"\"\"\n",
    "        SELECT {column_list}\n",
    "        FROM {acxiom_table}\n",
    "        WHERE {id_column} IS NOT NULL\n",
    "        ORDER BY rand()\n",
    "        LIMIT {sample_size}\n",
    "        \"\"\"\n",
    "        \n",
    "        # Execute the query\n",
    "        acxiom_df = spark.sql(simple_sql_query)\n",
    "        \n",
    "        # Check if we got enough data\n",
    "        row_count = acxiom_df.count()\n",
    "        if row_count == 0:\n",
    "            log_status(\"FATAL ERROR: No rows returned from Acxiom query\")\n",
    "            raise RuntimeError(\"No data returned from Acxiom database query. This is a fatal error.\")\n",
    "        \n",
    "        # Cache the result for faster subsequent operations\n",
    "        acxiom_df.cache()\n",
    "        \n",
    "        # Display sample in Databricks\n",
    "        if IN_DATABRICKS:\n",
    "            try:\n",
    "                display(HTML(\"<h4>Data Sample (first 5 rows)</h4>\"))\n",
    "                display(acxiom_df.limit(5))\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Prepare result dictionary\n",
    "        col_count = len(acxiom_df.columns)\n",
    "        extract_time = time.time() - extraction_start\n",
    "        log_status(f\"Successfully extracted {row_count} rows and {col_count} columns in {extract_time:.2f} seconds\")\n",
    "        \n",
    "        result = {\n",
    "            'spark_df': acxiom_df,\n",
    "            'id_column': id_column,\n",
    "            'column_map': column_map,\n",
    "            'row_count': row_count,\n",
    "            'extract_time': extract_time\n",
    "        }\n",
    "        \n",
    "        # Save extraction metadata (Python-friendly format)\n",
    "        import json\n",
    "        metadata_path = f\"/dbfs/FileStore/acxiom_clustering/extraction_metadata.json\"\n",
    "        metadata = {\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'sample_size': sample_size,\n",
    "            'rows_extracted': row_count,\n",
    "            'columns_extracted': col_count,\n",
    "            'execution_time_seconds': extract_time\n",
    "        }\n",
    "        with open(metadata_path, 'w') as f:\n",
    "            json.dump(metadata, f, indent=2)\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        log_status(f\"ERROR in direct data extraction: {str(e)}\")\n",
    "        log_status(f\"Error details: {traceback.format_exc()}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "616dd62f-cfd5-469f-aad1-cdb1aef684df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# CELL 11: Robust Data Preparation\n",
    "import time\n",
    "import traceback\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def robust_prepare_data_for_mca(acxiom_df, id_column=\"GM_PERSON_REALID\"):\n",
    "    \"\"\"\n",
    "    Prepare data for MCA analysis with improved type checking and error handling\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    acxiom_df : spark.DataFrame or pandas.DataFrame\n",
    "        DataFrame with extracted data (can be either Spark or pandas)\n",
    "    id_column : str\n",
    "        Name of the ID column\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary containing prepared data and metadata\n",
    "    \"\"\"\n",
    "    if acxiom_df is None:\n",
    "        log_status(\"ERROR: Input DataFrame is None\")\n",
    "        return None\n",
    "        \n",
    "    # Step 1: Convert to pandas for MCA processing if it's a Spark DataFrame\n",
    "    log_status(\"Preparing DataFrame for MCA analysis...\")\n",
    "    pandas_start = time.time()\n",
    "    \n",
    "    try:\n",
    "        if hasattr(acxiom_df, 'toPandas'):\n",
    "            log_status(\"Converting Spark DataFrame to pandas DataFrame...\")\n",
    "            df = acxiom_df.toPandas()\n",
    "        else:\n",
    "            log_status(\"Input is already a pandas DataFrame, using directly\")\n",
    "            df = acxiom_df\n",
    "            \n",
    "        pandas_time = time.time() - pandas_start\n",
    "        log_status(f\"DataFrame preparation complete in {pandas_time:.2f} seconds ({len(df)} rows, {len(df.columns)} columns)\")\n",
    "    except Exception as e:\n",
    "        log_status(f\"ERROR during DataFrame conversion: {str(e)}\")\n",
    "        log_status(traceback.format_exc())\n",
    "        return None\n",
    "    \n",
    "    # Step 2: Prepare data for MCA by category\n",
    "    log_status(\"Preparing data for MCA analysis with category-aware processing...\")\n",
    "    mca_prep_start = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Extract ID column\n",
    "        id_values = df[id_column].copy() if id_column in df.columns else None\n",
    "        if id_values is None:\n",
    "            log_status(f\"WARNING: ID column '{id_column}' not found, continuing without ID values\")\n",
    "        \n",
    "        # Get actual columns present in the data\n",
    "        id_column, vehicle_columns, propensity_columns, demographic_columns, lifestyle_columns, financial_columns = define_columns()\n",
    "        \n",
    "        available_vehicle_cols = [col for col in vehicle_columns if col in df.columns]\n",
    "        available_propensity_cols = [col for col in propensity_columns if col in df.columns]\n",
    "        available_demographic_cols = [col for col in demographic_columns if col in df.columns]\n",
    "        available_lifestyle_cols = [col for col in lifestyle_columns if col in df.columns]\n",
    "        available_financial_cols = [col for col in financial_columns if col in df.columns]\n",
    "        \n",
    "        # Combine all available feature columns\n",
    "        available_feature_cols = (available_vehicle_cols + available_propensity_cols + \n",
    "                                available_demographic_cols + available_lifestyle_cols + \n",
    "                                available_financial_cols)\n",
    "        \n",
    "        if len(available_feature_cols) < 5:\n",
    "            log_status(f\"ERROR: Not enough feature columns available (only {len(available_feature_cols)})\")\n",
    "            return None\n",
    "            \n",
    "        # Create a dictionary to track column categories\n",
    "        column_categories = {}\n",
    "        for col in available_vehicle_cols:\n",
    "            column_categories[col] = \"Vehicle\"\n",
    "        for col in available_propensity_cols:\n",
    "            column_categories[col] = \"Propensity\"\n",
    "        for col in available_demographic_cols:\n",
    "            column_categories[col] = \"Demographic\"\n",
    "        for col in available_lifestyle_cols:\n",
    "            column_categories[col] = \"Lifestyle\"\n",
    "        for col in available_financial_cols:\n",
    "            column_categories[col] = \"Financial\"\n",
    "        \n",
    "        # Extract features\n",
    "        features = df[available_feature_cols].copy()\n",
    "        \n",
    "        # Print column value counts for exploration\n",
    "        log_status(f\"Examining column value counts to determine categorical vs. numeric...\")\n",
    "        categorical_cols = []\n",
    "        \n",
    "        for col in available_feature_cols:\n",
    "            try:\n",
    "                # Get column category\n",
    "                category = column_categories.get(col, \"Unknown\")\n",
    "                \n",
    "                # Count unique values\n",
    "                unique_vals = features[col].nunique()\n",
    "                na_count = features[col].isna().sum()\n",
    "                \n",
    "                # Decide if categorical based on unique values\n",
    "                is_categorical = False\n",
    "                \n",
    "                # Check if already categorical/object type\n",
    "                if pd.api.types.is_object_dtype(features[col]) or pd.api.types.is_categorical_dtype(features[col]):\n",
    "                    is_categorical = True\n",
    "                    log_status(f\"- {col} ({category}): already categorical type\", False)\n",
    "                # Check if numeric with few unique values\n",
    "                elif pd.api.types.is_numeric_dtype(features[col]):\n",
    "                    if unique_vals <= 15:  # If numeric with few values, treat as categorical\n",
    "                        is_categorical = True\n",
    "                        log_status(f\"- {col} ({category}): {unique_vals} unique values - treating as categorical\", False)\n",
    "                    else:\n",
    "                        log_status(f\"- {col} ({category}): {unique_vals} unique values - binning into categories\", False)\n",
    "                        \n",
    "                        # Skip binning if column has many NaN values\n",
    "                        if na_count / len(features) > 0.5:\n",
    "                            log_status(f\"  Skipping binning due to high NaN ratio: {na_count/len(features):.2f}\", False)\n",
    "                            features[col] = features[col].astype(str)\n",
    "                            is_categorical = True\n",
    "                        else:\n",
    "                            # Only bin non-NaN values if there are enough\n",
    "                            non_na_mask = ~features[col].isna()\n",
    "                            if non_na_mask.sum() > 5:  # Need at least 5 non-NA values for binning\n",
    "                                try:\n",
    "                                    # Use quintiles for binning (ensuring we can handle duplicates)\n",
    "                                    features.loc[non_na_mask, col] = pd.qcut(\n",
    "                                        features.loc[non_na_mask, col], \n",
    "                                        q=5, \n",
    "                                        labels=False, \n",
    "                                        duplicates='drop'\n",
    "                                    )\n",
    "                                    # Fill NAs with special value\n",
    "                                    features[col] = features[col].fillna(-1)\n",
    "                                    is_categorical = True\n",
    "                                except Exception as bin_error:\n",
    "                                    # If binning fails, convert to string\n",
    "                                    log_status(f\"  Could not bin column {col}: {str(bin_error)}\", False)\n",
    "                                    features[col] = features[col].astype(str)\n",
    "                                    is_categorical = True\n",
    "                            else:\n",
    "                                # Not enough non-NA values, just convert to string\n",
    "                                features[col] = features[col].astype(str)\n",
    "                                is_categorical = True\n",
    "                else:\n",
    "                    # For non-numeric, always categorical\n",
    "                    is_categorical = True\n",
    "                    log_status(f\"- {col} ({category}): {unique_vals} unique values - treating as categorical\", False)\n",
    "                    \n",
    "                # Add to categorical list if determined to be categorical\n",
    "                if is_categorical:\n",
    "                    categorical_cols.append(col)\n",
    "                    # Convert to string to ensure MCA compatibility\n",
    "                    features[col] = features[col].astype(str)\n",
    "                \n",
    "                # Report on missing values\n",
    "                if na_count > 0:\n",
    "                    log_status(f\"  {na_count} missing values ({na_count/len(features)*100:.1f}%)\", False)\n",
    "                    \n",
    "            except Exception as col_error:\n",
    "                log_status(f\"WARNING: Error processing column {col}: {str(col_error)}\")\n",
    "                # Skip problematic column\n",
    "                continue\n",
    "        \n",
    "        # Check if we have enough categorical columns\n",
    "        if len(categorical_cols) < 3:\n",
    "            log_status(f\"ERROR: Not enough categorical columns for MCA (only {len(categorical_cols)})\")\n",
    "            return None\n",
    "        \n",
    "        # Check for columns with all NAs\n",
    "        na_counts = features[categorical_cols].isna().sum()\n",
    "        all_na_cols = na_counts[na_counts == len(features)].index.tolist()\n",
    "        \n",
    "        if all_na_cols:\n",
    "            log_status(f\"Removing {len(all_na_cols)} columns with all NAs\")\n",
    "            features = features.drop(columns=all_na_cols)\n",
    "            categorical_cols = [col for col in categorical_cols if col not in all_na_cols]\n",
    "        \n",
    "        # Final check for any remaining NAs in categorical columns\n",
    "        for col in categorical_cols:\n",
    "            if features[col].isna().any():\n",
    "                log_status(f\"Filling missing values in column {col}\")\n",
    "                features[col] = features[col].fillna(\"missing\")\n",
    "        \n",
    "        # Create dataset for MCA\n",
    "        prepared_data = {\n",
    "            'features': features,\n",
    "            'feature_cols': [col for col in features.columns],\n",
    "            'id_column': id_column,\n",
    "            'id_values': id_values,\n",
    "            'categorical_cols': categorical_cols,\n",
    "            'column_categories': column_categories\n",
    "        }\n",
    "        \n",
    "        mca_prep_time = time.time() - mca_prep_start\n",
    "        log_status(f\"Data preparation for MCA complete in {mca_prep_time:.2f} seconds with {len(categorical_cols)} categorical columns\")\n",
    "        \n",
    "        return prepared_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        log_status(f\"ERROR in data preparation: {str(e)}\")\n",
    "        log_status(traceback.format_exc())\n",
    "        return None\n",
    "\n",
    "print(\"\\n=== CELL 12: Robust Data Preparation completed ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "0c010f18-7bf6-46e8-a7ee-bc4106f83ff6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# CELL 12: K-prototypes data extraction and clustering w/o MCA\n",
    "import time\n",
    "import traceback\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "import os\n",
    "\n",
    "\n",
    "def extract_and_prepare_data_for_kprototypes(sample_size=GLOBAL_CONFIG.get('clustering_sample_size', 50000)):\n",
    "    \"\"\"\n",
    "    Extract and prepare data for K-Prototypes clustering\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    sample_size : int\n",
    "        Number of samples to extract. Defaults to GLOBAL_CONFIG['clustering_sample_size']\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary containing prepared data for k-prototypes\n",
    "    \"\"\"\n",
    "    # Validate sample size\n",
    "    if not isinstance(sample_size, int) or sample_size <= 0:\n",
    "        log_status(f\"Invalid sample size provided: {sample_size}. Using default: {GLOBAL_CONFIG.get('clustering_sample_size', 50000)}\")\n",
    "        sample_size = GLOBAL_CONFIG.get('clustering_sample_size', 50000)\n",
    "    \n",
    "    log_status(f\"Extracting and preparing data for k-prototypes, sample size: {sample_size}\")\n",
    "    \n",
    "    try:\n",
    "        # Specify the table name\n",
    "        acxiom_table = \"dataproducts_dev.bronze_acxiom.gm_consumer_list\"\n",
    "        \n",
    "        # Get column definitions with explicit error handling and logging\n",
    "        log_status(\"Calling define_columns() to get column definitions...\")\n",
    "        try:\n",
    "            id_column, vehicle_columns, propensity_columns, demographic_columns, lifestyle_columns, financial_columns = define_columns()\n",
    "            \n",
    "            # Debug logging for each column list\n",
    "            log_status(f\"ID Column: {id_column}\")\n",
    "            log_status(f\"Vehicle Columns: {vehicle_columns}\")\n",
    "            log_status(f\"Propensity Columns: {propensity_columns}\")\n",
    "            log_status(f\"Demographic Columns: {demographic_columns}\")\n",
    "            log_status(f\"Lifestyle Columns: {lifestyle_columns}\")\n",
    "            log_status(f\"Financial Columns: {financial_columns}\")\n",
    "        except Exception as def_error:\n",
    "            log_status(f\"ERROR in column definition: {str(def_error)}\")\n",
    "            log_status(f\"Error details: {traceback.format_exc()}\")\n",
    "            return None\n",
    "        \n",
    "        # Validate column lists to remove any None or invalid values\n",
    "        def clean_column_list(columns):\n",
    "            # Check if None is in the list\n",
    "            if None in columns:\n",
    "                log_status(f\"WARNING: None found in column list: {columns}\")\n",
    "            return [col for col in columns if col is not None and isinstance(col, str) and col.strip()]\n",
    "        \n",
    "        vehicle_columns = clean_column_list(vehicle_columns)\n",
    "        propensity_columns = clean_column_list(propensity_columns)\n",
    "        demographic_columns = clean_column_list(demographic_columns)\n",
    "        lifestyle_columns = clean_column_list(lifestyle_columns)\n",
    "        financial_columns = clean_column_list(financial_columns)\n",
    "        \n",
    "        # Validate ID column\n",
    "        if not id_column or not isinstance(id_column, str) or id_column.strip() == '':\n",
    "            log_status(\"ERROR: Invalid ID column\")\n",
    "            return None\n",
    "        \n",
    "        # First, get all available columns from the table\n",
    "        log_status(\"Fetching available table columns...\")\n",
    "        all_table_columns = spark.table(acxiom_table).columns\n",
    "        log_status(f\"Total columns in table: {len(all_table_columns)}\")\n",
    "        \n",
    "        # Add a check to log a sample of table columns\n",
    "        log_status(f\"First 20 table columns: {all_table_columns[:20]}\")\n",
    "        \n",
    "        # Filter columns that actually exist in the table\n",
    "        def filter_existing_columns(columns):\n",
    "            non_existing = [col for col in columns if col not in all_table_columns]\n",
    "            if non_existing:\n",
    "                log_status(f\"Columns not found in table: {non_existing}\")\n",
    "            return [col for col in columns if col in all_table_columns]\n",
    "        \n",
    "        # Filter and combine columns\n",
    "        vehicle_cols = filter_existing_columns(vehicle_columns)\n",
    "        propensity_cols = filter_existing_columns(propensity_columns)\n",
    "        demographic_cols = filter_existing_columns(demographic_columns)\n",
    "        lifestyle_cols = filter_existing_columns(lifestyle_columns)\n",
    "        financial_cols = filter_existing_columns(financial_columns)\n",
    "        \n",
    "        # Combine all valid columns, ensuring ID column is first\n",
    "        all_columns = [id_column] + vehicle_cols + propensity_cols + demographic_cols + lifestyle_cols + financial_cols\n",
    "        \n",
    "        # Remove duplicates while preserving order\n",
    "        all_columns = list(dict.fromkeys(all_columns))\n",
    "        \n",
    "        # Log the columns we're using\n",
    "        log_status(f\"Columns to extract: {all_columns}\")\n",
    "        \n",
    "        # Verify ID column exists\n",
    "        if id_column not in all_columns:\n",
    "            log_status(f\"ERROR: ID column {id_column} not found in table\")\n",
    "            return None\n",
    "        \n",
    "        # Safely quote column names\n",
    "        quoted_columns = [f\"`{col}`\" for col in all_columns]\n",
    "        column_list = \", \".join(quoted_columns)\n",
    "        \n",
    "        # Create safe SQL query with random sampling\n",
    "        sql_query = f\"\"\"\n",
    "        SELECT {column_list}\n",
    "        FROM {acxiom_table}\n",
    "        WHERE `{id_column}` IS NOT NULL\n",
    "        ORDER BY RAND()\n",
    "        LIMIT {sample_size}\n",
    "        \"\"\"\n",
    "        \n",
    "        # Log the exact SQL query\n",
    "        log_status(f\"SQL Query: {sql_query}\")\n",
    "        \n",
    "        # Execute the query\n",
    "        log_status(\"Executing SQL query to extract data...\")\n",
    "        spark_df = spark.sql(sql_query)\n",
    "        \n",
    "        # Convert to pandas\n",
    "        df = spark_df.toPandas()\n",
    "        \n",
    "        # Create category mappings\n",
    "        column_categories = {}\n",
    "        for col_list, category in [\n",
    "            (vehicle_cols, \"Vehicle\"),\n",
    "            (propensity_cols, \"Propensity\"),\n",
    "            (demographic_cols, \"Demographic\"),\n",
    "            (lifestyle_cols, \"Lifestyle\"),\n",
    "            (financial_cols, \"Financial\")\n",
    "        ]:\n",
    "            for col in col_list:\n",
    "                if col in df.columns:\n",
    "                    column_categories[col] = category\n",
    "        \n",
    "        # Prepare columns for K-Prototypes\n",
    "        feature_cols = [col for col in df.columns if col != id_column]\n",
    "        \n",
    "        # Determine categorical and numerical columns\n",
    "        categorical_cols = []\n",
    "        numerical_cols = []\n",
    "        \n",
    "        for col in feature_cols:\n",
    "            # Get unique values\n",
    "            unique_vals = df[col].nunique()\n",
    "            \n",
    "            # Categorize columns\n",
    "            if unique_vals <= 15 or pd.api.types.is_object_dtype(df[col]):\n",
    "                categorical_cols.append(col)\n",
    "                # Convert to string\n",
    "                df[col] = df[col].fillna(\"missing\").astype(str)\n",
    "            else:\n",
    "                numerical_cols.append(col)\n",
    "                # Fill NAs with median\n",
    "                df[col] = df[col].fillna(df[col].median())\n",
    "        \n",
    "        log_status(f\"Identified {len(categorical_cols)} categorical and {len(numerical_cols)} numerical columns\")\n",
    "        \n",
    "        # Verify we have at least some categorical columns for k-prototypes to be meaningful\n",
    "        if not categorical_cols:\n",
    "            log_status(\"ERROR: No categorical columns identified. K-prototypes requires categorical data.\")\n",
    "            return None\n",
    "        \n",
    "        # Return prepared data\n",
    "        return {\n",
    "            'data': df,\n",
    "            'id_column': id_column,\n",
    "            'categorical_cols': categorical_cols,\n",
    "            'numerical_cols': numerical_cols,\n",
    "            'column_categories': column_categories,\n",
    "            'row_count': len(df)\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        log_status(f\"ERROR in data extraction and preparation: {str(e)}\")\n",
    "        log_status(traceback.format_exc())\n",
    "        return None\n",
    "\n",
    "\n",
    "def run_kprototypes_clustering_standalone(sample_size=GLOBAL_CONFIG.get('clustering_sample_size', 50000), num_clusters=8, \n",
    "                                         output_prefix=\"/dbfs/FileStore/acxiom_clustering/kprototypes\", \n",
    "                                         categorical_weight=0.5, max_iterations=100):\n",
    "    \"\"\"\n",
    "    Standalone K-Prototypes clustering with no dependency on MCA or other functions\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    sample_size : int\n",
    "        Number of rows to sample\n",
    "    num_clusters : int\n",
    "        Number of clusters to generate\n",
    "    output_prefix : str\n",
    "        Prefix for output files\n",
    "    categorical_weight : float\n",
    "        Weight for categorical variables\n",
    "    max_iterations : int\n",
    "        Maximum iterations for k-prototypes algorithm\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary with k-prototypes results or None if failed\n",
    "    \"\"\"\n",
    "    log_status(f\"Starting standalone k-prototypes clustering with {num_clusters} clusters...\")\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Extract and prepare data\n",
    "        log_status(\"Step 1: Extracting and preparing data...\")\n",
    "        prepared_data = extract_and_prepare_data_for_kprototypes(sample_size)\n",
    "        \n",
    "        if prepared_data is None:\n",
    "            log_status(\"ERROR: Data preparation failed\")\n",
    "            return None\n",
    "        \n",
    "        # Get the data\n",
    "        df = prepared_data['data']\n",
    "        id_column = prepared_data['id_column']\n",
    "        categorical_cols = prepared_data['categorical_cols']\n",
    "        numerical_cols = prepared_data['numerical_cols']\n",
    "        \n",
    "        log_status(f\"Successfully prepared {len(df)} rows with {len(categorical_cols)} categorical and {len(numerical_cols)} numerical columns\")\n",
    "        \n",
    "        # Step 2: Prepare data for K-Prototypes\n",
    "        log_status(\"Step 2: Preparing arrays for K-Prototypes...\")\n",
    "        \n",
    "        # Extract numerical data and standardize if available\n",
    "        if numerical_cols:\n",
    "            numerical_df = df[numerical_cols]\n",
    "            scaler = StandardScaler()\n",
    "            numerical_data = scaler.fit_transform(numerical_df)\n",
    "        else:\n",
    "            log_status(\"No numerical columns found, proceeding with categorical data only\")\n",
    "            numerical_data = np.empty((len(df), 0))  # Empty array with zero columns\n",
    "        \n",
    "        # Extract categorical data\n",
    "        categorical_data = df[categorical_cols].values\n",
    "        \n",
    "        # Prepare categorical indices for K-Prototypes\n",
    "        categorical_indices = list(range(numerical_data.shape[1], \n",
    "                                      numerical_data.shape[1] + categorical_data.shape[1]))\n",
    "        \n",
    "        # Combine data\n",
    "        if numerical_data.shape[1] > 0:\n",
    "            combined_data = np.hstack((numerical_data, categorical_data))\n",
    "        else:\n",
    "            combined_data = categorical_data\n",
    "        \n",
    "        log_status(f\"Prepared data shape: {combined_data.shape} with {len(categorical_indices)} categorical features\")\n",
    "        \n",
    "        # Step 3: Run K-Prototypes clustering\n",
    "        log_status(f\"Step 3: Running K-Prototypes clustering with {num_clusters} clusters...\")\n",
    "        \n",
    "        # Check if kmodes package is installed\n",
    "        try:\n",
    "            from kmodes.kprototypes import KPrototypes\n",
    "        except ImportError:\n",
    "            log_status(\"Installing kmodes package...\")\n",
    "            import pip\n",
    "            pip.main(['install', 'kmodes'])\n",
    "            from kmodes.kprototypes import KPrototypes\n",
    "        \n",
    "        # Initialize and run K-Prototypes with gamma (categorical weight) during initialization\n",
    "        k_proto = KPrototypes(n_clusters=num_clusters, \n",
    "                             init='Huang', \n",
    "                             max_iter=max_iterations, \n",
    "                             n_init=5,\n",
    "                             verbose=1,\n",
    "                             gamma=categorical_weight,  # Set categorical weight here during initialization\n",
    "                             random_state=42)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Call fit_predict without gamma parameter, only with categorical indices\n",
    "        # FIXED: Removed the gamma parameter from fit_predict since it's already set during initialization\n",
    "        cluster_labels = k_proto.fit_predict(combined_data, categorical=categorical_indices)\n",
    "        \n",
    "        runtime = time.time() - start_time\n",
    "        log_status(f\"K-Prototypes clustering completed in {runtime:.2f} seconds\")\n",
    "        \n",
    "        # Step 4: Analyze results\n",
    "        log_status(\"Step 4: Analyzing clustering results...\")\n",
    "        \n",
    "        # Add cluster labels to original data\n",
    "        df_with_clusters = df.copy()\n",
    "        df_with_clusters['kproto_cluster'] = cluster_labels\n",
    "        \n",
    "        # Calculate cluster sizes\n",
    "        cluster_sizes = pd.Series(cluster_labels).value_counts().sort_index()\n",
    "        log_status(\"Cluster sizes:\")\n",
    "        for cluster_id, size in cluster_sizes.items():\n",
    "            percentage = (size / len(df_with_clusters)) * 100\n",
    "            log_status(f\"Cluster {cluster_id}: {size} records ({percentage:.1f}%)\")\n",
    "        \n",
    "        # Calculate silhouette score if we have numerical features and multiple clusters\n",
    "        silhouette = None\n",
    "        if len(np.unique(cluster_labels)) > 1 and numerical_data.shape[1] > 0:\n",
    "            try:\n",
    "                silhouette = silhouette_score(numerical_data, cluster_labels)\n",
    "                log_status(f\"Silhouette score: {silhouette:.4f}\")\n",
    "            except Exception as sil_error:\n",
    "                log_status(f\"Note: Could not calculate silhouette score: {str(sil_error)}\")\n",
    "        else:\n",
    "            log_status(\"Note: Silhouette score not calculated (requires multiple clusters and numerical features)\")\n",
    "        \n",
    "        # Create directory if it doesn't exist\n",
    "        os.makedirs(os.path.dirname(output_prefix), exist_ok=True)\n",
    "        \n",
    "        # Step 5: Visualize results\n",
    "        log_status(\"Step 5: Creating visualizations...\")\n",
    "        \n",
    "        # 5.1: Visualize cluster sizes\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        cluster_labels_str = [f\"Cluster {i}\" for i in range(num_clusters)]\n",
    "        bars = plt.bar(cluster_labels_str, cluster_sizes.values, \n",
    "                      color=plt.cm.tab20(np.linspace(0, 1, num_clusters)))\n",
    "        \n",
    "        plt.title(f\"K-Prototypes Clustering: {num_clusters} Segments Size Distribution\", fontsize=14)\n",
    "        plt.xlabel(\"Cluster\", fontsize=12)\n",
    "        plt.ylabel(\"Number of Records\", fontsize=12)\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # Add labels on bars\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            percentage = (height / len(df_with_clusters)) * 100\n",
    "            plt.text(bar.get_x() + bar.get_width()/2., height + 5,\n",
    "                    f\"{int(height)}\\n({percentage:.1f}%)\",\n",
    "                    ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        size_plot_path = f\"{output_prefix}_kproto_{num_clusters}_sizes.jpg\"\n",
    "        plt.savefig(size_plot_path, dpi=300, bbox_inches='tight')\n",
    "        \n",
    "        if IN_DATABRICKS:\n",
    "            display(plt.gcf())\n",
    "        \n",
    "        plt.close()\n",
    "        log_status(f\"Saved cluster size visualization to {size_plot_path}\")\n",
    "        \n",
    "        # 5.2: Visualize clusters in 2D space (if we have at least 2 numerical features)\n",
    "        if len(numerical_cols) >= 2:\n",
    "            # Select two numerical features for visualization\n",
    "            viz_cols = numerical_cols[:2]\n",
    "            \n",
    "            plt.figure(figsize=(12, 10))\n",
    "            \n",
    "            # Plot data points colored by cluster\n",
    "            scatter = plt.scatter(\n",
    "                df[viz_cols[0]],\n",
    "                df[viz_cols[1]],\n",
    "                c=cluster_labels,\n",
    "                cmap=plt.cm.tab20,\n",
    "                alpha=0.7,\n",
    "                s=40,\n",
    "                edgecolors='w',\n",
    "                linewidths=0.3\n",
    "            )\n",
    "            \n",
    "           \n",
    "            # Add cluster centroids\n",
    "            for i in range(num_clusters):\n",
    "                mask = df_with_clusters['kproto_cluster'] == i\n",
    "                center_x = df.loc[mask, viz_cols[0]].mean()\n",
    "                center_y = df.loc[mask, viz_cols[1]].mean()\n",
    "                \n",
    "                plt.scatter(center_x, center_y, \n",
    "                           s=200, marker='*', color='black', \n",
    "                           edgecolor='white', linewidth=1.5)\n",
    "                \n",
    "                plt.text(center_x, center_y, str(i), \n",
    "                        fontsize=14, ha='center', va='center', \n",
    "                        color='white', fontweight='bold')\n",
    "            \n",
    "            plt.colorbar(scatter, label=\"Cluster\")\n",
    "            plt.xlabel(viz_cols[0], fontsize=12)\n",
    "            plt.ylabel(viz_cols[1], fontsize=12)\n",
    "            plt.title(f\"K-Prototypes Clustering: {num_clusters} Segments\", fontsize=14)\n",
    "            plt.grid(alpha=0.3)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            cluster_plot_path = f\"{output_prefix}_kproto_{num_clusters}_clusters.jpg\"\n",
    "            plt.savefig(cluster_plot_path, dpi=300, bbox_inches='tight')\n",
    "            \n",
    "            if IN_DATABRICKS:\n",
    "                display(plt.gcf())\n",
    "            \n",
    "            plt.close()\n",
    "            log_status(f\"Saved cluster visualization to {cluster_plot_path}\")\n",
    "        \n",
    "        # Step 6: Save results\n",
    "        log_status(\"Step 6: Saving results...\")\n",
    "        \n",
    "        # Save clustering results\n",
    "        results_path = f\"{output_prefix}_kproto_{num_clusters}_results.csv\"\n",
    "        df_with_clusters.to_csv(results_path, index=False)\n",
    "        log_status(f\"Saved clustering results to {results_path}\")\n",
    "        \n",
    "        # Return results dictionary\n",
    "        return {\n",
    "            'num_clusters': num_clusters,\n",
    "            'silhouette_score': silhouette,\n",
    "            'cluster_sizes': cluster_sizes,\n",
    "            'cluster_data': df_with_clusters,\n",
    "            'categorical_cols': categorical_cols,\n",
    "            'numerical_cols': numerical_cols,\n",
    "            'runtime': runtime\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        log_status(f\"ERROR in k-prototypes clustering: {str(e)}\")\n",
    "        log_status(traceback.format_exc())\n",
    "        return None\n",
    "\n",
    "def run_kprototypes_multiple_standalone(cluster_counts=[8, 10, 12], sample_size=GLOBAL_CONFIG.get('clustering_sample_size', 50000), categorical_weight=0.5):\n",
    "    \"\"\"\n",
    "    Run k-prototypes with multiple cluster counts and compare results\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    cluster_counts : list\n",
    "        List of cluster counts to try\n",
    "    sample_size : int\n",
    "        Sample size to use\n",
    "    categorical_weight : float\n",
    "        Weight for categorical variables\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary with results for each cluster count\n",
    "    \"\"\"\n",
    "    log_status(f\"Running k-prototypes with multiple cluster counts: {cluster_counts}\")\n",
    "    \n",
    "    results = {}\n",
    "    silhouette_scores = {}\n",
    "    \n",
    "    for k in cluster_counts:\n",
    "        log_status(f\"Running k-prototypes with {k} clusters...\")\n",
    "        \n",
    "        result = run_kprototypes_clustering_standalone(\n",
    "            sample_size=sample_size,\n",
    "            num_clusters=k,\n",
    "            categorical_weight=categorical_weight\n",
    "        )\n",
    "        \n",
    "        if result:\n",
    "            results[k] = result\n",
    "            if result['silhouette_score'] is not None:\n",
    "                silhouette_scores[k] = result['silhouette_score']\n",
    "                \n",
    "            log_status(f\"Successfully completed k-prototypes with {k} clusters\")\n",
    "        else:\n",
    "            log_status(f\"Failed to run k-prototypes with {k} clusters\")\n",
    "    \n",
    "    # If we have multiple results with silhouette scores, create comparison chart\n",
    "    if len(silhouette_scores) > 1:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        x = list(silhouette_scores.keys())\n",
    "        y = list(silhouette_scores.values())\n",
    "        \n",
    "        plt.bar(\n",
    "            [str(k) for k in x],\n",
    "            y,\n",
    "            color=plt.cm.viridis(np.linspace(0.2, 0.8, len(x)))\n",
    "        )\n",
    "        \n",
    "        # Find optimal k\n",
    "        optimal_k = max(silhouette_scores, key=silhouette_scores.get)\n",
    "        optimal_score = silhouette_scores[optimal_k]\n",
    "        \n",
    "        plt.axhline(y=optimal_score, color='red', linestyle='--',\n",
    "                   label=f'Best Score: {optimal_score:.4f} (k={optimal_k})')\n",
    "        \n",
    "        plt.xlabel('Number of Clusters (k)', fontsize=12)\n",
    "        plt.ylabel('Silhouette Score', fontsize=12)\n",
    "        plt.title('Comparison of K-Prototypes Clustering Results', fontsize=14)\n",
    "        plt.grid(axis='y', alpha=0.3)\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        comparison_path = \"/dbfs/FileStore/acxiom_clustering/kproto_comparison.jpg\"\n",
    "        plt.savefig(comparison_path, dpi=300, bbox_inches='tight')\n",
    "        \n",
    "        if IN_DATABRICKS:\n",
    "            display(plt.gcf())\n",
    "        \n",
    "        plt.close()\n",
    "        log_status(f\"Saved comparison chart to {comparison_path}\")\n",
    "        \n",
    "        # Create a comparison report\n",
    "        report = f\"===== K-PROTOTYPES CLUSTERING COMPARISON =====\\n\\n\"\n",
    "        report += f\"Date: {time.strftime('%Y-%m-%d %H:%M:%S')}\\n\"\n",
    "        report += f\"Sample size: {sample_size}\\n\"\n",
    "        report += f\"Categorical weight: {categorical_weight}\\n\\n\"\n",
    "        \n",
    "        report += \"SILHOUETTE SCORES:\\n\"\n",
    "        report += \"-----------------\\n\"\n",
    "        for k in sorted(silhouette_scores.keys()):\n",
    "            score = silhouette_scores[k]\n",
    "            relative = score / optimal_score\n",
    "            report += f\"k={k}: {score:.4f} ({relative:.1%} of optimal)\\n\"\n",
    "        \n",
    "        report += f\"\\nOptimal cluster count: k={optimal_k} (silhouette={optimal_score:.4f})\\n\"\n",
    "        \n",
    "        report_path = \"/dbfs/FileStore/acxiom_clustering/kproto_comparison_report.txt\"\n",
    "        with open(report_path, \"w\") as f:\n",
    "            f.write(report)\n",
    "        \n",
    "        log_status(f\"Saved comparison report to {report_path}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Standalone function for easy execution\n",
    "def run_just_kprototypes(cluster_counts=GLOBAL_CONFIG.get('kmeans_clusters', [8, 10, 12]), sample_size=GLOBAL_CONFIG.get('clustering_sample_size', 50000), categorical_weight=0.5):\n",
    "    \"\"\"\n",
    "    Run standalone K-Prototypes clustering with flexible parameters\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    cluster_counts : list, optional\n",
    "        List of cluster counts to generate (defaults to [8, 10, 12])\n",
    "    sample_size : int, optional\n",
    "        Sample size to use (defaults to global config)\n",
    "    categorical_weight : float, optional\n",
    "        Weight for categorical variables (default 0.5)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    bool\n",
    "        True if successful, False otherwise\n",
    "    \"\"\"\n",
    "    # Use default cluster counts if not specified\n",
    "    if cluster_counts is None:\n",
    "        cluster_counts = GLOBAL_CONFIG.get('kmeans_clusters', [8, 10, 12])\n",
    "    \n",
    "    log_status(f\"===== EXECUTING K-PROTOTYPES CLUSTERING =====\")\n",
    "    log_status(f\"Cluster counts: {cluster_counts}\")\n",
    "    log_status(f\"Sample size: {sample_size if sample_size is not None else 'Default from Global Config'}\")\n",
    "    log_status(f\"Categorical weight: {categorical_weight}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Run k-prototypes with multiple cluster counts\n",
    "        results = run_kprototypes_multiple_standalone(\n",
    "            cluster_counts=cluster_counts,\n",
    "            sample_size=sample_size,\n",
    "            categorical_weight=categorical_weight\n",
    "        )\n",
    "        \n",
    "        end_time = time.time()\n",
    "        total_time = end_time - start_time\n",
    "        \n",
    "        if results:\n",
    "            log_status(f\"✅ K-Prototypes clustering completed successfully in {total_time:.2f} seconds\")\n",
    "            \n",
    "            # Optional: Log detailed results\n",
    "            for k, result in results.items():\n",
    "                log_status(f\"Cluster {k}: {len(result['cluster_data'])} records\")\n",
    "                if 'silhouette_score' in result and result['silhouette_score'] is not None:\n",
    "                    log_status(f\"  Silhouette Score: {result['silhouette_score']:.4f}\")\n",
    "            \n",
    "            return True\n",
    "        else:\n",
    "            log_status(f\"❌ K-Prototypes clustering failed after {total_time:.2f} seconds\")\n",
    "            return False\n",
    "    \n",
    "    except Exception as e:\n",
    "        log_status(f\"ERROR in k-prototypes clustering: {str(e)}\")\n",
    "        log_status(traceback.format_exc())\n",
    "        return False\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "d4cdf3ec-41f5-4bb1-a381-77ed85a932a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# CELL 12.1 K-prototypes with marketing report\n",
    "def run_kprototypes_with_marketing_report(cluster_counts=None, sample_size=None, categorical_weight=0.5):\n",
    "    \"\"\"\n",
    "    Run K-prototypes clustering with comprehensive marketing report generation\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    cluster_counts : list, optional\n",
    "        List of cluster counts to try (default [8, 10, 12])\n",
    "    sample_size : int, optional\n",
    "        Sample size to use for clustering\n",
    "    categorical_weight : float, optional\n",
    "        Weight for categorical variables (default 0.5)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    bool\n",
    "        True if successful, False otherwise\n",
    "    \"\"\"\n",
    "    # Use default values if not specified\n",
    "    if cluster_counts is None:\n",
    "        cluster_counts = [8, 10, 12]\n",
    "    \n",
    "    if sample_size is None:\n",
    "        sample_size = GLOBAL_CONFIG.get('clustering_sample_size', 50000)\n",
    "    \n",
    "    log_status(f\"===== EXECUTING K-PROTOTYPES CLUSTERING WITH MARKETING REPORT =====\")\n",
    "    log_status(f\"Cluster counts: {cluster_counts}\")\n",
    "    log_status(f\"Sample size: {sample_size}\")\n",
    "    log_status(f\"Categorical weight: {categorical_weight}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # First check if kmodes package is available (k-prototypes is in the same package)\n",
    "        if not install_kmodes_if_needed():\n",
    "            log_status(\"ERROR: Required kmodes package not available\")\n",
    "            return False\n",
    "        \n",
    "        # Run K-prototypes with multiple cluster counts\n",
    "        results = {}\n",
    "        cost_values = {}\n",
    "        \n",
    "        # Extract and prepare data (do this only once)\n",
    "        prepared_data = extract_and_prepare_data_for_kprototypes(sample_size)\n",
    "        if not prepared_data:\n",
    "            log_status(\"ERROR: Failed to prepare data for K-prototypes clustering\")\n",
    "            return False\n",
    "        \n",
    "        # Run K-prototypes for each cluster count\n",
    "        for k in cluster_counts:\n",
    "            log_status(f\"Running K-prototypes with {k} clusters...\")\n",
    "            \n",
    "            result = run_kprototypes_clustering_standalone(\n",
    "                sample_size=sample_size,\n",
    "                num_clusters=k,\n",
    "                categorical_weight=categorical_weight\n",
    "            )\n",
    "            \n",
    "            if result:\n",
    "                results[k] = result\n",
    "                cost_values[k] = result.get('cost', float('inf'))  # Use a default if cost not available\n",
    "                log_status(f\"Successfully completed K-prototypes with {k} clusters\")\n",
    "            else:\n",
    "                log_status(f\"Failed to run K-prototypes with {k} clusters\")\n",
    "                \n",
    "        if not results:\n",
    "            log_status(\"ERROR: All K-prototypes clustering attempts failed\")\n",
    "            return False\n",
    "                \n",
    "        # Choose best clustering (some measure of quality)\n",
    "        # For K-prototypes, use silhouette score if available\n",
    "        best_scores = {}\n",
    "        for k, result in results.items():\n",
    "            if 'silhouette_score' in result and result['silhouette_score'] is not None:\n",
    "                best_scores[k] = result['silhouette_score']\n",
    "        \n",
    "        # Use silhouette scores if available, otherwise use cost values\n",
    "        if best_scores:\n",
    "            best_k = max(best_scores, key=best_scores.get)\n",
    "            log_status(f\"Selected optimal clustering with k={best_k} clusters (best silhouette score)\")\n",
    "        else:\n",
    "            # For cost values, lower is better\n",
    "            best_k = min(cost_values, key=cost_values.get)\n",
    "            log_status(f\"Selected optimal clustering with k={best_k} clusters (lowest cost)\")\n",
    "            \n",
    "        # Get the best result\n",
    "        best_result = results[best_k]\n",
    "        \n",
    "        # Adapt format for marketing report generation\n",
    "        kprototypes_for_marketing = {\n",
    "            'cluster_data': best_result['cluster_data'],\n",
    "            'centroids_df': pd.DataFrame({\n",
    "                'cluster_id': range(best_k),\n",
    "                # Add dummy columns to match expected format\n",
    "                **{col: ['NA'] * best_k for col in prepared_data['categorical_cols'][:5]}\n",
    "            }),\n",
    "            'num_clusters': best_k\n",
    "        }\n",
    "        \n",
    "        # Generate marketing report\n",
    "        report_path = generate_kmodes_marketing_report(kprototypes_for_marketing, \n",
    "                                        output_prefix=f\"/dbfs/FileStore/acxiom_clustering/kprototypes\")\n",
    "        \n",
    "        end_time = time.time()\n",
    "        total_time = end_time - start_time\n",
    "        \n",
    "        if report_path:\n",
    "            log_status(f\"✅ K-prototypes clustering and marketing report completed in {total_time:.2f} seconds\")\n",
    "            log_status(f\"Marketing report saved to: {report_path}\")\n",
    "            return True\n",
    "        else:\n",
    "            log_status(f\"❌ Marketing report generation failed after {total_time:.2f} seconds\")\n",
    "            return False\n",
    "        \n",
    "    except Exception as e:\n",
    "        log_status(f\"ERROR in K-prototypes clustering with marketing report: {str(e)}\")\n",
    "        log_status(traceback.format_exc())\n",
    "        return False\n",
    "\n",
    "# Function to generate segment names specifically for K-prototypes results\n",
    "def generate_kprototypes_segment_names(kproto_results, column_categories=None):\n",
    "    \"\"\"\n",
    "    Generate descriptive names for K-prototypes clusters\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    kproto_results : dict\n",
    "        Dictionary with K-prototypes results\n",
    "    column_categories : dict, optional\n",
    "        Dictionary mapping columns to categories\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary mapping cluster IDs to generated names\n",
    "    \"\"\"\n",
    "    log_status(\"Generating descriptive segment names for K-prototypes clusters...\")\n",
    "    \n",
    "    try:\n",
    "        # Extract cluster data and numerical features\n",
    "        cluster_data = kproto_results['cluster_data']\n",
    "        num_clusters = kproto_results['num_clusters']\n",
    "        \n",
    "        # Try to get numerical columns\n",
    "        numerical_cols = []\n",
    "        if 'numerical_cols' in kproto_results:\n",
    "            numerical_cols = kproto_results['numerical_cols']\n",
    "        \n",
    "        # Initialize segment names dictionary\n",
    "        segment_names = {}\n",
    "        \n",
    "        # Get cluster sizes for reference\n",
    "        if 'kmeans_cluster' in cluster_data.columns:\n",
    "            cluster_col = 'kmeans_cluster'\n",
    "        elif 'kproto_cluster' in cluster_data.columns:\n",
    "            cluster_col = 'kproto_cluster'\n",
    "        else:\n",
    "            # Try to find any column that might be the cluster column\n",
    "            potential_cluster_cols = [col for col in cluster_data.columns \n",
    "                                    if 'cluster' in col.lower() and col != 'cluster_id']\n",
    "            if potential_cluster_cols:\n",
    "                cluster_col = potential_cluster_cols[0]\n",
    "            else:\n",
    "                log_status(\"WARNING: Could not identify cluster column in results\")\n",
    "                # Create generic names and return\n",
    "                return {i: f\"Vehicle Segment {i+1}\" for i in range(num_clusters)}\n",
    "        \n",
    "        # Calculate cluster sizes as percentages\n",
    "        cluster_sizes = cluster_data[cluster_col].value_counts(normalize=True) * 100\n",
    "        \n",
    "        # For each cluster, analyze characteristics\n",
    "        for cluster_id in range(num_clusters):\n",
    "            # Get cluster records\n",
    "            cluster_mask = cluster_data[cluster_col] == cluster_id\n",
    "            cluster_records = cluster_data[cluster_mask]\n",
    "            \n",
    "            # Skip if no records\n",
    "            if len(cluster_records) == 0:\n",
    "                segment_names[cluster_id] = f\"Vehicle Segment {cluster_id+1}\"\n",
    "                continue\n",
    "            \n",
    "            # Analyze numerical features if available\n",
    "            numerical_profile = {}\n",
    "            if numerical_cols:\n",
    "                for col in numerical_cols:\n",
    "                    if col in cluster_records.columns:\n",
    "                        col_mean = cluster_records[col].mean()\n",
    "                        all_mean = cluster_data[col].mean()\n",
    "                        col_diff = col_mean - all_mean\n",
    "                        numerical_profile[col] = {\n",
    "                            'mean': col_mean,\n",
    "                            'diff': col_diff,\n",
    "                            'percentile': np.percentile(cluster_data[col], \n",
    "                                                       [25, 50, 75])\n",
    "                        }\n",
    "            \n",
    "            # Determine key characteristics\n",
    "            luxury_score = 0\n",
    "            value_score = 0\n",
    "            tech_score = 0\n",
    "            family_score = 0\n",
    "            utility_score = 0\n",
    "            \n",
    "            # Use numerical profiles to infer characteristics\n",
    "            for col, stats in numerical_profile.items():\n",
    "                col_lower = col.lower()\n",
    "                diff = stats['diff']\n",
    "                \n",
    "                # Infer luxury from income or premium indicators\n",
    "                if 'income' in col_lower or 'premium' in col_lower or 'luxury' in col_lower:\n",
    "                    if diff > 0:\n",
    "                        luxury_score += diff / all_mean * 5\n",
    "                    else:\n",
    "                        value_score -= diff / all_mean * 5\n",
    "                \n",
    "                # Infer tech adoption\n",
    "                if 'tech' in col_lower or 'digital' in col_lower or 'connected' in col_lower:\n",
    "                    if diff > 0:\n",
    "                        tech_score += diff / all_mean * 5\n",
    "                \n",
    "                # Infer family focus\n",
    "                if 'family' in col_lower or 'children' in col_lower or 'household' in col_lower:\n",
    "                    if diff > 0:\n",
    "                        family_score += diff / all_mean * 5\n",
    "                \n",
    "                # Infer utility priority\n",
    "                if 'utility' in col_lower or 'cargo' in col_lower or 'practical' in col_lower:\n",
    "                    if diff > 0:\n",
    "                        utility_score += diff / all_mean * 5\n",
    "            \n",
    "            # Size determination\n",
    "            size_pct = cluster_sizes.get(cluster_id, 0)\n",
    "            if size_pct > 20:\n",
    "                size_term = \"Mainstream\"\n",
    "            elif size_pct > 10:\n",
    "                size_term = \"Major\"\n",
    "            elif size_pct > 3:\n",
    "                size_term = \"Niche\"\n",
    "            else:\n",
    "                size_term = \"Specialty\"\n",
    "            \n",
    "            # Generate name based on scores\n",
    "            primary_traits = []\n",
    "            \n",
    "            if luxury_score > 5:\n",
    "                if luxury_score > 10:\n",
    "                    primary_traits.append(\"Premium Luxury\")\n",
    "                else:\n",
    "                    primary_traits.append(\"Upscale\")\n",
    "            \n",
    "            if tech_score > 5:\n",
    "                primary_traits.append(\"Tech-Forward\")\n",
    "            \n",
    "            if value_score > 5:\n",
    "                primary_traits.append(\"Value-Oriented\")\n",
    "            \n",
    "            if family_score > 5:\n",
    "                primary_traits.append(\"Family\")\n",
    "            \n",
    "            if utility_score > 5:\n",
    "                primary_traits.append(\"Utility\")\n",
    "            \n",
    "            # If no clear traits, use size term\n",
    "            if not primary_traits:\n",
    "                if size_pct > 15:\n",
    "                    primary_traits.append(\"Mainstream\")\n",
    "                elif luxury_score < -5:\n",
    "                    primary_traits.append(\"Economy\")\n",
    "                else:\n",
    "                    primary_traits.append(\"Standard\")\n",
    "            \n",
    "            # Limit to max 2 primary traits\n",
    "            primary_traits = primary_traits[:2]\n",
    "            \n",
    "            # Generate name\n",
    "            name_parts = []\n",
    "            name_parts.extend(primary_traits)\n",
    "            \n",
    "            # Add vehicle term\n",
    "            name_parts.append(\"Vehicle\")\n",
    "            \n",
    "            # Add buyer/segment term\n",
    "            if family_score > 5:\n",
    "                name_parts.append(\"Owners\")\n",
    "            elif luxury_score > 5:\n",
    "                name_parts.append(\"Enthusiasts\")\n",
    "            elif tech_score > 5:\n",
    "                name_parts.append(\"Early Adopters\")\n",
    "            else:\n",
    "                name_parts.append(\"Buyers\")\n",
    "            \n",
    "            # Create final name\n",
    "            segment_names[cluster_id] = \" \".join(name_parts)\n",
    "        \n",
    "        return segment_names\n",
    "    \n",
    "    except Exception as e:\n",
    "        log_status(f\"ERROR in segment name generation: {str(e)}\")\n",
    "        log_status(traceback.format_exc())\n",
    "        # Return generic names as fallback\n",
    "        return {i: f\"Vehicle Segment {i+1}\" for i in range(num_clusters)}\n",
    "\n",
    "# Function to analyze K-prototypes results for marketing insights\n",
    "def analyze_kprototypes_segments(cluster_data, centroids, k, numerical_cols=None, categorical_cols=None):\n",
    "    \"\"\"\n",
    "    Analyze K-prototypes clusters to extract marketing-relevant insights\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    cluster_data : pandas.DataFrame\n",
    "        DataFrame with cluster assignments\n",
    "    centroids : pandas.DataFrame\n",
    "        DataFrame with cluster centroids (or a placeholder)\n",
    "    k : int\n",
    "        Number of clusters\n",
    "    numerical_cols : list, optional\n",
    "        List of numerical columns used in clustering\n",
    "    categorical_cols : list, optional\n",
    "        List of categorical columns used in clustering\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary containing segment insights\n",
    "    \"\"\"\n",
    "    log_status(\"Analyzing K-prototypes clusters for marketing insights...\")\n",
    "    \n",
    "    # Identify cluster column\n",
    "    cluster_col = None\n",
    "    for col in cluster_data.columns:\n",
    "        if 'cluster' in col.lower() and col != 'cluster_id':\n",
    "            cluster_col = col\n",
    "            break\n",
    "    \n",
    "    if not cluster_col:\n",
    "        log_status(\"ERROR: Could not identify cluster column in data\")\n",
    "        return None\n",
    "    \n",
    "    # Calculate cluster sizes\n",
    "    cluster_counts = cluster_data[cluster_col].value_counts().sort_index()\n",
    "    cluster_sizes = [(count / len(cluster_data)) * 100 for count in cluster_counts]\n",
    "    \n",
    "    # Initialize insights dictionary\n",
    "    segment_insights = {\n",
    "        'k': k,\n",
    "        'cluster_sizes': cluster_sizes,\n",
    "        'segments': {}\n",
    "    }\n",
    "    \n",
    "    # Generate segment names\n",
    "    segment_names = generate_kprototypes_segment_names({\n",
    "        'cluster_data': cluster_data,\n",
    "        'num_clusters': k,\n",
    "        'numerical_cols': numerical_cols\n",
    "    })\n",
    "    \n",
    "    # Process each cluster\n",
    "    for cluster_id in range(k):\n",
    "        # Skip if no members in this cluster\n",
    "        if cluster_id not in cluster_data[cluster_col].unique():\n",
    "            continue\n",
    "            \n",
    "        # Get cluster records\n",
    "        cluster_records = cluster_data[cluster_data[cluster_col] == cluster_id]\n",
    "        \n",
    "        # Calculate cluster size percentage\n",
    "        size_pct = segment_insights['cluster_sizes'][cluster_id] if cluster_id < len(segment_insights['cluster_sizes']) else 0\n",
    "        \n",
    "        # Get segment name\n",
    "        segment_name = segment_names.get(cluster_id, f\"Vehicle Segment {cluster_id+1}\")\n",
    "        \n",
    "        # Create synthetic key features based on cluster analysis\n",
    "        # This is a placeholder - in a real implementation, you would analyze the actual data\n",
    "        key_features = {\n",
    "            'luxury_orientation': 50,  # Default mid-point\n",
    "            'price_sensitivity': 50,\n",
    "            'tech_adoption': 50,\n",
    "            'brand_loyalty': 50,\n",
    "            'family_focus': 50,\n",
    "            'utility_priority': 50\n",
    "        }\n",
    "        \n",
    "        # Adjust key features based on segment name\n",
    "        if \"Premium\" in segment_name or \"Luxury\" in segment_name:\n",
    "            key_features['luxury_orientation'] = 85\n",
    "            key_features['price_sensitivity'] = 20\n",
    "        elif \"Value\" in segment_name or \"Economy\" in segment_name:\n",
    "            key_features['luxury_orientation'] = 25\n",
    "            key_features['price_sensitivity'] = 85\n",
    "        \n",
    "        if \"Tech\" in segment_name:\n",
    "            key_features['tech_adoption'] = 85\n",
    "        \n",
    "        if \"Family\" in segment_name:\n",
    "            key_features['family_focus'] = 85\n",
    "        \n",
    "        if \"Utility\" in segment_name:\n",
    "            key_features['utility_priority'] = 85\n",
    "        \n",
    "        # Create synthetic demographics based on cluster analysis\n",
    "        demographics = {\n",
    "            'age': \"18-34\" if \"Tech\" in segment_name else\n",
    "                  \"35-54\" if \"Family\" in segment_name else\n",
    "                  \"55+\" if \"Traditional\" in segment_name else \"Mixed\",\n",
    "                  \n",
    "            'income': \"Upper-middle to High\" if key_features['luxury_orientation'] > 70 else\n",
    "                     \"Middle to Upper-middle\" if key_features['luxury_orientation'] > 50 else\n",
    "                     \"Low to Middle\",\n",
    "                     \n",
    "            'gender_split': \"Not available\",\n",
    "            'education': \"Not available\",\n",
    "            'geography': \"Not available\"\n",
    "        }\n",
    "        \n",
    "        # Create synthetic vehicle preferences\n",
    "        vehicle_preferences = []\n",
    "        \n",
    "        if key_features['luxury_orientation'] > 70:\n",
    "            vehicle_preferences.append(\"Luxury vehicles with premium features\")\n",
    "            if key_features['tech_adoption'] > 70:\n",
    "                vehicle_preferences.append(\"Premium models with advanced technology\")\n",
    "            if key_features['family_focus'] > 70:\n",
    "                vehicle_preferences.append(\"Premium SUVs and family vehicles\")\n",
    "        elif key_features['family_focus'] > 70:\n",
    "            vehicle_preferences.append(\"Family-oriented vehicles with versatile space\")\n",
    "            if key_features['utility_priority'] > 70:\n",
    "                vehicle_preferences.append(\"SUVs and crossovers with ample cargo space\")\n",
    "            else:\n",
    "                vehicle_preferences.append(\"Minivans and family sedans\")\n",
    "        elif key_features['utility_priority'] > 70:\n",
    "            vehicle_preferences.append(\"Trucks and utility vehicles\")\n",
    "            vehicle_preferences.append(\"Vehicles with high cargo capacity\")\n",
    "        elif key_features['price_sensitivity'] > 70:\n",
    "            vehicle_preferences.append(\"Economical vehicles with good fuel efficiency\")\n",
    "            vehicle_preferences.append(\"Value-focused models with essential features\")\n",
    "        \n",
    "        if key_features['tech_adoption'] > 70 and \"Tech\" in segment_name:\n",
    "            vehicle_preferences.append(\"Vehicles with latest technology features\")\n",
    "            vehicle_preferences.append(\"Connected cars with digital integration\")\n",
    "        \n",
    "        # Ensure we have at least some preferences\n",
    "        if not vehicle_preferences:\n",
    "            vehicle_preferences = [\"Standard vehicles\", \"Mixed preferences\"]\n",
    "        \n",
    "        # Create synthetic buying behavior\n",
    "        buying_behavior = []\n",
    "        \n",
    "        if key_features['price_sensitivity'] > 70:\n",
    "            buying_behavior.append(\"Price-sensitive purchasing decisions\")\n",
    "            buying_behavior.append(\"Value-focused comparison shopping\")\n",
    "        elif key_features['luxury_orientation'] > 70:\n",
    "            buying_behavior.append(\"Quality-focused over price-sensitive\")\n",
    "            buying_behavior.append(\"Prefers premium buying experience\")\n",
    "        \n",
    "        if key_features['tech_adoption'] > 70:\n",
    "            buying_behavior.append(\"Research-intensive purchase process\")\n",
    "            buying_behavior.append(\"Digital-first research approach\")\n",
    "        \n",
    "        if key_features['brand_loyalty'] > 70:\n",
    "            buying_behavior.append(\"Brand-loyal purchase decisions\")\n",
    "        \n",
    "        # Ensure we have at least some behaviors\n",
    "        if not buying_behavior:\n",
    "            buying_behavior = [\"Standard purchasing process\", \"Mixed buying patterns\"]\n",
    "        \n",
    "        # Generate marketing recommendations\n",
    "        marketing_strategy = generate_marketing_recommendations(\n",
    "            key_features, \n",
    "            demographics, \n",
    "            vehicle_preferences, \n",
    "            buying_behavior,\n",
    "            size_pct\n",
    "        )\n",
    "        \n",
    "        # Store segment insights\n",
    "        segment_insights['segments'][cluster_id] = {\n",
    "            'name': segment_name,\n",
    "            'size_pct': size_pct,\n",
    "            'key_features': key_features,\n",
    "            'demographics': demographics,\n",
    "            'vehicle_preferences': vehicle_preferences,\n",
    "            'buying_behavior': buying_behavior,\n",
    "            'marketing_strategy': marketing_strategy\n",
    "        }\n",
    "    \n",
    "    return segment_insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "637e7991-7728-4668-8959-a88fa4f13ae5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# CELL 13: K-modes Clustering Implementation for Categorical Data\n",
    "import time\n",
    "import traceback\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, HTML\n",
    "import os\n",
    "\n",
    "def install_kmodes_if_needed():\n",
    "    \"\"\"\n",
    "    Check if kmodes package is installed, and install it if not.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import kmodes\n",
    "        log_status(f\"kmodes package found (version: {kmodes.__version__})\")\n",
    "        return True\n",
    "    except ImportError:\n",
    "        log_status(\"kmodes package not found. Attempting to install...\")\n",
    "        try:\n",
    "            import pip\n",
    "            pip.main(['install', 'kmodes'])\n",
    "            import kmodes\n",
    "            log_status(f\"Successfully installed kmodes package (version: {kmodes.__version__})\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            log_status(f\"ERROR: Failed to install kmodes package: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "def extract_and_prepare_data_for_kmodes(sample_size=GLOBAL_CONFIG.get('clustering_sample_size', 50000)):\n",
    "    \"\"\"\n",
    "    Extract and prepare data specifically for K-modes clustering\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    sample_size : int\n",
    "        Number of samples to extract\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary containing prepared categorical data for K-modes\n",
    "    \"\"\"\n",
    "    log_status(f\"Extracting and preparing data for K-modes, sample size: {sample_size}\")\n",
    "    \n",
    "    try:\n",
    "        # Get column definitions\n",
    "        id_column, vehicle_columns, propensity_columns, demographic_columns, lifestyle_columns, financial_columns = define_columns()\n",
    "        \n",
    "        # Combine all columns to extract\n",
    "        all_columns = [id_column] + vehicle_columns + propensity_columns + demographic_columns + lifestyle_columns + financial_columns\n",
    "        \n",
    "        # Remove any duplicates while preserving order\n",
    "        all_columns = list(dict.fromkeys(all_columns))\n",
    "        \n",
    "        # Specify the table name\n",
    "        acxiom_table = \"dataproducts_dev.bronze_acxiom.gm_consumer_list\"\n",
    "        \n",
    "        # Verify table access\n",
    "        log_status(\"Verifying database access...\")\n",
    "        try:\n",
    "            spark.sql(f\"SELECT 1 FROM {acxiom_table} LIMIT 1\")\n",
    "            log_status(\"Database access verified\")\n",
    "        except Exception as e:\n",
    "            log_status(f\"ERROR: Cannot access Acxiom table: {str(e)}\")\n",
    "            return None\n",
    "        \n",
    "        # Verify column existence\n",
    "        sample_df = spark.sql(f\"SELECT * FROM {acxiom_table} LIMIT 1\")\n",
    "        available_columns = set(sample_df.columns)\n",
    "        \n",
    "        # Filter for only available columns\n",
    "        valid_columns = [col for col in all_columns if col in available_columns]\n",
    "        \n",
    "        # Report on missing columns\n",
    "        missing_columns = set(all_columns) - set(valid_columns)\n",
    "        if missing_columns:\n",
    "            log_status(f\"WARNING: {len(missing_columns)} columns not found in dataset\")\n",
    "        \n",
    "        # Ensure we have enough columns\n",
    "        if len(valid_columns) < 10:\n",
    "            log_status(f\"ERROR: Not enough valid columns found (only {len(valid_columns)})\")\n",
    "            return None\n",
    "        \n",
    "        # Build column list for query\n",
    "        column_list = \", \".join(valid_columns)\n",
    "        \n",
    "        # Create simple query with random sampling\n",
    "        log_status(\"Executing data extraction query...\")\n",
    "        sql_query = f\"\"\"\n",
    "        SELECT {column_list}\n",
    "        FROM {acxiom_table}\n",
    "        WHERE {id_column} IS NOT NULL\n",
    "        ORDER BY rand()\n",
    "        LIMIT {sample_size}\n",
    "        \"\"\"\n",
    "        \n",
    "        # Execute the query\n",
    "        spark_df = spark.sql(sql_query)\n",
    "        \n",
    "        # Check if we got enough data\n",
    "        row_count = spark_df.count()\n",
    "        if row_count == 0:\n",
    "            log_status(\"ERROR: No rows returned from query\")\n",
    "            return None\n",
    "        \n",
    "        log_status(f\"Successfully extracted {row_count} rows\")\n",
    "        \n",
    "        # Convert to pandas for processing\n",
    "        log_status(\"Converting to pandas DataFrame...\")\n",
    "        df = spark_df.toPandas()\n",
    "        \n",
    "        # Create category mappings\n",
    "        column_categories = {}\n",
    "        for col in vehicle_columns:\n",
    "            if col in df.columns:\n",
    "                column_categories[col] = \"Vehicle\"\n",
    "                \n",
    "        for col in propensity_columns:\n",
    "            if col in df.columns:\n",
    "                column_categories[col] = \"Propensity\"\n",
    "                \n",
    "        for col in demographic_columns:\n",
    "            if col in df.columns:\n",
    "                column_categories[col] = \"Demographic\"\n",
    "                \n",
    "        for col in lifestyle_columns:\n",
    "            if col in df.columns:\n",
    "                column_categories[col] = \"Lifestyle\"\n",
    "                \n",
    "        for col in financial_columns:\n",
    "            if col in df.columns:\n",
    "                column_categories[col] = \"Financial\"\n",
    "        \n",
    "        # For K-modes, we need to ensure all data is categorical\n",
    "        # Convert all columns to string data type\n",
    "        feature_cols = [col for col in df.columns if col != id_column]\n",
    "        \n",
    "        for col in feature_cols:\n",
    "            # Convert to string (categorical)\n",
    "            df[col] = df[col].fillna(\"missing\").astype(str)\n",
    "        \n",
    "        log_status(f\"Prepared {len(feature_cols)} categorical columns for K-modes clustering\")\n",
    "        \n",
    "        # Return prepared data\n",
    "        return {\n",
    "            'data': df,\n",
    "            'id_column': id_column,\n",
    "            'categorical_cols': feature_cols,\n",
    "            'column_categories': column_categories,\n",
    "            'row_count': row_count\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        log_status(f\"ERROR in data extraction and preparation: {str(e)}\")\n",
    "        log_status(traceback.format_exc())\n",
    "        return None\n",
    "\n",
    "def determine_optimal_k_for_kmodes(data, max_k=15, n_init=3, init_method='Huang'):\n",
    "    \"\"\"\n",
    "    Determine optimal number of clusters for K-modes using elbow method\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : pandas.DataFrame\n",
    "        DataFrame containing categorical data\n",
    "    max_k : int\n",
    "        Maximum number of clusters to evaluate\n",
    "    n_init : int\n",
    "        Number of times to run K-modes with different initializations\n",
    "    init_method : str\n",
    "        Initialization method ('Huang' or 'Cao')\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (optimal_k, cost_curve)\n",
    "    \"\"\"\n",
    "    log_status(f\"Determining optimal K for K-modes clustering (max_k={max_k})...\")\n",
    "    \n",
    "    try:\n",
    "        from kmodes.kmodes import KModes\n",
    "        \n",
    "        # Start with at least 2 clusters\n",
    "        k_range = range(2, max_k + 1)\n",
    "        cost_values = []\n",
    "        \n",
    "        # For each k, run K-modes and get cost\n",
    "        for k in k_range:\n",
    "            log_status(f\"Testing K-modes with k={k}...\")\n",
    "            kmode = KModes(n_clusters=k, init=init_method, n_init=n_init, verbose=1, random_state=42)\n",
    "            kmode.fit(data)\n",
    "            cost = kmode.cost_\n",
    "            cost_values.append(cost)\n",
    "            log_status(f\"K={k}, Cost={cost:.4f}\")\n",
    "        \n",
    "        # Find optimal k using elbow method\n",
    "        # Looking for point of diminishing returns in cost reduction\n",
    "        cost_diffs = np.diff(cost_values)\n",
    "        cost_diffs2 = np.diff(cost_diffs)  # Second derivative\n",
    "        \n",
    "        # Simple approach: find the first elbow point where second derivative is maximum\n",
    "        if len(cost_diffs2) > 0:\n",
    "            elbow_index = np.argmax(cost_diffs2) + 2  # +2 because we start at k=2 and diff reduces array length\n",
    "            optimal_k = k_range[elbow_index]\n",
    "        else:\n",
    "            # Fall back to a reasonable default if we can't find an elbow\n",
    "            optimal_k = 8\n",
    "            \n",
    "        log_status(f\"Determined optimal k for K-modes: k={optimal_k}\")\n",
    "        \n",
    "        # Return optimal k and the cost curve for plotting\n",
    "        return optimal_k, list(zip(k_range, cost_values))\n",
    "        \n",
    "    except Exception as e:\n",
    "        log_status(f\"ERROR in optimal k determination: {str(e)}\")\n",
    "        log_status(traceback.format_exc())\n",
    "        return 8, None  # Default to 8 clusters on error\n",
    "\n",
    "def run_kmodes_clustering(prepared_data, num_clusters=8, init_method='Huang', output_prefix=\"/dbfs/FileStore/acxiom_clustering/kmodes\"):\n",
    "    \"\"\"\n",
    "    Run K-modes clustering algorithm on categorical data\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    prepared_data : dict\n",
    "        Dictionary containing prepared data for K-modes\n",
    "    num_clusters : int\n",
    "        Number of clusters to generate\n",
    "    init_method : str\n",
    "        Initialization method ('Huang' or 'Cao')\n",
    "    output_prefix : str\n",
    "        Prefix for output files\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary with K-modes results or None if failed\n",
    "    \"\"\"\n",
    "    log_status(f\"Starting K-modes clustering with {num_clusters} clusters...\")\n",
    "    \n",
    "    try:\n",
    "        # Check if kmodes package is available\n",
    "        if not install_kmodes_if_needed():\n",
    "            return None\n",
    "            \n",
    "        from kmodes.kmodes import KModes\n",
    "        \n",
    "        # Extract data from prepared_data\n",
    "        df = prepared_data['data']\n",
    "        id_column = prepared_data['id_column']\n",
    "        categorical_cols = prepared_data['categorical_cols']\n",
    "        \n",
    "        # Prepare data for clustering\n",
    "        cluster_data = df[categorical_cols].copy()\n",
    "        \n",
    "        # Initialize and run K-modes\n",
    "        start_time = time.time()\n",
    "        \n",
    "        log_status(f\"Running K-modes with {num_clusters} clusters using {init_method} initialization...\")\n",
    "        kmode = KModes(\n",
    "            n_clusters=num_clusters,\n",
    "            init=init_method,\n",
    "            n_init=5,  # Number of times algorithm will be run with different initializations\n",
    "            verbose=1,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        # Fit and predict clusters\n",
    "        cluster_labels = kmode.fit_predict(cluster_data)\n",
    "        \n",
    "        runtime = time.time() - start_time\n",
    "        log_status(f\"K-modes clustering completed in {runtime:.2f} seconds\")\n",
    "        \n",
    "        # Add cluster labels to original data\n",
    "        result_df = df.copy()\n",
    "        result_df['kmodes_cluster'] = cluster_labels\n",
    "        \n",
    "        # Calculate cluster sizes\n",
    "        cluster_sizes = pd.Series(cluster_labels).value_counts().sort_index()\n",
    "        log_status(\"Cluster sizes:\")\n",
    "        for cluster_id, size in cluster_sizes.items():\n",
    "            percentage = (size / len(result_df)) * 100\n",
    "            log_status(f\"Cluster {cluster_id}: {size} records ({percentage:.1f}%)\")\n",
    "        \n",
    "        # Get centroids (modes)\n",
    "        centroids = kmode.cluster_centroids_\n",
    "        \n",
    "        # Create directory if it doesn't exist\n",
    "        os.makedirs(os.path.dirname(output_prefix), exist_ok=True)\n",
    "        \n",
    "        # Save results\n",
    "        result_path = f\"{output_prefix}_results_{num_clusters}.csv\"\n",
    "        result_df.to_csv(result_path, index=False)\n",
    "        log_status(f\"Saved K-modes results to {result_path}\")\n",
    "        \n",
    "        # Save centroids\n",
    "        centroids_df = pd.DataFrame(centroids, columns=categorical_cols)\n",
    "        centroids_df['cluster_id'] = range(num_clusters)\n",
    "        centroids_path = f\"{output_prefix}_centroids_{num_clusters}.csv\"\n",
    "        centroids_df.to_csv(centroids_path, index=False)\n",
    "        log_status(f\"Saved K-modes centroids to {centroids_path}\")\n",
    "        \n",
    "        # Create visualization of cluster sizes\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        bars = plt.bar(\n",
    "            [f\"Cluster {i}\" for i in range(num_clusters)],\n",
    "            cluster_sizes.values,\n",
    "            color=plt.cm.tab20(np.linspace(0, 1, num_clusters))\n",
    "        )\n",
    "        \n",
    "        plt.xlabel('Cluster', fontsize=12)\n",
    "        plt.ylabel('Number of Records', fontsize=12)\n",
    "        plt.title(f'K-modes Clustering: {num_clusters} Segments Size Distribution', fontsize=14)\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # Add size and percentage labels\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            percentage = (height / len(result_df)) * 100\n",
    "            plt.text(bar.get_x() + bar.get_width()/2., height + 5,\n",
    "                    f'{int(height)}\\n({percentage:.1f}%)', ha='center', fontsize=9)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save visualization\n",
    "        viz_path = f\"{output_prefix}_sizes_{num_clusters}.jpg\"\n",
    "        plt.savefig(viz_path, dpi=300, bbox_inches='tight')\n",
    "        \n",
    "        # Display in Databricks\n",
    "        if IN_DATABRICKS:\n",
    "            display(plt.gcf())\n",
    "            \n",
    "        plt.close()\n",
    "        log_status(f\"Saved cluster size visualization to {viz_path}\")\n",
    "        \n",
    "        # Create centroids profiles\n",
    "        report_content = f\"===== K-MODES CLUSTERING CENTROIDS PROFILES =====\\n\\n\"\n",
    "        report_content += f\"Number of segments: {num_clusters}\\n\"\n",
    "        report_content += f\"Date: {time.strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\"\n",
    "        \n",
    "        # For each cluster, describe the centroid\n",
    "        for i in range(num_clusters):\n",
    "            report_content += f\"Cluster {i}:\\n\"\n",
    "            report_content += f\"{'=' * (len(f'Cluster {i}:') + 5)}\\n\"\n",
    "            report_content += f\"Size: {cluster_sizes[i]} records ({cluster_sizes[i]/len(result_df)*100:.1f}%)\\n\\n\"\n",
    "            \n",
    "            # Get centroid values for key columns\n",
    "            report_content += \"Key attributes (modes):\\n\"\n",
    "            \n",
    "            # Group columns by category for better readability\n",
    "            for category in [\"Vehicle\", \"Propensity\", \"Demographic\", \"Financial\", \"Lifestyle\"]:\n",
    "                category_cols = [col for col, cat in prepared_data['column_categories'].items() \n",
    "                               if cat == category and col in categorical_cols]\n",
    "                \n",
    "                if category_cols:\n",
    "                    report_content += f\"\\n{category} Attributes:\\n\"\n",
    "                    for col in category_cols[:10]:  # Limit to first 10 per category\n",
    "                        col_idx = categorical_cols.index(col)\n",
    "                        mode_value = centroids[i, col_idx]\n",
    "                        report_content += f\"- {col}: {mode_value}\\n\"\n",
    "            \n",
    "            report_content += \"\\n\"\n",
    "            \n",
    "        # Save centroids profile report\n",
    "        report_path = f\"{output_prefix}_profiles_{num_clusters}.txt\"\n",
    "        with open(report_path, \"w\") as f:\n",
    "            f.write(report_content)\n",
    "            \n",
    "        log_status(f\"Saved centroids profiles to {report_path}\")\n",
    "        \n",
    "        # Return clustering results\n",
    "        return {\n",
    "            'cluster_data': result_df,\n",
    "            'centroids': centroids,\n",
    "            'centroids_df': centroids_df,\n",
    "            'cluster_sizes': cluster_sizes,\n",
    "            'cost': kmode.cost_,\n",
    "            'num_clusters': num_clusters,\n",
    "            'runtime': runtime\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        log_status(f\"ERROR in K-modes clustering: {str(e)}\")\n",
    "        log_status(traceback.format_exc())\n",
    "        return None\n",
    "\n",
    "def run_kmodes_multiple(cluster_counts=[8, 10, 12], sample_size=GLOBAL_CONFIG.get('clustering_sample_size', 50000)):\n",
    "    \"\"\"\n",
    "    Run K-modes clustering with multiple cluster counts and compare results\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    cluster_counts : list\n",
    "        List of cluster counts to try\n",
    "    sample_size : int\n",
    "        Sample size to use\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary with results for each cluster count\n",
    "    \"\"\"\n",
    "    log_status(f\"Running K-modes with multiple cluster counts: {cluster_counts}\")\n",
    "    \n",
    "    results = {}\n",
    "    cost_values = {}\n",
    "    \n",
    "    # Extract and prepare data (do this only once)\n",
    "    prepared_data = extract_and_prepare_data_for_kmodes(sample_size)\n",
    "    if not prepared_data:\n",
    "        log_status(\"ERROR: Failed to prepare data for K-modes clustering\")\n",
    "        return None\n",
    "    \n",
    "    # Run K-modes for each cluster count\n",
    "    for k in cluster_counts:\n",
    "        log_status(f\"Running K-modes with {k} clusters...\")\n",
    "        \n",
    "        result = run_kmodes_clustering(\n",
    "            prepared_data=prepared_data,\n",
    "            num_clusters=k\n",
    "        )\n",
    "        \n",
    "        if result:\n",
    "            results[k] = result\n",
    "            cost_values[k] = result['cost']\n",
    "            log_status(f\"Successfully completed K-modes with {k} clusters\")\n",
    "        else:\n",
    "            log_status(f\"Failed to run K-modes with {k} clusters\")\n",
    "    \n",
    "    # If we have multiple results, create comparison chart\n",
    "    if len(cost_values) > 1:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        x = list(cost_values.keys())\n",
    "        y = list(cost_values.values())\n",
    "        \n",
    "        plt.plot(x, y, 'o-', linewidth=2, markersize=8)\n",
    "        \n",
    "        # Find best (lowest cost)\n",
    "        best_k = min(cost_values, key=cost_values.get)\n",
    "        best_cost = cost_values[best_k]\n",
    "        \n",
    "        plt.axvline(x=best_k, color='red', linestyle='--',\n",
    "                   label=f'Best Cost: {best_cost:.2f} (k={best_k})')\n",
    "        \n",
    "        plt.xlabel('Number of Clusters (k)', fontsize=12)\n",
    "        plt.ylabel('Cost', fontsize=12)\n",
    "        plt.title('Comparison of K-modes Clustering Results', fontsize=14)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        comparison_path = \"/dbfs/FileStore/acxiom_clustering/kmodes_cost_comparison.jpg\"\n",
    "        plt.savefig(comparison_path, dpi=300, bbox_inches='tight')\n",
    "        \n",
    "        if IN_DATABRICKS:\n",
    "            display(plt.gcf())\n",
    "        \n",
    "        plt.close()\n",
    "        log_status(f\"Saved comparison chart to {comparison_path}\")\n",
    "        \n",
    "        # Create a comparison report\n",
    "        report = f\"===== K-MODES CLUSTERING COMPARISON =====\\n\\n\"\n",
    "        report += f\"Date: {time.strftime('%Y-%m-%d %H:%M:%S')}\\n\"\n",
    "        report += f\"Sample size: {sample_size}\\n\\n\"\n",
    "        \n",
    "        report += \"COST VALUES:\\n\"\n",
    "        report += \"------------\\n\"\n",
    "        for k in sorted(cost_values.keys()):\n",
    "            cost = cost_values[k]\n",
    "            relative = best_cost / cost\n",
    "            report += f\"k={k}: {cost:.2f} ({relative:.1%} of best cost)\\n\"\n",
    "        \n",
    "        report += f\"\\nOptimal cluster count (lowest cost): k={best_k} (cost={best_cost:.2f})\\n\"\n",
    "        \n",
    "        report_path = \"/dbfs/FileStore/acxiom_clustering/kmodes_comparison_report.txt\"\n",
    "        with open(report_path, \"w\") as f:\n",
    "            f.write(report)\n",
    "        \n",
    "        log_status(f\"Saved comparison report to {report_path}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def generate_kmodes_segment_names(kmodes_results, output_prefix=\"/dbfs/FileStore/acxiom_clustering/kmodes\"):\n",
    "    \"\"\"\n",
    "    Generate descriptive names for K-modes clusters based on centroids\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    kmodes_results : dict\n",
    "        Dictionary with K-modes results from run_kmodes_clustering()\n",
    "    output_prefix : str\n",
    "        Prefix for output files\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary mapping cluster IDs to generated names\n",
    "    \"\"\"\n",
    "    log_status(\"Generating descriptive segment names based on K-modes centroids...\")\n",
    "    \n",
    "    try:\n",
    "        # Extract centroids and data\n",
    "        centroids_df = kmodes_results['centroids_df']\n",
    "        cluster_data = kmodes_results['cluster_data']\n",
    "        cluster_sizes = kmodes_results['cluster_sizes']\n",
    "        num_clusters = kmodes_results['num_clusters']\n",
    "        \n",
    "        # Initialize segment names\n",
    "        segment_names = {}\n",
    "        \n",
    "        # Vehicle type keywords to look for in centroid values\n",
    "        vehicle_keywords = {\n",
    "            \"luxury\": [\"luxury\", \"premium\", \"upscale\", \"high-end\"],\n",
    "            \"suv\": [\"suv\", \"crossover\", \"utility\", \"cross-over\"],\n",
    "            \"sedan\": [\"sedan\", \"4-door\", \"four-door\"],\n",
    "            \"sports\": [\"sports\", \"performance\", \"convertible\", \"coupe\"],\n",
    "            \"pickup\": [\"pickup\", \"truck\", \"4x4\"],\n",
    "            \"economy\": [\"economy\", \"compact\", \"subcompact\", \"budget\"],\n",
    "            \"family\": [\"family\", \"minivan\", \"van\"]\n",
    "        }\n",
    "        \n",
    "        # Buyer type keywords to look for in centroid values\n",
    "        buyer_keywords = {\n",
    "            \"tech_savvy\": [\"tech\", \"advanced\", \"connected\", \"innovation\"],\n",
    "            \"value_oriented\": [\"value\", \"price\", \"budget\", \"economical\"],\n",
    "            \"luxury_oriented\": [\"luxury\", \"premium\", \"exclusive\"],\n",
    "            \"family_focused\": [\"family\", \"children\", \"safety\"],\n",
    "            \"eco_conscious\": [\"eco\", \"hybrid\", \"electric\", \"environment\"]\n",
    "        }\n",
    "        \n",
    "        # For each cluster, analyze centroid values\n",
    "        for cluster_id in range(num_clusters):\n",
    "            # Extract centroid for this cluster\n",
    "            centroid = centroids_df[centroids_df['cluster_id'] == cluster_id]\n",
    "            \n",
    "            # Get cluster size percentage\n",
    "            size = cluster_sizes[cluster_id]\n",
    "            size_pct = (size / cluster_sizes.sum()) * 100\n",
    "            \n",
    "            # Initialize keyword counts\n",
    "            vehicle_type_matches = {key: 0 for key in vehicle_keywords}\n",
    "            buyer_type_matches = {key: 0 for key in buyer_keywords}\n",
    "            \n",
    "            # Scan centroid values for keyword matches\n",
    "            for col in centroid.columns:\n",
    "                if col == 'cluster_id':\n",
    "                    continue\n",
    "                \n",
    "                value = str(centroid[col].values[0]).lower()\n",
    "                \n",
    "                # Check for vehicle type matches\n",
    "                for v_type, keywords in vehicle_keywords.items():\n",
    "                    for keyword in keywords:\n",
    "                        if keyword in value:\n",
    "                            vehicle_type_matches[v_type] += 1\n",
    "                \n",
    "                # Check for buyer type matches\n",
    "                for b_type, keywords in buyer_keywords.items():\n",
    "                    for keyword in keywords:\n",
    "                        if keyword in value:\n",
    "                            buyer_type_matches[b_type] += 1\n",
    "            \n",
    "            # Find dominant vehicle type\n",
    "            dominant_vehicle = max(vehicle_type_matches.items(), key=lambda x: x[1])\n",
    "            vehicle_type = \"\"\n",
    "            if dominant_vehicle[1] > 0:\n",
    "                vehicle_type = {\n",
    "                    \"luxury\": \"Luxury\",\n",
    "                    \"suv\": \"SUV/Crossover\",\n",
    "                    \"sedan\": \"Sedan\",\n",
    "                    \"sports\": \"Sports/Performance\",\n",
    "                    \"pickup\": \"Pickup/Truck\",\n",
    "                    \"economy\": \"Economy/Compact\",\n",
    "                    \"family\": \"Family/Minivan\"\n",
    "                }.get(dominant_vehicle[0], \"\")\n",
    "            \n",
    "            # Find dominant buyer type\n",
    "            dominant_buyer = max(buyer_type_matches.items(), key=lambda x: x[1])\n",
    "            buyer_type = \"\"\n",
    "            if dominant_buyer[1] > 0:\n",
    "                buyer_type = {\n",
    "                    \"tech_savvy\": \"Tech-Savvy\",\n",
    "                    \"value_oriented\": \"Value-Oriented\",\n",
    "                    \"luxury_oriented\": \"Luxury\",\n",
    "                    \"family_focused\": \"Family-Focused\",\n",
    "                    \"eco_conscious\": \"Eco-Conscious\"\n",
    "                }.get(dominant_buyer[0], \"\")\n",
    "            \n",
    "            # Generate name based on size and identified types\n",
    "            if size_pct > 20:\n",
    "                size_descriptor = \"Mainstream\"\n",
    "            elif size_pct > 10:\n",
    "                size_descriptor = \"Significant\"\n",
    "            elif size_pct > 5:\n",
    "                size_descriptor = \"Niche\"\n",
    "            else:\n",
    "                size_descriptor = \"Specialty\"\n",
    "            \n",
    "            # Combine elements to form name\n",
    "            name_parts = []\n",
    "            \n",
    "            if buyer_type:\n",
    "                name_parts.append(buyer_type)\n",
    "            \n",
    "            if vehicle_type:\n",
    "                name_parts.append(vehicle_type)\n",
    "            \n",
    "            name_parts.append(f\"{size_descriptor} Segment\")\n",
    "            \n",
    "            # Special case: If we couldn't identify types\n",
    "            if not buyer_type and not vehicle_type:\n",
    "                segment_names[cluster_id] = f\"Vehicle Segment {cluster_id + 1} ({size_descriptor})\"\n",
    "            else:\n",
    "                segment_names[cluster_id] = \" \".join(name_parts)\n",
    "        \n",
    "        # Save segment names\n",
    "        names_df = pd.DataFrame({\n",
    "            'cluster_id': segment_names.keys(),\n",
    "            'segment_name': segment_names.values(),\n",
    "            'size': [cluster_sizes[k] for k in segment_names.keys()],\n",
    "            'size_percentage': [cluster_sizes[k]/cluster_sizes.sum()*100 for k in segment_names.keys()]\n",
    "        })\n",
    "        \n",
    "        names_path = f\"{output_prefix}_segment_names_{num_clusters}.csv\"\n",
    "        names_df.to_csv(names_path, index=False)\n",
    "        log_status(f\"Saved segment names to {names_path}\")\n",
    "        \n",
    "        return segment_names\n",
    "    \n",
    "    except Exception as e:\n",
    "        log_status(f\"ERROR in segment name generation: {str(e)}\")\n",
    "        log_status(traceback.format_exc())\n",
    "        return {i: f\"Segment {i+1}\" for i in range(kmodes_results['num_clusters'])}\n",
    "\n",
    "def run_just_kmodes(cluster_counts=None, sample_size=GLOBAL_CONFIG.get('clustering_sample_size', 50000)):\n",
    "    \"\"\"\n",
    "    Run standalone K-modes clustering with flexible parameters\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    cluster_counts : list, optional\n",
    "        List of cluster counts to generate (defaults to [8, 10, 12])\n",
    "    sample_size : int, optional\n",
    "        Sample size to use\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    bool\n",
    "        True if successful, False otherwise\n",
    "    \"\"\"\n",
    "    # Use default cluster counts if not specified\n",
    "    if cluster_counts is None:\n",
    "        cluster_counts = [8, 10, 12]\n",
    "    \n",
    "    # Use default sample size if not specified\n",
    "    if sample_size is None:\n",
    "        sample_size = 10000\n",
    "    \n",
    "    log_status(f\"===== EXECUTING K-MODES CLUSTERING =====\")\n",
    "    log_status(f\"Cluster counts: {cluster_counts}\")\n",
    "    log_status(f\"Sample size: {sample_size}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # First check if kmodes package is available\n",
    "        if not install_kmodes_if_needed():\n",
    "            log_status(\"ERROR: Required kmodes package not available\")\n",
    "            return False\n",
    "        \n",
    "        # Run k-modes with multiple cluster counts\n",
    "        results = run_kmodes_multiple(\n",
    "            cluster_counts=cluster_counts,\n",
    "            sample_size=sample_size\n",
    "        )\n",
    "        \n",
    "        end_time = time.time()\n",
    "        total_time = end_time - start_time\n",
    "        \n",
    "        if results:\n",
    "            log_status(f\"✅ K-modes clustering completed successfully in {total_time:.2f} seconds\")\n",
    "            \n",
    "            # Generate segment names for the best clustering (lowest cost)\n",
    "            cost_values = {k: result['cost'] for k, result in results.items()}\n",
    "            best_k = min(cost_values, key=cost_values.get)\n",
    "            \n",
    "            log_status(f\"Generating segment names for best clustering (k={best_k})...\")\n",
    "            segment_names = generate_kmodes_segment_names(results[best_k])\n",
    "            \n",
    "            if segment_names:\n",
    "                log_status(\"Cluster segment names:\")\n",
    "                for cluster_id, name in segment_names.items():\n",
    "                    size = results[best_k]['cluster_sizes'][cluster_id]\n",
    "                    percentage = size / sum(results[best_k]['cluster_sizes']) * 100\n",
    "                    log_status(f\"  Cluster {cluster_id}: {name} ({percentage:.1f}%)\")\n",
    "            \n",
    "            return True\n",
    "        else:\n",
    "            log_status(f\"❌ K-modes clustering failed after {total_time:.2f} seconds\")\n",
    "            return False\n",
    "    \n",
    "    except Exception as e:\n",
    "        log_status(f\"ERROR in k-modes clustering: {str(e)}\")\n",
    "        log_status(traceback.format_exc())\n",
    "        return False\n",
    "\n",
    "# Testing optimal k determination with elbow method\n",
    "def determine_optimal_k_for_kmodes_and_run(sample_size=GLOBAL_CONFIG.get('clustering_sample_size', 50000), max_k=15):\n",
    "    \"\"\"\n",
    "    Determine optimal number of clusters using elbow method and then run K-modes\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    sample_size : int\n",
    "        Sample size to use\n",
    "    max_k : int\n",
    "        Maximum number of clusters to evaluate\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    bool\n",
    "        True if successful, False otherwise\n",
    "    \"\"\"\n",
    "    log_status(f\"===== DETERMINING OPTIMAL K FOR K-MODES AND RUNNING CLUSTERING =====\")\n",
    "    \n",
    "    try:\n",
    "        # First check if kmodes package is available\n",
    "        if not install_kmodes_if_needed():\n",
    "            log_status(\"ERROR: Required kmodes package not available\")\n",
    "            return False\n",
    "        \n",
    "        # Extract and prepare data\n",
    "        prepared_data = extract_and_prepare_data_for_kmodes(sample_size)\n",
    "        if not prepared_data:\n",
    "            log_status(\"ERROR: Failed to prepare data for K-modes clustering\")\n",
    "            return False\n",
    "        \n",
    "        # Determine optimal k\n",
    "        optimal_k, cost_curve = determine_optimal_k_for_kmodes(\n",
    "            prepared_data['data'][prepared_data['categorical_cols']],\n",
    "            max_k=max_k\n",
    "        )\n",
    "        \n",
    "        if not optimal_k:\n",
    "            log_status(\"ERROR: Failed to determine optimal k\")\n",
    "            return False\n",
    "        \n",
    "        # Plot cost curve\n",
    "        if cost_curve:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            \n",
    "            k_values, costs = zip(*cost_curve)\n",
    "            plt.plot(k_values, costs, 'o-', linewidth=2, markersize=8)\n",
    "            \n",
    "            plt.axvline(x=optimal_k, color='red', linestyle='--',\n",
    "                       label=f'Optimal k: {optimal_k}')\n",
    "            \n",
    "            plt.xlabel('Number of Clusters (k)', fontsize=12)\n",
    "            plt.ylabel('Cost', fontsize=12)\n",
    "            plt.title('Elbow Method for Optimal k in K-modes Clustering', fontsize=14)\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.legend()\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            \n",
    "            elbow_path = \"/dbfs/FileStore/acxiom_clustering/kmodes_elbow_method.jpg\"\n",
    "            plt.savefig(elbow_path, dpi=300, bbox_inches='tight')\n",
    "            \n",
    "            if IN_DATABRICKS:\n",
    "                display(plt.gcf())\n",
    "            \n",
    "            plt.close()\n",
    "            log_status(f\"Saved elbow method plot to {elbow_path}\")\n",
    "        \n",
    "        # Run K-modes with optimal k\n",
    "        log_status(f\"Running K-modes with optimal k={optimal_k}...\")\n",
    "        \n",
    "        result = run_kmodes_clustering(\n",
    "            prepared_data=prepared_data,\n",
    "            num_clusters=optimal_k\n",
    "        )\n",
    "        \n",
    "        if result:\n",
    "            log_status(f\"✅ Successfully completed K-modes clustering with optimal k={optimal_k}\")\n",
    "            \n",
    "            # Generate segment names\n",
    "            segment_names = generate_kmodes_segment_names(result)\n",
    "            \n",
    "            if segment_names:\n",
    "                log_status(\"Cluster segment names:\")\n",
    "                for cluster_id, name in segment_names.items():\n",
    "                    size = result['cluster_sizes'][cluster_id]\n",
    "                    percentage = size / sum(result['cluster_sizes']) * 100\n",
    "                    log_status(f\"  Cluster {cluster_id}: {name} ({percentage:.1f}%)\")\n",
    "            \n",
    "            return True\n",
    "        else:\n",
    "            log_status(f\"❌ Failed to run K-modes clustering with optimal k={optimal_k}\")\n",
    "            return False\n",
    "    \n",
    "    except Exception as e:\n",
    "        log_status(f\"ERROR in K-modes clustering with optimal k: {str(e)}\")\n",
    "        log_status(traceback.format_exc())\n",
    "        return False\n",
    "\n",
    "# OPTIONAL: Add a function to compare K-modes with other clustering methods\n",
    "def compare_kmodes_with_other_methods(sample_size=GLOBAL_CONFIG.get('clustering_sample_size', 50000), num_clusters=8):\n",
    "    \"\"\"\n",
    "    Compare K-modes clustering with other methods (K-means, K-prototypes, Hierarchical)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    sample_size : int\n",
    "        Sample size to use\n",
    "    num_clusters : int\n",
    "        Number of clusters to generate\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    bool\n",
    "        True if successful, False otherwise\n",
    "    \"\"\"\n",
    "    log_status(f\"===== COMPARING K-MODES WITH OTHER CLUSTERING METHODS =====\")\n",
    "    log_status(f\"Sample size: {sample_size}, Clusters: {num_clusters}\")\n",
    "    \n",
    "    # This function can be expanded to run different clustering methods\n",
    "    # and compare their results, metrics, and computation times\n",
    "    \n",
    "    # For now, just run K-modes\n",
    "    return run_just_kmodes(cluster_counts=[num_clusters], sample_size=sample_size)\n",
    "\n",
    "print(\"\\n=== CELL 13: K-modes Clustering Implementation completed ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "d088da53-312a-4f66-8df9-59ac2793bbb0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# CELL 14: Data Extraction Function - Modified to use dbutils for directories\n",
    "import time\n",
    "import traceback\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def direct_extract_acxiom_data(sample_size=10000):\n",
    "    \"\"\"\n",
    "    Extract Acxiom data directly using SQL without complex stratification\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    sample_size : int\n",
    "        Number of rows to sample\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary containing extracted data and metadata\n",
    "    \"\"\"\n",
    "    extraction_start = time.time()\n",
    "    log_status(f\"Starting direct Acxiom data extraction with sample size: {sample_size}\")\n",
    "    \n",
    "    try:\n",
    "        # Get column definitions\n",
    "        id_column, vehicle_columns, propensity_columns, demographic_columns, lifestyle_columns, financial_columns = define_columns()\n",
    "        \n",
    "        # Combine all columns to extract\n",
    "        all_columns = [id_column] + vehicle_columns + propensity_columns + demographic_columns + lifestyle_columns + financial_columns\n",
    "        \n",
    "        # Remove any duplicates while preserving order\n",
    "        all_columns = list(dict.fromkeys(all_columns))\n",
    "        \n",
    "        # Create a column mapping for return\n",
    "        column_map = {\n",
    "            \"id\": id_column,\n",
    "            \"vehicle\": vehicle_columns,\n",
    "            \"propensity\": propensity_columns,\n",
    "            \"demographic\": demographic_columns,\n",
    "            \"lifestyle\": lifestyle_columns,\n",
    "            \"financial\": financial_columns\n",
    "        }\n",
    "        \n",
    "        # Specify the correct table name\n",
    "        acxiom_table = \"dataproducts_dev.bronze_acxiom.gm_consumer_list\"\n",
    "        \n",
    "        # Verify table access\n",
    "        log_status(\"Verifying database access...\")\n",
    "        try:\n",
    "            spark.sql(f\"SELECT 1 FROM {acxiom_table} LIMIT 1\")\n",
    "            log_status(\"Database access verified\")\n",
    "        except Exception as e:\n",
    "            log_status(f\"FATAL ERROR: Cannot access Acxiom table: {str(e)}\")\n",
    "            raise RuntimeError(f\"Cannot access {acxiom_table}. This is a fatal error.\")\n",
    "        \n",
    "        # Verify column existence (get a sample row)\n",
    "        sample_df = spark.sql(f\"SELECT * FROM {acxiom_table} LIMIT 1\")\n",
    "        available_columns = set(sample_df.columns)\n",
    "        \n",
    "        # Filter for only available columns\n",
    "        valid_columns = [col for col in all_columns if col in available_columns]\n",
    "        \n",
    "        # Report on missing columns\n",
    "        missing_columns = set(all_columns) - set(valid_columns)\n",
    "        if missing_columns:\n",
    "            log_status(f\"WARNING: {len(missing_columns)} columns not found in dataset\")\n",
    "            # Only log first 10 missing columns to avoid clutter\n",
    "            if len(missing_columns) > 10:\n",
    "                log_status(f\"First 10 missing columns: {list(missing_columns)[:10]}\")\n",
    "            else:\n",
    "                log_status(f\"Missing columns: {list(missing_columns)}\")\n",
    "        \n",
    "        # Ensure we have enough columns\n",
    "        if len(valid_columns) < 10:\n",
    "            log_status(f\"FATAL ERROR: Not enough valid columns found (only {len(valid_columns)}). Need at least 10 columns for meaningful analysis.\")\n",
    "            raise RuntimeError(\"Not enough valid columns found for analysis. This is a fatal error.\")\n",
    "        \n",
    "        # Build column list for query\n",
    "        column_list = \", \".join(valid_columns)\n",
    "        \n",
    "        # Create simple query - just use random sampling\n",
    "        log_status(\"Executing simple random sampling...\")\n",
    "        simple_sql_query = f\"\"\"\n",
    "        SELECT {column_list}\n",
    "        FROM {acxiom_table}\n",
    "        WHERE {id_column} IS NOT NULL\n",
    "        ORDER BY rand()\n",
    "        LIMIT {sample_size}\n",
    "        \"\"\"\n",
    "        \n",
    "        # Execute the query\n",
    "        acxiom_df = spark.sql(simple_sql_query)\n",
    "        \n",
    "        # Check if we got enough data\n",
    "        row_count = acxiom_df.count()\n",
    "        if row_count == 0:\n",
    "            log_status(\"FATAL ERROR: No rows returned from Acxiom query\")\n",
    "            raise RuntimeError(\"No data returned from Acxiom database query. This is a fatal error.\")\n",
    "        \n",
    "        # Cache the result for faster subsequent operations\n",
    "        acxiom_df.cache()\n",
    "        \n",
    "        # Display sample in Databricks\n",
    "        if IN_DATABRICKS:\n",
    "            try:\n",
    "                display(HTML(\"<h4>Data Sample (first 5 rows)</h4>\"))\n",
    "                display(acxiom_df.limit(5))\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Prepare result dictionary\n",
    "        col_count = len(acxiom_df.columns)\n",
    "        extract_time = time.time() - extraction_start\n",
    "        log_status(f\"Successfully extracted {row_count} rows and {col_count} columns in {extract_time:.2f} seconds\")\n",
    "        \n",
    "        result = {\n",
    "            'spark_df': acxiom_df,\n",
    "            'id_column': id_column,\n",
    "            'column_map': column_map,\n",
    "            'row_count': row_count,\n",
    "            'extract_time': extract_time\n",
    "        }\n",
    "        \n",
    "        # Save extraction metadata - FIXED: Create directory using dbutils\n",
    "        directory_path = \"/FileStore/acxiom_clustering\"\n",
    "        metadata_path = f\"/dbfs{directory_path}/extraction_metadata.json\"\n",
    "        \n",
    "        # Create directory using dbutils\n",
    "        dbutils.fs.mkdirs(directory_path)\n",
    "        log_status(f\"Created directory: {directory_path}\")\n",
    "        \n",
    "        import json\n",
    "        metadata = {\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'sample_size': sample_size,\n",
    "            'rows_extracted': row_count,\n",
    "            'columns_extracted': col_count,\n",
    "            'execution_time_seconds': extract_time\n",
    "        }\n",
    "        \n",
    "        # Now write the file after ensuring directory exists\n",
    "        with open(metadata_path, 'w') as f:\n",
    "            json.dump(metadata, f, indent=2)\n",
    "            \n",
    "        log_status(f\"Saved extraction metadata to {metadata_path}\")\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        log_status(f\"ERROR in direct data extraction: {str(e)}\")\n",
    "        log_status(f\"Error details: {traceback.format_exc()}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "55c67569-e93b-4ccb-b63a-040af6ab0356",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# CELL 15: Improved MCA Analysis with Saved Results Check\n",
    "import time\n",
    "import traceback\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "def robust_run_mca_analysis(prepared_data, output_prefix=None):\n",
    "    \"\"\"\n",
    "    Run MCA analysis with improved error handling and stability\n",
    "    Checks for existing saved MCA coordinates before computing new ones\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    prepared_data : dict\n",
    "        Dictionary containing prepared data from robust_prepare_data_for_mca()\n",
    "    output_prefix : str, optional\n",
    "        Prefix for output files, used to locate existing MCA results\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary with MCA results or None if analysis failed\n",
    "    \"\"\"\n",
    "    if prepared_data is None:\n",
    "        log_status(\"ERROR: Prepared data is None\")\n",
    "        return None\n",
    "        \n",
    "    if 'categorical_cols' not in prepared_data or not prepared_data['categorical_cols']:\n",
    "        log_status(\"ERROR: No categorical columns available for MCA\")\n",
    "        return None\n",
    "    \n",
    "    # Check if saved MCA coordinates exist\n",
    "    if output_prefix:\n",
    "        coords_path = f\"{output_prefix}_mca_coordinates.parquet\"\n",
    "        model_path = f\"{output_prefix}_mca_model.pickle\"\n",
    "        var_path = f\"{output_prefix}_mca_variance.csv\"\n",
    "        \n",
    "        # Check if all required files exist\n",
    "        try:\n",
    "            import os\n",
    "            if os.path.exists(coords_path) and os.path.exists(var_path):\n",
    "                log_status(f\"Found existing MCA coordinates at {coords_path}, loading instead of recomputing\")\n",
    "                \n",
    "                # Load MCA coordinates\n",
    "                mca_coords = pd.read_parquet(coords_path)\n",
    "                \n",
    "                # Load variance data\n",
    "                var_df = pd.read_csv(var_path)\n",
    "                eigenvalues = var_df['eigenvalue'].values\n",
    "                var_explained = var_df['variance_explained'].values\n",
    "                cum_var = var_df['cumulative_variance'].values\n",
    "                \n",
    "                # Load MCA model if available\n",
    "                mca_model = None\n",
    "                if os.path.exists(model_path):\n",
    "                    try:\n",
    "                        with open(model_path, 'rb') as f:\n",
    "                            mca_model = pickle.load(f)\n",
    "                        log_status(\"Successfully loaded MCA model\")\n",
    "                    except Exception as e:\n",
    "                        log_status(f\"Warning: Could not load MCA model: {str(e)}\")\n",
    "                \n",
    "                # Determine number of dimensions (find where cumulative variance reaches 70%)\n",
    "                n_dims_70 = np.where(cum_var >= 0.7)[0]\n",
    "                n_dims = n_dims_70[0] + 1 if len(n_dims_70) > 0 else min(len(var_explained), 10)\n",
    "                n_dims = min(n_dims, 10)  # Cap at 10 dimensions for practical use\n",
    "                \n",
    "                log_status(f\"Using previously computed MCA with {n_dims} significant dimensions explaining {cum_var[n_dims-1]:.1%} of variance\")\n",
    "                \n",
    "                # Return loaded MCA results\n",
    "                return {\n",
    "                    'mca_coords': mca_coords,\n",
    "                    'eigenvalues': eigenvalues,\n",
    "                    'var_explained': var_explained,\n",
    "                    'cum_var': cum_var,\n",
    "                    'n_dims': n_dims,\n",
    "                    'mca_model': mca_model,\n",
    "                    'from_saved': True  # Flag indicating results were loaded from saved files\n",
    "                }\n",
    "        except Exception as load_error:\n",
    "            log_status(f\"Warning: Failed to load existing MCA results: {str(load_error)}\")\n",
    "            log_status(\"Proceeding with new MCA computation\")\n",
    "        \n",
    "    # If we get here, we need to compute new MCA\n",
    "    log_status(\"Starting MCA analysis...\")\n",
    "    mca_start = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Extract categorical data\n",
    "        categorical_cols = prepared_data['categorical_cols']\n",
    "        features = prepared_data['features'][categorical_cols].copy()\n",
    "        \n",
    "        # Ensure all columns are object type (strings)\n",
    "        for col in categorical_cols:\n",
    "            if not pd.api.types.is_object_dtype(features[col]):\n",
    "                features[col] = features[col].astype(str)\n",
    "                \n",
    "        # Make sure no NaN values are present\n",
    "        for col in categorical_cols:\n",
    "            if features[col].isna().any():\n",
    "                features[col] = features[col].fillna(\"missing\")\n",
    "        \n",
    "        log_status(f\"Running MCA on {len(categorical_cols)} categorical columns with {len(features)} rows\")\n",
    "        \n",
    "        # Import prince library with error handling\n",
    "        try:\n",
    "            import prince\n",
    "        except ImportError:\n",
    "            log_status(\"Warning: prince library not found. Attempting to install...\")\n",
    "            try:\n",
    "                import pip\n",
    "                pip.main(['install', 'prince'])\n",
    "                import prince\n",
    "                log_status(\"Successfully installed prince library\")\n",
    "            except Exception as pip_error:\n",
    "                log_status(f\"Failed to install prince: {str(pip_error)}\")\n",
    "                return None\n",
    "        \n",
    "        # Determine max components to use\n",
    "        max_components = min(30, len(categorical_cols))\n",
    "        \n",
    "        # Initialize MCA with bounded iteration to prevent long runs\n",
    "        mca = prince.MCA(\n",
    "            n_components=max_components,\n",
    "            n_iter=5,\n",
    "            copy=True,\n",
    "            check_input=True,\n",
    "            engine='sklearn',\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        # Fit MCA model\n",
    "        log_status(\"Fitting MCA model...\")\n",
    "        mca.fit(features)\n",
    "        \n",
    "        # Transform data to get coordinates\n",
    "        log_status(\"Transforming data to MCA coordinates...\")\n",
    "        mca_coords = mca.transform(features)\n",
    "        \n",
    "        # Name the columns properly\n",
    "        mca_coords.columns = [f'MCA_dim{i+1}' for i in range(mca_coords.shape[1])]\n",
    "        \n",
    "        # Add ID column if available with robust error handling\n",
    "        if 'id_values' in prepared_data and prepared_data['id_values'] is not None:\n",
    "            id_column = prepared_data['id_column']\n",
    "            try:\n",
    "                mca_coords[id_column] = prepared_data['id_values'].values\n",
    "            except Exception as e:\n",
    "                log_status(f\"Warning: Could not add ID column directly: {str(e)}\")\n",
    "                try:\n",
    "                    # Try alternative approach\n",
    "                    mca_coords[id_column] = list(prepared_data['id_values'])\n",
    "                except Exception as e2:\n",
    "                    log_status(f\"Warning: Could not add ID column as list: {str(e2)}\")\n",
    "        \n",
    "        # Get eigenvalues and variance explained\n",
    "        eigenvalues = mca.eigenvalues_\n",
    "        var_explained = eigenvalues / sum(eigenvalues)\n",
    "        cum_var = np.cumsum(var_explained)\n",
    "        \n",
    "        # Determine number of dimensions to retain (70% variance explained)\n",
    "        n_dims_70 = np.where(cum_var >= 0.7)[0]\n",
    "        n_dims = n_dims_70[0] + 1 if len(n_dims_70) > 0 else max_components\n",
    "        n_dims = min(n_dims, 10)  # Cap at 10 dimensions for practical use\n",
    "        \n",
    "        mca_time = time.time() - mca_start\n",
    "        log_status(f\"MCA analysis completed in {mca_time:.2f} seconds\")\n",
    "        log_status(f\"Identified {n_dims} significant dimensions explaining {cum_var[n_dims-1]:.1%} of variance\")\n",
    "        \n",
    "        # Return MCA results\n",
    "        return {\n",
    "            'mca_coords': mca_coords,\n",
    "            'eigenvalues': eigenvalues,\n",
    "            'var_explained': var_explained,\n",
    "            'cum_var': cum_var,\n",
    "            'n_dims': n_dims,\n",
    "            'mca_model': mca,  # Store the model for serialization with pickle\n",
    "            'from_saved': False  # Flag indicating results were newly computed\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        log_status(f\"ERROR in MCA analysis: {str(e)}\")\n",
    "        log_status(traceback.format_exc())\n",
    "        return None\n",
    "\n",
    "print(\"\\n=== CELL 15: Improved MCA Analysis with Saved Results Check completed ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "58d6a97a-3352-4669-808e-df2bafdd22be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# CELL 16: MCA Pipeline function\n",
    "def run_mca_pipeline():\n",
    "    \"\"\"\n",
    "    Run the MCA analysis pipeline and save the results.\n",
    "    This function extracts data, prepares it, runs MCA, and saves the results.\n",
    "    \"\"\"\n",
    "    log_status(\"===== STARTING MCA PIPELINE =====\")\n",
    "    log_status(f\"Using sample size: {GLOBAL_CONFIG['mca_sample_size']}\")\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Extract data using direct SQL approach\n",
    "        log_status(\"STEP 1: Extracting data for MCA...\")\n",
    "        \n",
    "        extracted_data = direct_extract_acxiom_data(sample_size=GLOBAL_CONFIG['mca_sample_size'])\n",
    "        if not extracted_data:\n",
    "            log_status(\"ERROR: Data extraction failed for MCA\")\n",
    "            return False\n",
    "            \n",
    "        log_status(f\"Successfully extracted {extracted_data['row_count']} rows for MCA\")\n",
    "        \n",
    "        # Step 2: Prepare data for MCA\n",
    "        log_status(\"STEP 2: Preparing data for MCA...\")\n",
    "        \n",
    "        prepared_data = robust_prepare_data_for_mca(extracted_data['spark_df'], \n",
    "                                                   GLOBAL_CONFIG['id_column'])\n",
    "        if not prepared_data:\n",
    "            log_status(\"ERROR: Data preparation failed for MCA\")\n",
    "            return False\n",
    "            \n",
    "        log_status(f\"Successfully prepared {len(prepared_data['categorical_cols'])} categorical columns for MCA\")\n",
    "        \n",
    "        # Step 3: Run MCA analysis\n",
    "        log_status(\"STEP 3: Running MCA analysis...\")\n",
    "        \n",
    "        mca_analysis = robust_run_mca_analysis(prepared_data)\n",
    "        if not mca_analysis:\n",
    "            log_status(\"ERROR: MCA analysis failed\")\n",
    "            return False\n",
    "            \n",
    "        log_status(f\"Successfully completed MCA analysis with {mca_analysis['n_dims']} dimensions\")\n",
    "        \n",
    "        # Step 4: Save MCA results\n",
    "        log_status(\"STEP 4: Saving MCA results...\")\n",
    "        \n",
    "        output_prefix = GLOBAL_CONFIG['mca_output_prefix']\n",
    "        try:\n",
    "            # Save MCA coordinates\n",
    "            coords_path = f\"{output_prefix}_coordinates.parquet\"\n",
    "            mca_analysis['mca_coords'].to_parquet(coords_path)\n",
    "            log_status(f\"Saved MCA coordinates to {coords_path}\")\n",
    "            \n",
    "            # Also save in CSV format for compatibility\n",
    "            csv_path = f\"{output_prefix}_coordinates.csv\"\n",
    "            mca_analysis['mca_coords'].to_csv(csv_path, index=False)\n",
    "            log_status(f\"Saved MCA coordinates as CSV to {csv_path}\")\n",
    "            \n",
    "            # Save variance explained data\n",
    "            var_path = f\"{output_prefix}_variance.csv\"\n",
    "            var_df = pd.DataFrame({\n",
    "                'dimension': range(1, len(mca_analysis['eigenvalues']) + 1),\n",
    "                'eigenvalue': mca_analysis['eigenvalues'],\n",
    "                'variance_explained': mca_analysis['var_explained'],\n",
    "                'cumulative_variance': mca_analysis['cum_var']\n",
    "            })\n",
    "            var_df.to_csv(var_path, index=False)\n",
    "            log_status(f\"Saved variance explained data to {var_path}\")\n",
    "            \n",
    "            # Create visualizations\n",
    "            visualize_mca(mca_analysis, prepared_data, output_prefix)\n",
    "            \n",
    "            # Save analysis report\n",
    "            report_path = f\"{output_prefix}_analysis_report.txt\"\n",
    "            create_enhanced_mca_analysis_report(mca_analysis, prepared_data, report_path)\n",
    "            \n",
    "            log_status(\"Successfully saved all MCA results\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as save_error:\n",
    "            log_status(f\"ERROR saving MCA results: {str(save_error)}\")\n",
    "            log_status(f\"Error details: {traceback.format_exc()}\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        log_status(f\"CRITICAL ERROR in MCA pipeline: {str(e)}\")\n",
    "        log_status(traceback.format_exc())\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "d39bbd8b-1fe9-4759-948d-dfe4d89dbc3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# CELL 17: Extended Hierarchical Clustering Implementation with MCA Reuse Support\n",
    "import time\n",
    "import traceback\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "from scipy.cluster.hierarchy import linkage, fcluster, dendrogram\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def perform_fixed_hierarchical_clustering(mca_data, num_clusters=8):\n",
    "    \"\"\"\n",
    "    Perform hierarchical clustering on MCA coordinates with a fixed number of clusters\n",
    "    Updated to work with MCA data loaded from saved files\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    mca_data : dict\n",
    "        Dictionary with MCA results from robust_run_mca_analysis()\n",
    "    num_clusters : int\n",
    "        Fixed number of clusters to generate (overrides optimal selection)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary with hierarchical clustering results or None if clustering failed\n",
    "    \"\"\"\n",
    "    log_status(f\"Starting hierarchical clustering analysis with fixed {num_clusters} clusters...\")\n",
    "    \n",
    "    try:\n",
    "        # Extract dimensional data for clustering\n",
    "        if 'mca_coords' not in mca_data or 'n_dims' not in mca_data:\n",
    "            log_status(\"ERROR: MCA data is missing required components\")\n",
    "            return None\n",
    "            \n",
    "        n_dims = mca_data['n_dims']\n",
    "        dim_cols = [f'MCA_dim{i+1}' for i in range(n_dims)]\n",
    "        \n",
    "        # Check if the columns exist in the dataframe\n",
    "        missing_cols = [col for col in dim_cols if col not in mca_data['mca_coords'].columns]\n",
    "        if missing_cols:\n",
    "            log_status(f\"ERROR: Missing MCA dimension columns: {missing_cols}\")\n",
    "            return None\n",
    "            \n",
    "        # Extract data for clustering\n",
    "        mca_for_clustering = mca_data['mca_coords'][dim_cols].copy()\n",
    "        \n",
    "        # Scale the data\n",
    "        scaler = StandardScaler()\n",
    "        scaled_data = scaler.fit_transform(mca_for_clustering)\n",
    "        \n",
    "        # Perform hierarchical clustering\n",
    "        # Use Ward's method for linkage (tends to create more balanced clusters)\n",
    "        log_status(\"Computing hierarchical linkage (this may take a moment)...\")\n",
    "        Z = linkage(scaled_data, method='ward')\n",
    "        \n",
    "        # We'll still calculate silhouette scores for reference, but use fixed number of clusters\n",
    "        log_status(\"Evaluating different numbers of clusters for reference...\")\n",
    "        \n",
    "        # Calculate silhouette scores for a range of clusters\n",
    "        max_k = max(num_clusters + 4, 15)  # Evaluate a few more than requested\n",
    "        silhouette_scores = []\n",
    "        \n",
    "        for k in range(2, max_k + 1):\n",
    "            # Cut the dendrogram to get k clusters\n",
    "            labels = fcluster(Z, k, criterion='maxclust') - 1  # Convert to 0-based indexing\n",
    "            \n",
    "            # Calculate silhouette score\n",
    "            sil_score = silhouette_score(scaled_data, labels)\n",
    "            silhouette_scores.append(sil_score)\n",
    "            \n",
    "            log_status(f\"  k={k}: silhouette={sil_score:.3f}\")\n",
    "        \n",
    "        # Get statistically optimal k for reference\n",
    "        optimal_k = silhouette_scores.index(max(silhouette_scores)) + 2\n",
    "        log_status(f\"Statistically optimal number of clusters would be: k={optimal_k}\")\n",
    "        log_status(f\"But using fixed number of clusters: k={num_clusters}\")\n",
    "        \n",
    "        # Get final cluster labels with fixed number of clusters\n",
    "        final_labels = fcluster(Z, num_clusters, criterion='maxclust') - 1\n",
    "        \n",
    "        # Add cluster labels to MCA coordinates\n",
    "        result_df = mca_data['mca_coords'].copy()\n",
    "        result_df['hier_cluster'] = final_labels\n",
    "        \n",
    "        log_status(f\"Successfully clustered data into {num_clusters} hierarchical segments\")\n",
    "        \n",
    "        # Return clustering results\n",
    "        return {\n",
    "            'cluster_data': result_df,\n",
    "            'linkage': Z,\n",
    "            'optimal_k': optimal_k,  # Still include the statistically optimal k\n",
    "            'fixed_k': num_clusters,  # Also include the fixed k\n",
    "            'silhouette_scores': silhouette_scores,\n",
    "            'hier_labels': final_labels\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        log_status(f\"ERROR in hierarchical clustering: {str(e)}\")\n",
    "        log_status(traceback.format_exc())\n",
    "        return None\n",
    "\n",
    "print(\"\\n=== CELL 17: Extended Hierarchical Clustering Implementation with MCA Reuse Support completed ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "f8829a97-a4d8-45e3-960f-1f5be8a56837",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# CELL 18: Visualization of Hierarchical Clustering\n",
    "def visualize_extended_hierarchical_clustering(hier_results, output_prefix):\n",
    "    \"\"\"\n",
    "    Create more detailed visualizations for hierarchical clustering results\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    hier_results : dict\n",
    "        Dictionary with hierarchical clustering results\n",
    "    output_prefix : str\n",
    "        Prefix for output files\n",
    "    \"\"\"\n",
    "    if hier_results is None:\n",
    "        log_status(\"ERROR: No hierarchical clustering results to visualize\")\n",
    "        return\n",
    "        \n",
    "    log_status(\"Creating hierarchical clustering visualizations...\")\n",
    "    \n",
    "    try:\n",
    "        # Create dendrogram visualization with colored clusters\n",
    "        plt.figure(figsize=(15, 8))\n",
    "        \n",
    "        # Draw dendrogram with more details\n",
    "        dendrogram(\n",
    "            hier_results['linkage'],\n",
    "            truncate_mode='lastp',\n",
    "            p=30,  # Show only the last p merged clusters\n",
    "            leaf_rotation=90.,\n",
    "            leaf_font_size=12.,\n",
    "            show_contracted=True,\n",
    "            color_threshold=0.7*max(hier_results['linkage'][:,2])  # Color threshold for better visualization\n",
    "        )\n",
    "        \n",
    "        # Add horizontal line at the cut point for the fixed number of clusters\n",
    "        cut_height = hier_results['linkage'][-(hier_results['fixed_k']-1), 2]\n",
    "        plt.axhline(y=cut_height, color='r', linestyle='--', \n",
    "                   label=f'Cut for {hier_results[\"fixed_k\"]} clusters')\n",
    "        \n",
    "        # Add horizontal line at the statistically optimal cut point\n",
    "        if hier_results['fixed_k'] != hier_results['optimal_k']:\n",
    "            opt_cut_height = hier_results['linkage'][-(hier_results['optimal_k']-1), 2]\n",
    "            plt.axhline(y=opt_cut_height, color='g', linestyle=':', \n",
    "                      label=f'Optimal cut ({hier_results[\"optimal_k\"]} clusters)')\n",
    "        \n",
    "        plt.title(f'Hierarchical Clustering Dendrogram ({hier_results[\"fixed_k\"]} clusters)', fontsize=14)\n",
    "        plt.xlabel('Sample index or (cluster size)', fontsize=12)\n",
    "        plt.ylabel('Distance', fontsize=12)\n",
    "        plt.legend()\n",
    "        \n",
    "        # Save the dendrogram (Python-friendly jpg format)\n",
    "        dendro_path = f\"{output_prefix}_hierarchical_dendrogram_{hier_results['fixed_k']}_clusters.jpg\"\n",
    "        plt.savefig(dendro_path, dpi=300, bbox_inches='tight')\n",
    "        \n",
    "        # Display in Databricks\n",
    "        if IN_DATABRICKS:\n",
    "            display(plt.gcf())\n",
    "            \n",
    "        plt.close()\n",
    "        log_status(f\"Saved hierarchical dendrogram to {dendro_path}\")\n",
    "        \n",
    "        # Create 2D scatter plot of clusters with better differentiation\n",
    "        if 'cluster_data' in hier_results and 'MCA_dim1' in hier_results['cluster_data'].columns and 'MCA_dim2' in hier_results['cluster_data'].columns:\n",
    "            plt.figure(figsize=(14, 12))\n",
    "            \n",
    "            # Use a distinct colormap\n",
    "            cluster_cmap = plt.cm.get_cmap('tab20', hier_results['fixed_k'])\n",
    "            \n",
    "            # Plot points colored by cluster\n",
    "            scatter = plt.scatter(\n",
    "                hier_results['cluster_data']['MCA_dim1'],\n",
    "                hier_results['cluster_data']['MCA_dim2'],\n",
    "                c=hier_results['cluster_data']['hier_cluster'],\n",
    "                cmap=cluster_cmap,\n",
    "                alpha=0.7,\n",
    "                s=40,\n",
    "                edgecolors='w',\n",
    "                linewidths=0.3\n",
    "            )\n",
    "            \n",
    "            # Add colorbar legend\n",
    "            cbar = plt.colorbar(scatter, label='Cluster', ticks=range(hier_results['fixed_k']))\n",
    "            cbar.set_label('Cluster', fontsize=12)\n",
    "            \n",
    "            # Calculate and plot cluster centroids\n",
    "            centroids = []\n",
    "            for i in range(hier_results['fixed_k']):\n",
    "                mask = hier_results['cluster_data']['hier_cluster'] == i\n",
    "                centroid_x = hier_results['cluster_data'].loc[mask, 'MCA_dim1'].mean()\n",
    "                centroid_y = hier_results['cluster_data'].loc[mask, 'MCA_dim2'].mean()\n",
    "                centroids.append((centroid_x, centroid_y))\n",
    "                \n",
    "                # Add cluster number labels\n",
    "                plt.text(centroid_x, centroid_y, str(i), \n",
    "                        fontsize=15, ha='center', va='center',\n",
    "                        bbox=dict(facecolor='white', alpha=0.7, edgecolor='black', boxstyle='round,pad=0.5'))\n",
    "            \n",
    "            # Convert to numpy array for plotting\n",
    "            centroids = np.array(centroids)\n",
    "            \n",
    "            # Plot centroids\n",
    "            plt.scatter(centroids[:, 0], centroids[:, 1], \n",
    "                       s=200, marker='*', c='black', edgecolor='white', linewidth=1.5,\n",
    "                       label='Cluster Centroids')\n",
    "            \n",
    "            # Add labels and title\n",
    "            plt.xlabel(\"MCA Dimension 1\", fontsize=12)\n",
    "            plt.ylabel(\"MCA Dimension 2\", fontsize=12)\n",
    "            plt.title(f'Hierarchical Clustering: {hier_results[\"fixed_k\"]} Segments in MCA Space', fontsize=14)\n",
    "            \n",
    "            # Add grid\n",
    "            plt.axhline(y=0, color='gray', linestyle='-', alpha=0.3)\n",
    "            plt.axvline(x=0, color='gray', linestyle='-', alpha=0.3)\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            \n",
    "            plt.legend()\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # Save the plot (Python-friendly jpg format)\n",
    "            scatter_path = f\"{output_prefix}_hierarchical_clusters_{hier_results['fixed_k']}.jpg\"\n",
    "            plt.savefig(scatter_path, dpi=300, bbox_inches='tight')\n",
    "            \n",
    "            # Display in Databricks\n",
    "            if IN_DATABRICKS:\n",
    "                display(plt.gcf())\n",
    "                \n",
    "            plt.close()\n",
    "            log_status(f\"Saved hierarchical cluster visualization to {scatter_path}\")\n",
    "            \n",
    "        # Create silhouette score plot for reference\n",
    "        if 'silhouette_scores' in hier_results:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            \n",
    "            k_range = range(2, len(hier_results['silhouette_scores']) + 2)\n",
    "            plt.plot(k_range, hier_results['silhouette_scores'], 'o-', color='#1f77b4', linewidth=2)\n",
    "            \n",
    "            # Mark the fixed number of clusters\n",
    "            plt.axvline(x=hier_results['fixed_k'], color='#ff7f0e', linestyle='--',\n",
    "                       label=f'Fixed k: {hier_results[\"fixed_k\"]}')\n",
    "            \n",
    "            # Mark the statistically optimal number of clusters\n",
    "            if hier_results['fixed_k'] != hier_results['optimal_k']:\n",
    "                plt.axvline(x=hier_results['optimal_k'], color='green', linestyle=':',\n",
    "                           label=f'Optimal k: {hier_results[\"optimal_k\"]}')\n",
    "            \n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.xlabel('Number of Clusters (k)', fontsize=12)\n",
    "            plt.ylabel('Silhouette Score', fontsize=12)\n",
    "            plt.title('Silhouette Scores for Hierarchical Clustering', fontsize=14)\n",
    "            plt.legend()\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # Save silhouette plot (Python-friendly jpg format)\n",
    "            sil_path = f\"{output_prefix}_hierarchical_silhouette_{hier_results['fixed_k']}.jpg\"\n",
    "            plt.savefig(sil_path, dpi=300, bbox_inches='tight')\n",
    "            \n",
    "            # Display in Databricks\n",
    "            if IN_DATABRICKS:\n",
    "                display(plt.gcf())\n",
    "                \n",
    "            plt.close()\n",
    "            log_status(f\"Saved silhouette score plot to {sil_path}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        log_status(f\"WARNING: Error creating hierarchical visualizations: {str(e)}\")\n",
    "        log_status(traceback.format_exc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "747c00aa-8363-4c65-b7ce-d34c793b470a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# CELL 19: K-means Clustering Pipeline function\n",
    "def run_kmeans_clustering_pipeline():\n",
    "    \"\"\"\n",
    "    Run the K-means clustering pipeline using previously saved MCA results.\n",
    "    \"\"\"\n",
    "    log_status(\"===== STARTING K-MEANS CLUSTERING PIPELINE =====\")\n",
    "    log_status(f\"Using sample size: {GLOBAL_CONFIG['clustering_sample_size']}\")\n",
    "    \n",
    "    try:\n",
    "        # First check if MCA results exist\n",
    "        coords_path = f\"{GLOBAL_CONFIG['mca_output_prefix']}_coordinates.parquet\"\n",
    "        var_path = f\"{GLOBAL_CONFIG['mca_output_prefix']}_variance.csv\"\n",
    "        \n",
    "        if not os.path.exists(coords_path) or not os.path.exists(var_path):\n",
    "            log_status(\"ERROR: MCA results not found. Please run MCA pipeline first.\")\n",
    "            return False\n",
    "        \n",
    "        # Load MCA coordinates and variance data\n",
    "        log_status(\"Loading existing MCA results...\")\n",
    "        \n",
    "        try:\n",
    "            # Load MCA coordinates\n",
    "            mca_coords = pd.read_parquet(coords_path)\n",
    "            \n",
    "            # Load variance data\n",
    "            var_df = pd.read_csv(var_path)\n",
    "            eigenvalues = var_df['eigenvalue'].values\n",
    "            var_explained = var_df['variance_explained'].values\n",
    "            cum_var = var_df['cumulative_variance'].values\n",
    "            \n",
    "            # Determine number of dimensions\n",
    "            n_dims_70 = np.where(cum_var >= 0.7)[0]\n",
    "            n_dims = n_dims_70[0] + 1 if len(n_dims_70) > 0 else min(len(var_explained), 10)\n",
    "            n_dims = min(n_dims, 10)  # Cap at 10 dimensions\n",
    "            \n",
    "            mca_analysis = {\n",
    "                'mca_coords': mca_coords,\n",
    "                'eigenvalues': eigenvalues,\n",
    "                'var_explained': var_explained,\n",
    "                'cum_var': cum_var,\n",
    "                'n_dims': n_dims,\n",
    "                'from_saved': True\n",
    "            }\n",
    "            \n",
    "            log_status(f\"Successfully loaded MCA results with {n_dims} dimensions\")\n",
    "            \n",
    "        except Exception as load_error:\n",
    "            log_status(f\"ERROR loading MCA results: {str(load_error)}\")\n",
    "            return False\n",
    "        \n",
    "        # Run K-means clustering for each specified cluster count\n",
    "        kmeans_results = {}\n",
    "        \n",
    "        for k in GLOBAL_CONFIG['kmeans_clusters']:\n",
    "            log_status(f\"Running K-means clustering with k={k}...\")\n",
    "            \n",
    "            try:\n",
    "                # Select MCA dimensions for clustering\n",
    "                dim_cols = [f'MCA_dim{i+1}' for i in range(n_dims)]\n",
    "                cluster_df = mca_analysis['mca_coords'][dim_cols].copy()\n",
    "                \n",
    "                # Fill NAs with 0 (required for k-means)\n",
    "                cluster_df = cluster_df.fillna(0)\n",
    "                \n",
    "                # Initialize and fit KMeans\n",
    "                from sklearn.cluster import KMeans\n",
    "                from sklearn.metrics import silhouette_score\n",
    "                \n",
    "                kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "                labels = kmeans.fit_predict(cluster_df)\n",
    "                \n",
    "                # Calculate silhouette score\n",
    "                sil_score = silhouette_score(cluster_df, labels)\n",
    "                log_status(f\"K-means with k={k}: silhouette score = {sil_score:.4f}\")\n",
    "                \n",
    "                # Add cluster labels to original data\n",
    "                result_df = mca_analysis['mca_coords'].copy()\n",
    "                result_df[f'kmeans_cluster_{k}'] = labels\n",
    "                \n",
    "                # Create centers dataframe\n",
    "                centers = pd.DataFrame(kmeans.cluster_centers_, columns=dim_cols)\n",
    "                centers['cluster'] = range(k)\n",
    "                \n",
    "                # Save results\n",
    "                output_prefix = GLOBAL_CONFIG['clustering_output_prefix']\n",
    "                result_path = f\"{output_prefix}_kmeans_{k}_clusters.parquet\"\n",
    "                result_df.to_parquet(result_path)\n",
    "                log_status(f\"Saved K-means {k} cluster assignments to {result_path}\")\n",
    "                \n",
    "                centers_path = f\"{output_prefix}_kmeans_{k}_centers.csv\"\n",
    "                centers.to_csv(centers_path, index=False)\n",
    "                log_status(f\"Saved K-means {k} centers to {centers_path}\")\n",
    "                \n",
    "                # Create visualization\n",
    "                create_kmeans_visualization(result_df, centers, k, output_prefix)\n",
    "                \n",
    "                # Store results for comparison\n",
    "                kmeans_results[k] = {\n",
    "                    'silhouette': sil_score,\n",
    "                    'centers': centers,\n",
    "                    'labels': labels\n",
    "                }\n",
    "                \n",
    "                log_status(f\"K-means clustering with k={k} completed successfully\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                log_status(f\"ERROR in K-means clustering with k={k}: {str(e)}\")\n",
    "                log_status(traceback.format_exc())\n",
    "        \n",
    "        # If we have results, create comparison report\n",
    "        if kmeans_results:\n",
    "            create_kmeans_comparison_report(kmeans_results, GLOBAL_CONFIG['clustering_output_prefix'])\n",
    "            log_status(\"K-means clustering pipeline completed successfully\")\n",
    "            return True\n",
    "        else:\n",
    "            log_status(\"ERROR: No K-means clustering results generated\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        log_status(f\"CRITICAL ERROR in K-means clustering pipeline: {str(e)}\")\n",
    "        log_status(traceback.format_exc())\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "9c320439-139e-43ac-925a-bb12ff24e5b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# CELL 20: Hierarchical Clustering Pipeline function\n",
    "def run_hierarchical_clustering_pipeline():\n",
    "    \"\"\"\n",
    "    Run the hierarchical clustering pipeline using previously saved MCA results.\n",
    "    \"\"\"\n",
    "    log_status(\"===== STARTING HIERARCHICAL CLUSTERING PIPELINE =====\")\n",
    "    log_status(f\"Using sample size: {GLOBAL_CONFIG['clustering_sample_size']}\")\n",
    "    \n",
    "    try:\n",
    "        # First check if MCA results exist\n",
    "        coords_path = f\"{GLOBAL_CONFIG['mca_output_prefix']}_coordinates.parquet\"\n",
    "        var_path = f\"{GLOBAL_CONFIG['mca_output_prefix']}_variance.csv\"\n",
    "        \n",
    "        if not os.path.exists(coords_path) or not os.path.exists(var_path):\n",
    "            log_status(\"ERROR: MCA results not found. Please run MCA pipeline first.\")\n",
    "            return False\n",
    "        \n",
    "        # Load MCA coordinates and variance data\n",
    "        log_status(\"Loading existing MCA results...\")\n",
    "        \n",
    "        try:\n",
    "            # Load MCA coordinates\n",
    "            mca_coords = pd.read_parquet(coords_path)\n",
    "            \n",
    "            # Load variance data\n",
    "            var_df = pd.read_csv(var_path)\n",
    "            eigenvalues = var_df['eigenvalue'].values\n",
    "            var_explained = var_df['variance_explained'].values\n",
    "            cum_var = var_df['cumulative_variance'].values\n",
    "            \n",
    "            # Determine number of dimensions\n",
    "            n_dims_70 = np.where(cum_var >= 0.7)[0]\n",
    "            n_dims = n_dims_70[0] + 1 if len(n_dims_70) > 0 else min(len(var_explained), 10)\n",
    "            n_dims = min(n_dims, 10)  # Cap at 10 dimensions\n",
    "            \n",
    "            mca_analysis = {\n",
    "                'mca_coords': mca_coords,\n",
    "                'eigenvalues': eigenvalues,\n",
    "                'var_explained': var_explained,\n",
    "                'cum_var': cum_var,\n",
    "                'n_dims': n_dims,\n",
    "                'from_saved': True\n",
    "            }\n",
    "            \n",
    "            log_status(f\"Successfully loaded MCA results with {n_dims} dimensions\")\n",
    "            \n",
    "        except Exception as load_error:\n",
    "            log_status(f\"ERROR loading MCA results: {str(load_error)}\")\n",
    "            return False\n",
    "        \n",
    "        # Run hierarchical clustering for each specified cluster count\n",
    "        hier_results = {}\n",
    "        \n",
    "        for k in GLOBAL_CONFIG['hierarchical_clusters']:\n",
    "            log_status(f\"Running hierarchical clustering with k={k} clusters...\")\n",
    "            \n",
    "            try:\n",
    "                # Run extended hierarchical clustering\n",
    "                result = perform_fixed_hierarchical_clustering(mca_analysis, num_clusters=k)\n",
    "                \n",
    "                if result:\n",
    "                    # Save results\n",
    "                    output_prefix = GLOBAL_CONFIG['clustering_output_prefix']\n",
    "                    result_path = f\"{output_prefix}_hierarchical_{k}_clusters.parquet\"\n",
    "                    result['cluster_data'].to_parquet(result_path)\n",
    "                    log_status(f\"Saved hierarchical {k} cluster assignments to {result_path}\")\n",
    "                    \n",
    "                    # Create visualizations\n",
    "                    visualize_extended_hierarchical_clustering(result, output_prefix)\n",
    "                    \n",
    "                    # Store results\n",
    "                    hier_results[k] = result\n",
    "                    log_status(f\"Hierarchical clustering with k={k} completed successfully\")\n",
    "                else:\n",
    "                    log_status(f\"WARNING: Hierarchical clustering with k={k} failed\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                log_status(f\"ERROR in hierarchical clustering with k={k}: {str(e)}\")\n",
    "                log_status(traceback.format_exc())\n",
    "        \n",
    "        # If we have results, create comparison report\n",
    "        if hier_results:\n",
    "            create_hierarchical_comparison_report(hier_results, GLOBAL_CONFIG['clustering_output_prefix'])\n",
    "            log_status(\"Hierarchical clustering pipeline completed successfully\")\n",
    "            return True\n",
    "        else:\n",
    "            log_status(\"ERROR: No hierarchical clustering results generated\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        log_status(f\"CRITICAL ERROR in hierarchical clustering pipeline: {str(e)}\")\n",
    "        log_status(traceback.format_exc())\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "47761f25-6931-4396-ac5d-73bfb8fe3d28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# CELL 21: Helper functions for visualization and reporting\n",
    "def create_kmeans_visualization(result_df, centers, k, output_prefix):\n",
    "    \"\"\"Create and save visualizations for K-means clustering results.\"\"\"\n",
    "    try:\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        \n",
    "        # Use a distinct colormap\n",
    "        cluster_cmap = plt.cm.get_cmap('tab20', k)\n",
    "        \n",
    "        # Plot points colored by cluster\n",
    "        scatter = plt.scatter(\n",
    "            result_df['MCA_dim1'],\n",
    "            result_df['MCA_dim2'],\n",
    "            c=result_df[f'kmeans_cluster_{k}'],\n",
    "            cmap=cluster_cmap,\n",
    "            alpha=0.7,\n",
    "            s=40,\n",
    "            edgecolors='w',\n",
    "            linewidths=0.3\n",
    "        )\n",
    "        \n",
    "        # Add colorbar legend\n",
    "        cbar = plt.colorbar(scatter, label='Cluster', ticks=range(k))\n",
    "        cbar.set_label('Cluster', fontsize=12)\n",
    "        \n",
    "        # Calculate and plot cluster centroids\n",
    "        centroids = []\n",
    "        for i in range(k):\n",
    "            mask = result_df[f'kmeans_cluster_{k}'] == i\n",
    "            centroid_x = result_df.loc[mask, 'MCA_dim1'].mean()\n",
    "            centroid_y = result_df.loc[mask, 'MCA_dim2'].mean()\n",
    "            centroids.append((centroid_x, centroid_y))\n",
    "            \n",
    "            # Add cluster number labels\n",
    "            plt.text(centroid_x, centroid_y, str(i), \n",
    "                    fontsize=15, ha='center', va='center',\n",
    "                    bbox=dict(facecolor='white', alpha=0.7, edgecolor='black', boxstyle='round,pad=0.5'))\n",
    "        \n",
    "        # Convert to numpy array for plotting\n",
    "        centroids = np.array(centroids)\n",
    "        \n",
    "        # Plot centroids\n",
    "        plt.scatter(centroids[:, 0], centroids[:, 1], \n",
    "                   s=200, marker='*', c='black', edgecolor='white', linewidth=1.5,\n",
    "                   label='Cluster Centroids')\n",
    "        \n",
    "        # Add labels and title\n",
    "        plt.xlabel(\"MCA Dimension 1\", fontsize=12)\n",
    "        plt.ylabel(\"MCA Dimension 2\", fontsize=12)\n",
    "        plt.title(f'K-means Clustering: {k} Segments in MCA Space', fontsize=14)\n",
    "        \n",
    "        # Add grid\n",
    "        plt.axhline(y=0, color='gray', linestyle='-', alpha=0.3)\n",
    "        plt.axvline(x=0, color='gray', linestyle='-', alpha=0.3)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the plot\n",
    "        viz_path = f\"{output_prefix}_kmeans_{k}_clusters.jpg\"\n",
    "        plt.savefig(viz_path, dpi=300, bbox_inches='tight')\n",
    "        \n",
    "        # Display in Databricks\n",
    "        if IN_DATABRICKS:\n",
    "            display(plt.gcf())\n",
    "            \n",
    "        plt.close()\n",
    "        log_status(f\"Saved K-means visualization to {viz_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        log_status(f\"WARNING: Error creating K-means visualization: {str(e)}\")\n",
    "\n",
    "def create_kmeans_comparison_report(kmeans_results, output_prefix):\n",
    "    \"\"\"Create and save a comparison report for different K-means cluster counts.\"\"\"\n",
    "    try:\n",
    "        # Create comparison data\n",
    "        comparison_data = []\n",
    "        for k, result in sorted(kmeans_results.items()):\n",
    "            comparison_data.append({\n",
    "                'cluster_count': k,\n",
    "                'silhouette_score': result['silhouette']\n",
    "            })\n",
    "        \n",
    "        comparison_df = pd.DataFrame(comparison_data)\n",
    "        \n",
    "        # Save comparison table\n",
    "        comparison_path = f\"{output_prefix}_kmeans_comparison.csv\"\n",
    "        comparison_df.to_csv(comparison_path, index=False)\n",
    "        \n",
    "        # Create visualization\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        plt.bar(\n",
    "            comparison_df['cluster_count'].astype(str),\n",
    "            comparison_df['silhouette_score'],\n",
    "            color=plt.cm.viridis(np.linspace(0.2, 0.8, len(comparison_df)))\n",
    "        )\n",
    "        \n",
    "        # Add best score indicator\n",
    "        best_k = comparison_df.loc[comparison_df['silhouette_score'].idxmax(), 'cluster_count']\n",
    "        best_score = comparison_df.loc[comparison_df['silhouette_score'].idxmax(), 'silhouette_score']\n",
    "        plt.axhline(y=best_score, color='red', linestyle='--', \n",
    "                   label=f'Best Score: {best_score:.4f} (k={best_k})')\n",
    "        \n",
    "        plt.xlabel('Number of Clusters (k)', fontsize=12)\n",
    "        plt.ylabel('Silhouette Score', fontsize=12)\n",
    "        plt.title('Comparison of Silhouette Scores for K-means Clustering', fontsize=14)\n",
    "        plt.grid(True, alpha=0.3, axis='y')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save comparison plot\n",
    "        plot_path = f\"{output_prefix}_kmeans_comparison.jpg\"\n",
    "        plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "        \n",
    "        # Display in Databricks\n",
    "        if IN_DATABRICKS:\n",
    "            display(plt.gcf())\n",
    "            \n",
    "        plt.close()\n",
    "        \n",
    "        # Create report\n",
    "        report_content = \"===== K-MEANS CLUSTERING COMPARISON REPORT =====\\n\\n\"\n",
    "        report_content += f\"Date: {time.strftime('%Y-%m-%d %H:%M:%S')}\\n\"\n",
    "        report_content += f\"Cluster counts evaluated: {', '.join(map(str, sorted(kmeans_results.keys())))}\\n\\n\"\n",
    "        \n",
    "        report_content += \"SILHOUETTE SCORES:\\n\"\n",
    "        report_content += \"-----------------\\n\"\n",
    "        for k in sorted(kmeans_results.keys()):\n",
    "            score = kmeans_results[k]['silhouette']\n",
    "            report_content += f\"k={k}: {score:.4f}\"\n",
    "            \n",
    "            # Add relative quality\n",
    "            relative = score / best_score\n",
    "            report_content += f\" ({relative:.1%} of best score)\\n\"\n",
    "        \n",
    "        report_content += f\"\\nStatistically optimal cluster count: k={best_k} (score: {best_score:.4f})\\n\"\n",
    "        \n",
    "        # Save report\n",
    "        report_path = f\"{output_prefix}_kmeans_comparison_report.txt\"\n",
    "        with open(report_path, \"w\") as f:\n",
    "            f.write(report_content)\n",
    "            \n",
    "        log_status(f\"Saved K-means comparison report to {report_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        log_status(f\"WARNING: Error creating K-means comparison report: {str(e)}\")\n",
    "\n",
    "def create_hierarchical_comparison_report(hier_results, output_prefix):\n",
    "    \"\"\"Create and save a comparison report for different hierarchical cluster counts.\"\"\"\n",
    "    try:\n",
    "        # Extract silhouette scores for each k\n",
    "        cluster_silhouettes = {}\n",
    "        for k, result in hier_results.items():\n",
    "            k_idx = k - 2\n",
    "            if k_idx < len(result['silhouette_scores']):\n",
    "                cluster_silhouettes[k] = result['silhouette_scores'][k_idx]\n",
    "        \n",
    "        if not cluster_silhouettes:\n",
    "            log_status(\"WARNING: No silhouette scores available for hierarchical clustering\")\n",
    "            return\n",
    "        \n",
    "        # Create comparison data\n",
    "        comparison_data = []\n",
    "        for k in sorted(cluster_silhouettes.keys()):\n",
    "            comparison_data.append({\n",
    "                'cluster_count': k,\n",
    "                'silhouette_score': cluster_silhouettes[k],\n",
    "                'relative_quality': cluster_silhouettes[k] / max(cluster_silhouettes.values())\n",
    "            })\n",
    "        \n",
    "        comparison_df = pd.DataFrame(comparison_data)\n",
    "        \n",
    "        # Save comparison table\n",
    "        comparison_path = f\"{output_prefix}_hierarchical_comparison.csv\"\n",
    "        comparison_df.to_csv(comparison_path, index=False)\n",
    "        \n",
    "        # Create visualization\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        plt.bar(\n",
    "            comparison_df['cluster_count'].astype(str),\n",
    "            comparison_df['silhouette_score'],\n",
    "            color=plt.cm.viridis(np.linspace(0.2, 0.8, len(comparison_df)))\n",
    "        )\n",
    "        \n",
    "        # Add best silhouette score indicator\n",
    "        best_k = comparison_df.loc[comparison_df['silhouette_score'].idxmax(), 'cluster_count']\n",
    "        best_score = comparison_df.loc[comparison_df['silhouette_score'].idxmax(), 'silhouette_score']\n",
    "        plt.axhline(y=best_score, color='red', linestyle='--', \n",
    "                   label=f'Best Score: {best_score:.4f} (k={best_k})')\n",
    "        \n",
    "        plt.xlabel('Number of Clusters (k)', fontsize=12)\n",
    "        plt.ylabel('Silhouette Score', fontsize=12)\n",
    "        plt.title('Comparison of Silhouette Scores for Hierarchical Clustering', fontsize=14)\n",
    "        plt.grid(True, alpha=0.3, axis='y')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save comparison plot\n",
    "        plot_path = f\"{output_prefix}_hierarchical_comparison.jpg\"\n",
    "        plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "        \n",
    "        # Display in Databricks\n",
    "        if IN_DATABRICKS:\n",
    "            display(plt.gcf())\n",
    "            \n",
    "        plt.close()\n",
    "        \n",
    "        # Create report\n",
    "        report_content = \"===== HIERARCHICAL CLUSTERING COMPARISON REPORT =====\\n\\n\"\n",
    "        report_content += f\"Date: {time.strftime('%Y-%m-%d %H:%M:%S')}\\n\"\n",
    "        report_content += f\"Cluster counts evaluated: {', '.join(map(str, sorted(cluster_silhouettes.keys())))}\\n\\n\"\n",
    "        \n",
    "        report_content += \"SILHOUETTE SCORES:\\n\"\n",
    "        report_content += \"-----------------\\n\"\n",
    "        for k in sorted(cluster_silhouettes.keys()):\n",
    "            score = cluster_silhouettes[k]\n",
    "            report_content += f\"k={k}: {score:.4f}\"\n",
    "            \n",
    "            # Add relative quality\n",
    "            relative = score / best_score\n",
    "            report_content += f\" ({relative:.1%} of best score)\\n\"\n",
    "        \n",
    "        # Sort clusters by silhouette score\n",
    "        sorted_clusters = sorted(cluster_silhouettes.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        report_content += f\"\\nStatistically optimal cluster count: k={sorted_clusters[0][0]} (score: {sorted_clusters[0][1]:.4f})\\n\"\n",
    "        \n",
    "        # Get cluster with best balance of quantity vs quality\n",
    "        balanced_k = None\n",
    "        best_balance = 0\n",
    "        for k, score in cluster_silhouettes.items():\n",
    "            # Balance formula: higher is better\n",
    "            balance = score * np.log(k)\n",
    "            if balance > best_balance:\n",
    "                best_balance = balance\n",
    "                balanced_k = k\n",
    "        \n",
    "        report_content += f\"Best balance of granularity vs. cohesion: k={balanced_k}\\n\"\n",
    "        \n",
    "        # Save report\n",
    "        report_path = f\"{output_prefix}_hierarchical_comparison_report.txt\"\n",
    "        with open(report_path, \"w\") as f:\n",
    "            f.write(report_content)\n",
    "            \n",
    "        log_status(f\"Saved hierarchical comparison report to {report_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        log_status(f\"WARNING: Error creating hierarchical comparison report: {str(e)}\")\n",
    "\n",
    "# CELL 6: Execution Cell - Run MCA Pipeline only\n",
    "def execute_mca_pipeline():\n",
    "    \"\"\"Execute the MCA pipeline.\"\"\"\n",
    "    log_status(\"===== EXECUTING MCA PIPELINE =====\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    success = run_mca_pipeline()\n",
    "    end_time = time.time()\n",
    "    \n",
    "    if success:\n",
    "        log_status(f\"✅ MCA pipeline completed successfully in {end_time - start_time:.2f} seconds\")\n",
    "        log_status(f\"MCA results saved to {GLOBAL_CONFIG['mca_output_prefix']}_* files\")\n",
    "        return True\n",
    "    else:\n",
    "        log_status(f\"❌ MCA pipeline failed after {end_time - start_time:.2f} seconds\")\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "2b63dc31-7fae-492b-adf4-f2ba6c1945a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# CELL22: Pure K-Prototypes Clustering Implementation (No MCA)\n",
    "import time\n",
    "import traceback\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "from IPython.display import display, HTML\n",
    "import os\n",
    "\n",
    "def extract_and_prepare_data_for_kprototypes(sample_size=GLOBAL_CONFIG.get('clustering_sample_size', 50000)):\n",
    "    \"\"\"\n",
    "    Extract data directly from the database and prepare it for k-prototypes\n",
    "    without any reference to MCA\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    sample_size : int\n",
    "        Number of samples to extract\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary containing prepared data for k-prototypes\n",
    "    \"\"\"\n",
    "    log_status(f\"Extracting and preparing data for k-prototypes, sample size: {sample_size}\")\n",
    "    \n",
    "    try:\n",
    "        # Get column definitions with expanded columns\n",
    "        id_column, vehicle_columns, propensity_columns, demographic_columns, lifestyle_columns, financial_columns = define_columns()\n",
    "        \n",
    "        # Combine all columns to extract\n",
    "        all_columns = [id_column] + vehicle_columns + propensity_columns + demographic_columns + lifestyle_columns + financial_columns\n",
    "        \n",
    "        # Remove any duplicates while preserving order\n",
    "        all_columns = list(dict.fromkeys(all_columns))\n",
    "        \n",
    "        # Specify the table name\n",
    "        acxiom_table = \"dataproducts_dev.bronze_acxiom.gm_consumer_list\"\n",
    "        \n",
    "        # Verify table access\n",
    "        log_status(\"Verifying database access...\")\n",
    "        try:\n",
    "            spark.sql(f\"SELECT 1 FROM {acxiom_table} LIMIT 1\")\n",
    "            log_status(\"Database access verified\")\n",
    "        except Exception as e:\n",
    "            log_status(f\"ERROR: Cannot access Acxiom table: {str(e)}\")\n",
    "            return None\n",
    "        \n",
    "        # Verify column existence\n",
    "        sample_df = spark.sql(f\"SELECT * FROM {acxiom_table} LIMIT 1\")\n",
    "        available_columns = set(sample_df.columns)\n",
    "        \n",
    "        # Filter for only available columns\n",
    "        valid_columns = [col for col in all_columns if col in available_columns]\n",
    "        \n",
    "        # Report on missing columns\n",
    "        missing_columns = set(all_columns) - set(valid_columns)\n",
    "        if missing_columns:\n",
    "            log_status(f\"WARNING: {len(missing_columns)} columns not found in dataset\")\n",
    "            log_status(f\"Missing columns: {list(missing_columns)[:20]}...\")\n",
    "        \n",
    "        # Ensure we have enough columns\n",
    "        if len(valid_columns) < 10:\n",
    "            log_status(f\"ERROR: Not enough valid columns found (only {len(valid_columns)})\")\n",
    "            return None\n",
    "        \n",
    "        # Build column list for query\n",
    "        column_list = \", \".join(valid_columns)\n",
    "        \n",
    "        # Create simple query with random sampling\n",
    "        log_status(\"Executing data extraction query...\")\n",
    "        sql_query = f\"\"\"\n",
    "        SELECT {column_list}\n",
    "        FROM {acxiom_table}\n",
    "        WHERE {id_column} IS NOT NULL\n",
    "        ORDER BY rand()\n",
    "        LIMIT {sample_size}\n",
    "        \"\"\"\n",
    "        \n",
    "        # Execute the query\n",
    "        spark_df = spark.sql(sql_query)\n",
    "        \n",
    "        # Check if we got enough data\n",
    "        row_count = spark_df.count()\n",
    "        if row_count == 0:\n",
    "            log_status(\"ERROR: No rows returned from query\")\n",
    "            return None\n",
    "        \n",
    "        log_status(f\"Successfully extracted {row_count} rows\")\n",
    "        \n",
    "        # Convert to pandas for processing\n",
    "        log_status(\"Converting to pandas DataFrame...\")\n",
    "        df = spark_df.toPandas()\n",
    "        \n",
    "        # Create category mappings\n",
    "        column_categories = {}\n",
    "        for col in vehicle_columns:\n",
    "            if col in df.columns:\n",
    "                column_categories[col] = \"Vehicle\"\n",
    "                \n",
    "        for col in propensity_columns:\n",
    "            if col in df.columns:\n",
    "                column_categories[col] = \"Propensity\"\n",
    "                \n",
    "        for col in demographic_columns:\n",
    "            if col in df.columns:\n",
    "                column_categories[col] = \"Demographic\"\n",
    "                \n",
    "        for col in lifestyle_columns:\n",
    "            if col in df.columns:\n",
    "                column_categories[col] = \"Lifestyle\"\n",
    "                \n",
    "        for col in financial_columns:\n",
    "            if col in df.columns:\n",
    "                column_categories[col] = \"Financial\"\n",
    "        \n",
    "        # Determine categorical and numerical columns with improved logic\n",
    "        log_status(\"Determining categorical and numerical columns...\")\n",
    "        categorical_cols = []\n",
    "        numerical_cols = []\n",
    "        \n",
    "        # Exclude ID column from features\n",
    "        feature_cols = [col for col in df.columns if col != id_column]\n",
    "        \n",
    "        # Known likely numeric columns\n",
    "        likely_numeric_cols = [\n",
    "            \"AP001425\",  # Household vehicle ownership count\n",
    "            \"AP001500\",  # Number of children in household\n",
    "            \"AP003015\",  # Household income (general)\n",
    "            \"AP003004\",  # Age (general)\n",
    "            \"AP003036\",  # Commute length\n",
    "            \"AP003580\",  # Household income tier\n",
    "            \"AP003581\",  # Credit card usage frequency\n",
    "            \"AP003582\",  # Savings behavior\n",
    "            \"AP004581\",  # Preferred loan duration\n",
    "            \"AP003583\",  # Risk tolerance\n",
    "            \"AP008215\",  # Vehicle upgrade timeframe\n",
    "            \"AP008219\",  # Price sensitivity\n",
    "            \"AP007821\",  # Technology adoption\n",
    "            \"AP008301\"   # Research intensity\n",
    "        ]\n",
    "        \n",
    "        for col in feature_cols:\n",
    "            # Check if column is likely numeric based on our predefined list\n",
    "            if col in likely_numeric_cols:\n",
    "                # Try to convert to numeric\n",
    "                try:\n",
    "                    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "                    # If successful and has more than 15 unique values, treat as numeric\n",
    "                    if df[col].nunique() > 15:\n",
    "                        numerical_cols.append(col)\n",
    "                        # Fill NAs with median\n",
    "                        df[col] = df[col].fillna(df[col].median())\n",
    "                        continue\n",
    "                except:\n",
    "                    pass  # If conversion fails, will be treated as categorical below\n",
    "            \n",
    "            # Get unique values for other columns\n",
    "            unique_vals = df[col].nunique()\n",
    "            \n",
    "            # Categorize columns based on uniqueness and data type\n",
    "            if unique_vals <= 15 or pd.api.types.is_object_dtype(df[col]):\n",
    "                categorical_cols.append(col)\n",
    "                # Convert to string\n",
    "                df[col] = df[col].fillna(\"missing\").astype(str)\n",
    "            else:\n",
    "                numerical_cols.append(col)\n",
    "                # Fill NAs with median\n",
    "                df[col] = df[col].fillna(df[col].median())\n",
    "        \n",
    "        log_status(f\"Identified {len(categorical_cols)} categorical and {len(numerical_cols)} numerical columns\")\n",
    "        \n",
    "        # Output the first few columns of each type for verification\n",
    "        log_status(f\"Sample categorical columns: {categorical_cols[:5]}\")\n",
    "        log_status(f\"Sample numerical columns: {numerical_cols[:5]}\")\n",
    "        \n",
    "        # Return prepared data\n",
    "        return {\n",
    "            'data': df,\n",
    "            'id_column': id_column,\n",
    "            'categorical_cols': categorical_cols,\n",
    "            'numerical_cols': numerical_cols,\n",
    "            'column_categories': column_categories,\n",
    "            'row_count': row_count\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        log_status(f\"ERROR in data extraction and preparation: {str(e)}\")\n",
    "        log_status(traceback.format_exc())\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "a144690a-269f-4598-a043-4b8d475e057a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# CELL 23: Execution Cell - Run MCA Pipeline only\n",
    "def execute_mca_pipeline():\n",
    "    \"\"\"Execute the MCA pipeline.\"\"\"\n",
    "    log_status(\"===== EXECUTING MCA PIPELINE =====\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    success = run_mca_pipeline()\n",
    "    end_time = time.time()\n",
    "    \n",
    "    if success:\n",
    "        log_status(f\"✅ MCA pipeline completed successfully in {end_time - start_time:.2f} seconds\")\n",
    "        log_status(f\"MCA results saved to {GLOBAL_CONFIG['mca_output_prefix']}_* files\")\n",
    "        return True\n",
    "    else:\n",
    "        log_status(f\"❌ MCA pipeline failed after {end_time - start_time:.2f} seconds\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "f1400811-d300-49d7-8913-e01179d8690c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# CELL 24: Execution Cell - Run K-means Clustering only\n",
    "def execute_kmeans_clustering():\n",
    "    \"\"\"Execute the K-means clustering pipeline.\"\"\"\n",
    "    log_status(\"===== EXECUTING K-MEANS CLUSTERING PIPELINE =====\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    success = run_kmeans_clustering_pipeline()\n",
    "    end_time = time.time()\n",
    "    \n",
    "    if success:\n",
    "        log_status(f\"✅ K-means clustering pipeline completed successfully in {end_time - start_time:.2f} seconds\")\n",
    "        log_status(f\"K-means results saved to {GLOBAL_CONFIG['clustering_output_prefix']}_kmeans_* files\")\n",
    "        return True\n",
    "    else:\n",
    "        log_status(f\"❌ K-means clustering pipeline failed after {end_time - start_time:.2f} seconds\")\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "703338db-1e39-4825-8142-3c5284691d96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# CELL 25: Execution Cell - Run Hierarchical Clustering only\n",
    "def execute_hierarchical_clustering():\n",
    "    \"\"\"Execute the hierarchical clustering pipeline.\"\"\"\n",
    "    log_status(\"===== EXECUTING HIERARCHICAL CLUSTERING PIPELINE =====\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    success = run_hierarchical_clustering_pipeline()\n",
    "    end_time = time.time()\n",
    "    \n",
    "    if success:\n",
    "        log_status(f\"✅ Hierarchical clustering pipeline completed successfully in {end_time - start_time:.2f} seconds\")\n",
    "        log_status(f\"Hierarchical results saved to {GLOBAL_CONFIG['clustering_output_prefix']}_hierarchical_* files\")\n",
    "        return True\n",
    "    else:\n",
    "        log_status(f\"❌ Hierarchical clustering pipeline failed after {end_time - start_time:.2f} seconds\")\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "d7ac8df0-d47d-45d7-91a5-087057748c40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# CELL 26: Execution Cell - Run All Pipelines\n",
    "def execute_all_pipelines():\n",
    "    \"\"\"Execute the full pipeline: MCA, K-means, and Hierarchical clustering.\"\"\"\n",
    "    log_status(\"===== EXECUTING FULL PIPELINE =====\")\n",
    "    \n",
    "    # Step 1: MCA\n",
    "    mca_success = execute_mca_pipeline()\n",
    "    if not mca_success:\n",
    "        log_status(\"❌ Full pipeline aborted due to MCA pipeline failure\")\n",
    "        return False\n",
    "    \n",
    "    # Step 2: K-means\n",
    "    kmeans_success = execute_kmeans_clustering()\n",
    "    if not kmeans_success:\n",
    "        log_status(\"⚠️ K-means clustering failed, continuing with hierarchical clustering\")\n",
    "    \n",
    "    # Step 3: Hierarchical\n",
    "    hier_success = execute_hierarchical_clustering()\n",
    "    if not hier_success:\n",
    "        log_status(\"⚠️ Hierarchical clustering failed\")\n",
    "    \n",
    "    # Overall success if at least one clustering method succeeded\n",
    "    if kmeans_success or hier_success:\n",
    "        log_status(\"✅ Full pipeline completed with at least one clustering method successful\")\n",
    "        return True\n",
    "    else:\n",
    "        log_status(\"❌ Full pipeline failed - all clustering methods failed\")\n",
    "        return False\n",
    "\t\t\n",
    "# CELL 10: Simplified execution functions for each specific task\n",
    "\n",
    "def run_just_mca(sample_size=None):\n",
    "    \"\"\"\n",
    "    Run only the MCA analysis with an optional sample size override.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    sample_size : int, optional\n",
    "        If provided, overrides the GLOBAL_CONFIG sample size for MCA\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    bool\n",
    "        True if successful, False otherwise\n",
    "    \"\"\"\n",
    "    if sample_size is not None:\n",
    "        log_status(f\"Overriding default MCA sample size to: {sample_size}\")\n",
    "        original_size = GLOBAL_CONFIG['mca_sample_size']\n",
    "        GLOBAL_CONFIG['mca_sample_size'] = sample_size\n",
    "    \n",
    "    success = execute_mca_pipeline()\n",
    "    \n",
    "    # Restore original sample size if we changed it\n",
    "    if sample_size is not None:\n",
    "        GLOBAL_CONFIG['mca_sample_size'] = original_size\n",
    "        \n",
    "    return success\n",
    "\n",
    "def run_just_kmeans(cluster_counts=None, sample_size=GLOBAL_CONFIG.get('clustering_sample_size', 50000)):\n",
    "    \"\"\"\n",
    "    Run only K-means clustering with optional parameter overrides.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    cluster_counts : list, optional\n",
    "        If provided, overrides the GLOBAL_CONFIG cluster counts for K-means\n",
    "    sample_size : int, optional\n",
    "        If provided, overrides the GLOBAL_CONFIG sample size for clustering\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    bool\n",
    "        True if successful, False otherwise\n",
    "    \"\"\"\n",
    "    # Save original values\n",
    "    original_clusters = GLOBAL_CONFIG['kmeans_clusters']\n",
    "    original_size = GLOBAL_CONFIG['clustering_sample_size']\n",
    "    \n",
    "    # Override if requested\n",
    "    if cluster_counts is not None:\n",
    "        log_status(f\"Overriding default K-means cluster counts to: {cluster_counts}\")\n",
    "        GLOBAL_CONFIG['kmeans_clusters'] = cluster_counts\n",
    "        \n",
    "    if sample_size is not None:\n",
    "        log_status(f\"Overriding default clustering sample size to: {sample_size}\")\n",
    "        GLOBAL_CONFIG['clustering_sample_size'] = sample_size\n",
    "    \n",
    "    success = execute_kmeans_clustering()\n",
    "    \n",
    "    # Restore original values\n",
    "    GLOBAL_CONFIG['kmeans_clusters'] = original_clusters\n",
    "    GLOBAL_CONFIG['clustering_sample_size'] = original_size\n",
    "        \n",
    "    return success\n",
    "\n",
    "def run_just_hierarchical(cluster_counts=None, sample_size=None):\n",
    "    \"\"\"\n",
    "    Run only hierarchical clustering with optional parameter overrides.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    cluster_counts : list, optional\n",
    "        If provided, overrides the GLOBAL_CONFIG cluster counts for hierarchical\n",
    "    sample_size : int, optional\n",
    "        If provided, overrides the GLOBAL_CONFIG sample size for clustering\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    bool\n",
    "        True if successful, False otherwise\n",
    "    \"\"\"\n",
    "    # Save original values\n",
    "    original_clusters = GLOBAL_CONFIG['hierarchical_clusters']\n",
    "    original_size = GLOBAL_CONFIG['clustering_sample_size']\n",
    "    \n",
    "    # Override if requested\n",
    "    if cluster_counts is not None:\n",
    "        log_status(f\"Overriding default hierarchical cluster counts to: {cluster_counts}\")\n",
    "        GLOBAL_CONFIG['hierarchical_clusters'] = cluster_counts\n",
    "        \n",
    "    if sample_size is not None:\n",
    "        log_status(f\"Overriding default clustering sample size to: {sample_size}\")\n",
    "        GLOBAL_CONFIG['clustering_sample_size'] = sample_size\n",
    "    \n",
    "    success = execute_hierarchical_clustering()\n",
    "    \n",
    "    # Restore original values\n",
    "    GLOBAL_CONFIG['hierarchical_clusters'] = original_clusters\n",
    "    GLOBAL_CONFIG['clustering_sample_size'] = original_size\n",
    "        \n",
    "    return success\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "80185f09-4867-41a4-95fa-e11bacc9829c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# CELL 27: Run Hierarchical Clustering with Proper Directory Creation\n",
    "import time\n",
    "import traceback\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def run_hierarchical_clustering_pipeline(sample_size=10000, num_clusters=8):\n",
    "    \"\"\"\n",
    "    Complete hierarchical clustering pipeline with proper directory creation\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    sample_size : int\n",
    "        Number of rows to extract from Acxiom\n",
    "    num_clusters : int\n",
    "        Number of clusters to create\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    bool\n",
    "        True if successful, False otherwise\n",
    "    \"\"\"\n",
    "    pipeline_start = time.time()\n",
    "    log_status(f\"Starting hierarchical clustering pipeline with {num_clusters} clusters\")\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Create output directory using dbutils\n",
    "        output_dir = \"/FileStore/acxiom_clustering\"\n",
    "        output_prefix = f\"/dbfs{output_dir}\"\n",
    "        \n",
    "        # Create directory using dbutils (this handles the permission properly)\n",
    "        dbutils.fs.mkdirs(output_dir)\n",
    "        log_status(f\"Created output directory: {output_dir}\")\n",
    "        \n",
    "        # Step 2: Extract data\n",
    "        log_status(\"Extracting data...\")\n",
    "        extracted_data = direct_extract_acxiom_data(sample_size)\n",
    "        if not extracted_data:\n",
    "            log_status(\"❌ Data extraction failed\")\n",
    "            return False\n",
    "        \n",
    "        log_status(f\"Successfully extracted {extracted_data['row_count']} rows\")\n",
    "        \n",
    "        # Step 3: Prepare data for MCA\n",
    "        log_status(\"Preparing data for MCA...\")\n",
    "        spark_df = extracted_data['spark_df']\n",
    "        \n",
    "        # Convert to pandas for further processing\n",
    "        pandas_df = spark_df.toPandas()\n",
    "        log_status(f\"Converted to pandas DataFrame with {len(pandas_df)} rows\")\n",
    "        \n",
    "        # Prepare MCA data\n",
    "        prepared_data = robust_prepare_data_for_mca(pandas_df, extracted_data['id_column'])\n",
    "        if not prepared_data:\n",
    "            log_status(\"❌ Data preparation for MCA failed\")\n",
    "            return False\n",
    "        \n",
    "        log_status(f\"Prepared {len(prepared_data['categorical_cols'])} categorical columns for MCA\")\n",
    "        \n",
    "        # Step 4: Run MCA\n",
    "        log_status(\"Running MCA analysis...\")\n",
    "        mca_results = robust_run_mca_analysis(prepared_data)\n",
    "        if not mca_results:\n",
    "            log_status(\"❌ MCA analysis failed\")\n",
    "            return False\n",
    "        \n",
    "        log_status(f\"MCA analysis completed with {mca_results['n_dims']} dimensions\")\n",
    "        \n",
    "        # Step 5: Perform hierarchical clustering\n",
    "        log_status(\"Performing hierarchical clustering...\")\n",
    "        \n",
    "        # Extract dimensional data for clustering\n",
    "        dim_cols = [f'MCA_dim{i+1}' for i in range(mca_results['n_dims'])]\n",
    "        mca_coords = mca_results['mca_coords'][dim_cols].copy()\n",
    "        \n",
    "        # Scale the data\n",
    "        scaler = StandardScaler()\n",
    "        scaled_data = scaler.fit_transform(mca_coords)\n",
    "        \n",
    "        # Compute linkage\n",
    "        log_status(\"Computing hierarchical linkage...\")\n",
    "        Z = linkage(scaled_data, method='ward')\n",
    "        \n",
    "        # Cluster the data\n",
    "        labels = fcluster(Z, num_clusters, criterion='maxclust') - 1  # Convert to 0-based indexing\n",
    "        \n",
    "        # Calculate silhouette score\n",
    "        sil_score = silhouette_score(scaled_data, labels)\n",
    "        log_status(f\"Silhouette score for {num_clusters} clusters: {sil_score:.4f}\")\n",
    "        \n",
    "        # Add cluster labels to MCA coordinates\n",
    "        result_df = mca_results['mca_coords'].copy()\n",
    "        result_df['cluster'] = labels\n",
    "        \n",
    "        # Step 6: Save results\n",
    "        log_status(\"Saving clustering results...\")\n",
    "        \n",
    "        # Save cluster assignments\n",
    "        clusters_path = f\"{output_prefix}/hierarchical_clusters_{num_clusters}.csv\"\n",
    "        result_df.to_csv(clusters_path, index=False)\n",
    "        log_status(f\"Saved cluster assignments to {clusters_path}\")\n",
    "        \n",
    "        # Create dendrogram visualization\n",
    "        plt.figure(figsize=(15, 8))\n",
    "        dendrogram(\n",
    "            Z,\n",
    "            truncate_mode='lastp',\n",
    "            p=30,\n",
    "            leaf_rotation=90.,\n",
    "            leaf_font_size=12.,\n",
    "            show_contracted=True,\n",
    "            color_threshold=0.7*max(Z[:,2])\n",
    "        )\n",
    "        \n",
    "        plt.title(f'Hierarchical Clustering Dendrogram ({num_clusters} clusters)', fontsize=14)\n",
    "        plt.xlabel('Sample index or (cluster size)', fontsize=12)\n",
    "        plt.ylabel('Distance', fontsize=12)\n",
    "        \n",
    "        # Save dendrogram\n",
    "        dendro_path = f\"{output_prefix}/hierarchical_dendrogram_{num_clusters}.jpg\"\n",
    "        plt.savefig(dendro_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        log_status(f\"Saved hierarchical dendrogram to {dendro_path}\")\n",
    "        \n",
    "        # Create cluster visualization\n",
    "        plt.figure(figsize=(14, 12))\n",
    "        \n",
    "        if 'MCA_dim1' in result_df.columns and 'MCA_dim2' in result_df.columns:\n",
    "            plt.scatter(\n",
    "                result_df['MCA_dim1'],\n",
    "                result_df['MCA_dim2'],\n",
    "                c=result_df['cluster'],\n",
    "                cmap='tab20',\n",
    "                alpha=0.7,\n",
    "                s=40,\n",
    "                edgecolors='w',\n",
    "                linewidths=0.3\n",
    "            )\n",
    "            \n",
    "            plt.title(f'Hierarchical Clustering: {num_clusters} Segments in MCA Space', fontsize=14)\n",
    "            plt.xlabel(\"MCA Dimension 1\", fontsize=12)\n",
    "            plt.ylabel(\"MCA Dimension 2\", fontsize=12)\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Save scatter plot\n",
    "            scatter_path = f\"{output_prefix}/hierarchical_scatter_{num_clusters}.jpg\"\n",
    "            plt.savefig(scatter_path, dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            log_status(f\"Saved cluster visualization to {scatter_path}\")\n",
    "        \n",
    "        # Save clustering metadata\n",
    "        metadata = {\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'num_clusters': num_clusters,\n",
    "            'sample_size': sample_size,\n",
    "            'silhouette_score': float(sil_score),\n",
    "            'method': 'hierarchical_ward',\n",
    "            'execution_time': float(time.time() - pipeline_start)\n",
    "        }\n",
    "        \n",
    "        metadata_path = f\"{output_prefix}/hierarchical_metadata_{num_clusters}.json\"\n",
    "        with open(metadata_path, 'w') as f:\n",
    "            json.dump(metadata, f, indent=2)\n",
    "            \n",
    "        log_status(f\"Saved clustering metadata to {metadata_path}\")\n",
    "        \n",
    "        # Step 7: Generate cluster profiles\n",
    "        log_status(\"Generating cluster profiles...\")\n",
    "        \n",
    "        # Calculate cluster sizes\n",
    "        cluster_sizes = result_df['cluster'].value_counts().sort_index()\n",
    "        \n",
    "        # Create cluster profiles report\n",
    "        report_content = f\"===== HIERARCHICAL CLUSTERING PROFILES =====\\n\\n\"\n",
    "        report_content += f\"Number of segments: {num_clusters}\\n\"\n",
    "        report_content += f\"Date: {time.strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\"\n",
    "        report_content += f\"Silhouette score: {sil_score:.4f}\\n\\n\"\n",
    "        \n",
    "        # Add cluster size information\n",
    "        report_content += \"CLUSTER SIZES:\\n\"\n",
    "        report_content += \"-------------\\n\"\n",
    "        for cluster_id, size in cluster_sizes.items():\n",
    "            percentage = (size / len(result_df)) * 100\n",
    "            report_content += f\"Cluster {cluster_id}: {size} records ({percentage:.1f}%)\\n\"\n",
    "        \n",
    "        # Add cluster dimensional profiles\n",
    "        report_content += \"\\nCLUSTER PROFILES:\\n\"\n",
    "        report_content += \"---------------\\n\"\n",
    "        \n",
    "        for i in range(num_clusters):\n",
    "            cluster_mask = result_df['cluster'] == i\n",
    "            cluster_df = result_df[cluster_mask]\n",
    "            \n",
    "            report_content += f\"\\nCluster {i}:\\n\"\n",
    "            report_content += f\"----------\\n\"\n",
    "            \n",
    "            # Dimensional profile\n",
    "            dim_means = []\n",
    "            for dim in range(min(5, mca_results['n_dims'])):\n",
    "                dim_col = f'MCA_dim{dim+1}'\n",
    "                dim_mean = cluster_df[dim_col].mean()\n",
    "                dim_means.append(f\"{dim_col}: {dim_mean:.3f}\")\n",
    "            \n",
    "            report_content += \"Dimensional profile: \" + \", \".join(dim_means) + \"\\n\"\n",
    "            report_content += f\"Size: {cluster_sizes[i]} records ({cluster_sizes[i]/len(result_df)*100:.1f}%)\\n\"\n",
    "        \n",
    "        # Save cluster profiles\n",
    "        profiles_path = f\"{output_prefix}/hierarchical_profiles_{num_clusters}.txt\"\n",
    "        with open(profiles_path, 'w') as f:\n",
    "            f.write(report_content)\n",
    "            \n",
    "        log_status(f\"Saved cluster profiles to {profiles_path}\")\n",
    "        \n",
    "        # Calculate total time\n",
    "        total_time = time.time() - pipeline_start\n",
    "        log_status(f\"✅ Hierarchical clustering pipeline completed successfully in {total_time:.2f} seconds\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        log_status(f\"ERROR in hierarchical clustering pipeline: {str(e)}\")\n",
    "        log_status(f\"Error details: {traceback.format_exc()}\")\n",
    "        log_status(\"❌ Hierarchical clustering pipeline failed\")\n",
    "        return False\n",
    "\n",
    "# Execute the hierarchical clustering pipeline\n",
    "run_hierarchical_clustering_pipeline(sample_size=10000, num_clusters=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "cda88a1b-4977-4053-8123-4c378e311e6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# CELL: K-modes Marketing Report Generation\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from matplotlib.patches import Circle, RegularPolygon\n",
    "from matplotlib.path import Path\n",
    "from matplotlib.projections.polar import PolarAxes\n",
    "from matplotlib.projections import register_projection\n",
    "from matplotlib.spines import Spine\n",
    "from matplotlib.transforms import Affine2D\n",
    "\n",
    "def generate_kmodes_marketing_report(kmodes_results, output_prefix=\"/dbfs/FileStore/acxiom_clustering/kmodes\"):\n",
    "    \"\"\"\n",
    "    Generate comprehensive marketing reports based on k-modes clustering results.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    kmodes_results : dict\n",
    "        Dictionary containing results from k-modes clustering\n",
    "    output_prefix : str\n",
    "        Base path for saving report files\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        Path to the generated report\n",
    "    \"\"\"\n",
    "    log_status(\"Generating comprehensive marketing report from k-modes clustering results...\")\n",
    "    \n",
    "    # Extract key information from results\n",
    "    cluster_data = kmodes_results.get('cluster_data')\n",
    "    centroids = kmodes_results.get('centroids_df')\n",
    "    k = kmodes_results.get('num_clusters')\n",
    "    \n",
    "    if cluster_data is None or centroids is None:\n",
    "        log_status(\"ERROR: Missing required data in k-modes results\")\n",
    "        return None\n",
    "    \n",
    "    # Create report directory if it doesn't exist\n",
    "    report_dir = f\"{output_prefix}_marketing_report_{k}\"\n",
    "    os.makedirs(report_dir, exist_ok=True)\n",
    "    \n",
    "    # Generate segment insights from cluster data\n",
    "    segment_insights = analyze_kmodes_segments(cluster_data, centroids, k)\n",
    "    \n",
    "    # Generate visualizations\n",
    "    visualization_paths = generate_segment_visualizations(segment_insights, report_dir, k)\n",
    "    \n",
    "    # Create the report document\n",
    "    report_path = create_marketing_report_document(segment_insights, visualization_paths, report_dir, k)\n",
    "    \n",
    "    log_status(f\"Marketing report successfully generated at: {report_path}\")\n",
    "    return report_path\n",
    "\n",
    "def analyze_kmodes_segments(cluster_data, centroids, k):\n",
    "    \"\"\"\n",
    "    Analyze k-modes clusters to extract marketing-relevant insights\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    cluster_data : pandas.DataFrame\n",
    "        DataFrame with cluster assignments\n",
    "    centroids : pandas.DataFrame\n",
    "        DataFrame with cluster centroids\n",
    "    k : int\n",
    "        Number of clusters\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary containing segment insights\n",
    "    \"\"\"\n",
    "    log_status(\"Analyzing k-modes clusters for marketing insights...\")\n",
    "    \n",
    "    # Calculate cluster sizes\n",
    "    cluster_counts = cluster_data['kmodes_cluster'].value_counts().sort_index()\n",
    "    cluster_sizes = [(count / len(cluster_data)) * 100 for count in cluster_counts]\n",
    "    \n",
    "    # Initialize insights dictionary\n",
    "    segment_insights = {\n",
    "        'k': k,\n",
    "        'cluster_sizes': cluster_sizes,\n",
    "        'segments': {}\n",
    "    }\n",
    "    \n",
    "    # Extract AP column patterns if available\n",
    "    ap_columns = [col for col in cluster_data.columns if col.startswith('AP') and col != 'kmodes_cluster']\n",
    "    \n",
    "    # Categorize columns into different attributes if possible\n",
    "    column_categories = {}\n",
    "    \n",
    "    try:\n",
    "        # Try to identify logical column categories based on patterns or external definitions\n",
    "        # These will vary based on your specific data\n",
    "        for col in ap_columns:\n",
    "            if any(vehicle_term in col.lower() for vehicle_term in ['vehicle', 'car', 'suv', 'luxury']):\n",
    "                column_categories[col] = \"Vehicle\"\n",
    "            elif any(demo_term in col.lower() for demo_term in ['income', 'age', 'demographic']):\n",
    "                column_categories[col] = \"Demographic\"\n",
    "            elif any(prop_term in col.lower() for prop_term in ['propensity', 'purchase', 'buy']):\n",
    "                column_categories[col] = \"Propensity\"\n",
    "            else:\n",
    "                column_categories[col] = \"Other\"\n",
    "    except:\n",
    "        # Fallback to basic categorization\n",
    "        log_status(\"Could not identify detailed column categories, using generic approach\")\n",
    "        column_categories = {col: \"Feature\" for col in ap_columns}\n",
    "    \n",
    "    # For each cluster, extract key attributes\n",
    "    for cluster_id in range(k):\n",
    "        # Extract cluster centroid\n",
    "        centroid = centroids[centroids['cluster_id'] == cluster_id].iloc[0]\n",
    "        \n",
    "        # Calculate cluster size\n",
    "        size_pct = segment_insights['cluster_sizes'][cluster_id]\n",
    "        \n",
    "        # Get cluster records\n",
    "        cluster_records = cluster_data[cluster_data['kmodes_cluster'] == cluster_id]\n",
    "        \n",
    "        # Generate a meaningful segment name\n",
    "        segment_name = generate_segment_name(centroid, cluster_records, ap_columns, column_categories)\n",
    "        \n",
    "        # Identify key distinguishing features\n",
    "        key_features = identify_key_features(centroid, centroids, ap_columns)\n",
    "        \n",
    "        # Extract demographic patterns if available\n",
    "        demographics = extract_demographics(cluster_records, column_categories)\n",
    "        \n",
    "        # Identify vehicle preferences\n",
    "        vehicle_preferences = extract_vehicle_preferences(cluster_records, centroid, column_categories)\n",
    "        \n",
    "        # Create buying behavior profile\n",
    "        buying_behavior = create_buying_behavior_profile(cluster_records, centroid, column_categories)\n",
    "        \n",
    "        # Generate marketing recommendations\n",
    "        marketing_strategy = generate_marketing_recommendations(\n",
    "            key_features, \n",
    "            demographics, \n",
    "            vehicle_preferences, \n",
    "            buying_behavior,\n",
    "            size_pct\n",
    "        )\n",
    "        \n",
    "        # Store all segment insights\n",
    "        segment_insights['segments'][cluster_id] = {\n",
    "            'name': segment_name,\n",
    "            'size_pct': size_pct,\n",
    "            'key_features': key_features,\n",
    "            'demographics': demographics,\n",
    "            'vehicle_preferences': vehicle_preferences,\n",
    "            'buying_behavior': buying_behavior,\n",
    "            'marketing_strategy': marketing_strategy\n",
    "        }\n",
    "    \n",
    "    return segment_insights\n",
    "\n",
    "def generate_segment_name(centroid, cluster_records, features, column_categories):\n",
    "    # Generate a meaningful segment name based on centroid characteristics and cluster records\n",
    "    # Store attribute scores to determine segment focus\n",
    "    segment_attributes = {\n",
    "        'luxury': 0,\n",
    "        'economy': 0,\n",
    "        'tech': 0,\n",
    "        'family': 0,\n",
    "        'utility': 0,\n",
    "        'premium': 0,\n",
    "        'value': 0\n",
    "    }\n",
    "    \n",
    "    # Analyze vehicle type features\n",
    "    vehicle_cols = [col for col, cat in column_categories.items() \n",
    "                  if cat == \"Vehicle\" and col in features]\n",
    "    \n",
    "    for col in vehicle_cols:\n",
    "        col_lower = col.lower()\n",
    "        col_value = str(centroid[col]).lower()\n",
    "        \n",
    "        # Check for luxury indicators\n",
    "        if any(term in col_lower for term in ['luxury', 'premium', 'high-end']):\n",
    "            if any(val in col_value for val in ['high', 'yes', '1', 'true']):\n",
    "                segment_attributes['luxury'] += 2\n",
    "                segment_attributes['premium'] += 1\n",
    "        \n",
    "        # Check for economy indicators\n",
    "        if any(term in col_lower for term in ['economy', 'compact', 'value']):\n",
    "            if any(val in col_value for val in ['high', 'yes', '1', 'true']):\n",
    "                segment_attributes['economy'] += 2\n",
    "                segment_attributes['value'] += 1\n",
    "        \n",
    "        # Check for SUV/truck indicators\n",
    "        if any(term in col_lower for term in ['suv', 'truck', 'pickup']):\n",
    "            if any(val in col_value for val in ['high', 'yes', '1', 'true']):\n",
    "                segment_attributes['utility'] += 1\n",
    "        \n",
    "        # Check for family vehicle indicators\n",
    "        if any(term in col_lower for term in ['family', 'minivan', 'passenger']):\n",
    "            if any(val in col_value for val in ['high', 'yes', '1', 'true']):\n",
    "                segment_attributes['family'] += 2\n",
    "    \n",
    "    # Analyze technology adoption\n",
    "    tech_cols = [col for col, cat in column_categories.items() \n",
    "               if cat in [\"Propensity\", \"Lifestyle\"] and col in features]\n",
    "    \n",
    "    for col in tech_cols:\n",
    "        col_lower = col.lower()\n",
    "        col_value = str(centroid[col]).lower()\n",
    "        \n",
    "        # Check for tech indicators\n",
    "        if any(term in col_lower for term in ['tech', 'digital', 'connected', 'innovation']):\n",
    "            if any(val in col_value for val in ['high', 'yes', '1', 'true']):\n",
    "                segment_attributes['tech'] += 2\n",
    "        \n",
    "        # Check for value consciousness\n",
    "        if any(term in col_lower for term in ['price', 'value', 'savings']):\n",
    "            if any(val in col_value for val in ['high', 'yes', '1', 'true']):\n",
    "                segment_attributes['value'] += 2\n",
    "    \n",
    "    # Analyze demographics\n",
    "    demo_cols = [col for col, cat in column_categories.items() \n",
    "               if cat == \"Demographic\" and col in features]\n",
    "    \n",
    "    for col in demo_cols:\n",
    "        col_lower = col.lower()\n",
    "        col_value = str(centroid[col]).lower()\n",
    "        \n",
    "        # Check for income/affluence indicators\n",
    "        if any(term in col_lower for term in ['income', 'affluent', 'wealthy']):\n",
    "            if any(val in col_value for val in ['high', 'upper', '1', 'true']):\n",
    "                segment_attributes['premium'] += 2\n",
    "                segment_attributes['luxury'] += 1\n",
    "            elif any(val in col_value for val in ['low', 'middle', '0']):\n",
    "                segment_attributes['value'] += 1\n",
    "                segment_attributes['economy'] += 1\n",
    "        \n",
    "        # Check for family/household indicators\n",
    "        if any(term in col_lower for term in ['household', 'children', 'family']):\n",
    "            if any(val in col_value for val in ['yes', 'high', '1', 'true']):\n",
    "                segment_attributes['family'] += 2\n",
    "    \n",
    "    # Find the top attributes\n",
    "    top_attributes = sorted(segment_attributes.items(), key=lambda x: x[1], reverse=True)\n",
    "    primary_attribute = top_attributes[0][0]\n",
    "    secondary_attribute = top_attributes[1][0] if top_attributes[1][1] > 0 else None\n",
    "    \n",
    "    # Generate name based on top attributes\n",
    "    if primary_attribute == 'luxury':\n",
    "        if secondary_attribute == 'tech':\n",
    "            name = \"Premium Tech-Forward Enthusiasts\"\n",
    "        elif secondary_attribute == 'family':\n",
    "            name = \"Luxury Family Vehicle Buyers\"\n",
    "        else:\n",
    "            name = \"Luxury Vehicle Enthusiasts\"\n",
    "    elif primary_attribute == 'premium':\n",
    "        if secondary_attribute == 'luxury':\n",
    "            name = \"Premium Luxury Buyers\"\n",
    "        elif secondary_attribute == 'tech':\n",
    "            name = \"Premium Tech-Savvy Consumers\"\n",
    "        else:\n",
    "            name = \"Premium Vehicle Segment\"\n",
    "    elif primary_attribute == 'tech':\n",
    "        if secondary_attribute == 'luxury':\n",
    "            name = \"Tech-Forward Luxury Buyers\"\n",
    "        elif secondary_attribute == 'value':\n",
    "            name = \"Tech-Savvy Value-Conscious Buyers\"\n",
    "        else:\n",
    "            name = \"Technology-Forward Early Adopters\"\n",
    "    elif primary_attribute == 'family':\n",
    "        if secondary_attribute == 'luxury':\n",
    "            name = \"Upscale Family Vehicle Buyers\"\n",
    "        elif secondary_attribute == 'value':\n",
    "            name = \"Practical Family Vehicle Owners\"\n",
    "        else:\n",
    "            name = \"Mainstream Family Vehicle Segment\"\n",
    "    elif primary_attribute == 'utility':\n",
    "        if secondary_attribute == 'luxury':\n",
    "            name = \"Premium Utility Vehicle Owners\"\n",
    "        elif secondary_attribute == 'value':\n",
    "            name = \"Practical Utility Vehicle Segment\"\n",
    "        else:\n",
    "            name = \"Utility-Focused Vehicle Buyers\"\n",
    "    elif primary_attribute == 'economy':\n",
    "        if secondary_attribute == 'tech':\n",
    "            name = \"Budget-Conscious Tech Adopters\"\n",
    "        elif secondary_attribute == 'family':\n",
    "            name = \"Economy Family Vehicle Segment\"\n",
    "        else:\n",
    "            name = \"Economy-Minded Vehicle Buyers\"\n",
    "    elif primary_attribute == 'value':\n",
    "        if secondary_attribute == 'tech':\n",
    "            name = \"Value-Oriented Tech Adopters\"\n",
    "        elif secondary_attribute == 'family':\n",
    "            name = \"Value-Conscious Family Segment\"\n",
    "        else:\n",
    "            name = \"Value-Oriented Vehicle Buyers\"\n",
    "    else:\n",
    "        name = f\"Vehicle Segment {centroid['cluster_id']}\"\n",
    "    \n",
    "    return name\n",
    "\n",
    "def identify_key_features(centroid, all_centroids, features):\n",
    "    # Identify key distinguishing features for a segment based on centroid values\n",
    "    # Initialize key features dictionary\n",
    "    key_features = {\n",
    "        'luxury_orientation': 0,\n",
    "        'price_sensitivity': 0,\n",
    "        'tech_adoption': 0,\n",
    "        'brand_loyalty': 0,\n",
    "        'family_focus': 0,\n",
    "        'utility_priority': 0\n",
    "    }\n",
    "    \n",
    "    # Extract all centroids into a DataFrame for comparison\n",
    "    centroids_df = pd.DataFrame(all_centroids)\n",
    "    \n",
    "    # Analyze centroid values to determine key features\n",
    "    # Luxury orientation\n",
    "    luxury_indicators = ['luxury', 'premium', 'high-end', 'upscale']\n",
    "    luxury_score = 0\n",
    "    luxury_cols = [col for col in features if any(ind in col.lower() for ind in luxury_indicators)]\n",
    "    \n",
    "    if luxury_cols:\n",
    "        for col in luxury_cols:\n",
    "            # Get this centroid's value\n",
    "            val = str(centroid[col]).lower()\n",
    "            \n",
    "            # Compare with other centroids\n",
    "            other_vals = centroids_df[centroids_df['cluster_id'] != centroid['cluster_id']][col].astype(str).str.lower()\n",
    "            \n",
    "            # Score based on positive indicators\n",
    "            if any(term in val for term in ['high', 'yes', '1', 'true']):\n",
    "                luxury_score += 1\n",
    "                \n",
    "                # Add extra points if this is distinctive\n",
    "                if all(term not in v for v in other_vals for term in ['high', 'yes', '1', 'true']):\n",
    "                    luxury_score += 2\n",
    "    \n",
    "    # Scale to 0-100\n",
    "    key_features['luxury_orientation'] = min(100, luxury_score * 20)\n",
    "    \n",
    "    # Price sensitivity\n",
    "    price_indicators = ['price', 'value', 'economy', 'budget']\n",
    "    price_score = 0\n",
    "    price_cols = [col for col in features if any(ind in col.lower() for ind in price_indicators)]\n",
    "    \n",
    "    if price_cols:\n",
    "        for col in price_cols:\n",
    "            val = str(centroid[col]).lower()\n",
    "            other_vals = centroids_df[centroids_df['cluster_id'] != centroid['cluster_id']][col].astype(str).str.lower()\n",
    "            \n",
    "            if any(term in val for term in ['high', 'yes', '1', 'true']):\n",
    "                price_score += 1\n",
    "                if all(term not in v for v in other_vals for term in ['high', 'yes', '1', 'true']):\n",
    "                    price_score += 1\n",
    "    \n",
    "    # Invert luxury orientation to also influence price sensitivity\n",
    "    price_score += (5 - (key_features['luxury_orientation'] / 20))\n",
    "    key_features['price_sensitivity'] = min(100, price_score * 15)\n",
    "    \n",
    "    # Tech adoption\n",
    "    tech_indicators = ['tech', 'digital', 'connected', 'smart', 'innovation']\n",
    "    tech_score = 0\n",
    "    tech_cols = [col for col in features if any(ind in col.lower() for ind in tech_indicators)]\n",
    "    \n",
    "    if tech_cols:\n",
    "        for col in tech_cols:\n",
    "            val = str(centroid[col]).lower()\n",
    "            other_vals = centroids_df[centroids_df['cluster_id'] != centroid['cluster_id']][col].astype(str).str.lower()\n",
    "            \n",
    "            if any(term in val for term in ['high', 'yes', '1', 'true']):\n",
    "                tech_score += 1\n",
    "                if all(term not in v for v in other_vals for term in ['high', 'yes', '1', 'true']):\n",
    "                    tech_score += 2\n",
    "    \n",
    "    key_features['tech_adoption'] = min(100, tech_score * 20)\n",
    "    \n",
    "    # Brand loyalty\n",
    "    brand_indicators = ['brand', 'loyal', 'preference']\n",
    "    brand_score = 0\n",
    "    brand_cols = [col for col in features if any(ind in col.lower() for ind in brand_indicators)]\n",
    "    \n",
    "    if brand_cols:\n",
    "        for col in brand_cols:\n",
    "            val = str(centroid[col]).lower()\n",
    "            \n",
    "            if any(term in val for term in ['high', 'yes', '1', 'true']):\n",
    "                brand_score += 1\n",
    "    \n",
    "    # If no direct columns, infer from luxury and price\n",
    "    if not brand_cols or brand_score == 0:\n",
    "        # Luxury segments tend to have higher brand loyalty\n",
    "        brand_score += key_features['luxury_orientation'] / 25\n",
    "        # Price sensitive segments tend to have lower brand loyalty\n",
    "        brand_score -= key_features['price_sensitivity'] / 50\n",
    "    \n",
    "    key_features['brand_loyalty'] = max(0, min(100, 50 + brand_score * 15))\n",
    "    \n",
    "    # Family focus\n",
    "    family_indicators = ['family', 'children', 'kid', 'passenger']\n",
    "    family_score = 0\n",
    "    family_cols = [col for col in features if any(ind in col.lower() for ind in family_indicators)]\n",
    "    \n",
    "    if family_cols:\n",
    "        for col in family_cols:\n",
    "            val = str(centroid[col]).lower()\n",
    "            other_vals = centroids_df[centroids_df['cluster_id'] != centroid['cluster_id']][col].astype(str).str.lower()\n",
    "            \n",
    "            if any(term in val for term in ['high', 'yes', '1', 'true']):\n",
    "                family_score += 1\n",
    "                if all(term not in v for v in other_vals for term in ['high', 'yes', '1', 'true']):\n",
    "                    family_score += 1\n",
    "    \n",
    "    key_features['family_focus'] = min(100, family_score * 25)\n",
    "    \n",
    "    # Utility priority\n",
    "    utility_indicators = ['utility', 'practical', 'cargo', 'towing', 'truck', 'suv']\n",
    "    utility_score = 0\n",
    "    utility_cols = [col for col in features if any(ind in col.lower() for ind in utility_indicators)]\n",
    "    \n",
    "    if utility_cols:\n",
    "        for col in utility_cols:\n",
    "            val = str(centroid[col]).lower()\n",
    "            other_vals = centroids_df[centroids_df['cluster_id'] != centroid['cluster_id']][col].astype(str).str.lower()\n",
    "            \n",
    "            if any(term in val for term in ['high', 'yes', '1', 'true']):\n",
    "                utility_score += 1\n",
    "                if all(term not in v for v in other_vals for term in ['high', 'yes', '1', 'true']):\n",
    "                    utility_score += 1\n",
    "    \n",
    "    key_features['utility_priority'] = min(100, utility_score * 20)\n",
    "    \n",
    "    return key_features\n",
    "\n",
    "def extract_demographics(cluster_records, column_categories):\n",
    "    # Extract demographic information from cluster records\n",
    "    # Get demographic columns\n",
    "    demo_cols = [col for col, category in column_categories.items() \n",
    "               if category == \"Demographic\"]\n",
    "    \n",
    "    if not demo_cols or len(cluster_records) == 0:\n",
    "        # Return default demographics if no data\n",
    "        return {\n",
    "            'age': \"Mixed\",\n",
    "            'income': \"Mixed\",\n",
    "            'gender_split': \"Not available\",\n",
    "            'education': \"Not available\",\n",
    "            'geography': \"Not available\"\n",
    "        }\n",
    "    \n",
    "    # Initialize demographics dictionary\n",
    "    demographics = {}\n",
    "    \n",
    "    # Analyze age if possible\n",
    "    age_cols = [col for col in demo_cols if 'age' in col.lower()]\n",
    "    if age_cols:\n",
    "        # Simple approach: check for most common age range\n",
    "        age_values = {}\n",
    "        for col in age_cols:\n",
    "            if col in cluster_records.columns:\n",
    "                value_counts = cluster_records[col].value_counts(normalize=True)\n",
    "                \n",
    "                for val, count in value_counts.items():\n",
    "                    val_str = str(val).lower()\n",
    "                    \n",
    "                    if '18-34' in val_str or 'young' in val_str:\n",
    "                        age_values['18-34'] = age_values.get('18-34', 0) + count\n",
    "                    elif '35-54' in val_str or 'middle' in val_str:\n",
    "                        age_values['35-54'] = age_values.get('35-54', 0) + count\n",
    "                    elif '55+' in val_str or 'senior' in val_str or '65+' in val_str:\n",
    "                        age_values['55+'] = age_values.get('55+', 0) + count\n",
    "        \n",
    "        # Determine primary age range\n",
    "        if age_values:\n",
    "            primary_age = max(age_values.items(), key=lambda x: x[1])[0]\n",
    "            demographics['age'] = primary_age\n",
    "        else:\n",
    "            demographics['age'] = \"Mixed\"\n",
    "    else:\n",
    "        demographics['age'] = \"Mixed\"\n",
    "    \n",
    "    # Analyze income if possible\n",
    "    income_cols = [col for col in demo_cols if 'income' in col.lower()]\n",
    "    if income_cols:\n",
    "        # Check for income brackets\n",
    "        income_values = {}\n",
    "        for col in income_cols:\n",
    "            if col in cluster_records.columns:\n",
    "                value_counts = cluster_records[col].value_counts(normalize=True)\n",
    "                \n",
    "                for val, count in value_counts.items():\n",
    "                    val_str = str(val).lower()\n",
    "                    \n",
    "                    if any(term in val_str for term in ['high', '150k', '200k', 'wealthy']):\n",
    "                        income_values['High'] = income_values.get('High', 0) + count\n",
    "                    elif any(term in val_str for term in ['middle', '50k', '100k']):\n",
    "                        income_values['Middle'] = income_values.get('Middle', 0) + count\n",
    "                    elif any(term in val_str for term in ['low', 'under', 'less']):\n",
    "                        income_values['Low'] = income_values.get('Low', 0) + count\n",
    "        \n",
    "        # Determine primary income level\n",
    "        if income_values:\n",
    "            primary_income = max(income_values.items(), key=lambda x: x[1])[0]\n",
    "            if primary_income == 'High':\n",
    "                demographics['income'] = \"Upper-middle to High\"\n",
    "            elif primary_income == 'Middle':\n",
    "                demographics['income'] = \"Middle to Upper-middle\"\n",
    "            else:\n",
    "                demographics['income'] = \"Low to Middle\"\n",
    "        else:\n",
    "            demographics['income'] = \"Mixed\"\n",
    "    else:\n",
    "        demographics['income'] = \"Mixed\"\n",
    "    \n",
    "    # Add other demographics with default values\n",
    "    demographics['gender_split'] = \"Not available\"\n",
    "    demographics['education'] = \"Not available\"\n",
    "    demographics['geography'] = \"Not available\"\n",
    "    \n",
    "    return demographics\n",
    "\n",
    "def extract_vehicle_preferences(cluster_records, centroid, column_categories):\n",
    "    # Extract vehicle preferences from cluster data\n",
    "    # Get vehicle columns\n",
    "    vehicle_cols = [col for col, category in column_categories.items() \n",
    "                  if category == \"Vehicle\"]\n",
    "    \n",
    "    if not vehicle_cols:\n",
    "        # Return default preferences if no data\n",
    "        return [\"Standard vehicles\", \"Mixed preferences\"]\n",
    "    \n",
    "    # Analyze centroid values to determine preferences\n",
    "    preferences = []\n",
    "    \n",
    "    # Look for luxury preference\n",
    "    luxury_cols = [col for col in vehicle_cols if any(term in col.lower() \n",
    "                                                   for term in ['luxury', 'premium', 'high-end'])]\n",
    "    if luxury_cols:\n",
    "        luxury_score = 0\n",
    "        for col in luxury_cols:\n",
    "            if col in centroid:\n",
    "                val = str(centroid[col]).lower()\n",
    "                if any(term in val for term in ['high', 'yes', '1', 'true']):\n",
    "                    luxury_score += 1\n",
    "        \n",
    "        if luxury_score > 0:\n",
    "            preferences.append(\"Luxury vehicles with premium features\")\n",
    "    \n",
    "    # Look for vehicle type preferences\n",
    "    suv_cols = [col for col in vehicle_cols if 'suv' in col.lower()]\n",
    "    sedan_cols = [col for col in vehicle_cols if any(term in col.lower() \n",
    "                                                  for term in ['sedan', 'car'])]\n",
    "    truck_cols = [col for col in vehicle_cols if any(term in col.lower() \n",
    "                                                  for term in ['truck', 'pickup'])]\n",
    "    minivan_cols = [col for col in vehicle_cols if any(term in col.lower() \n",
    "                                                     for term in ['minivan', 'van'])]\n",
    "    \n",
    "    # Score each vehicle type\n",
    "    vehicle_scores = {\n",
    "        'SUV/Crossover': sum(1 for col in suv_cols if col in centroid and \n",
    "                           any(term in str(centroid[col]).lower() for term in ['high', 'yes', '1', 'true'])),\n",
    "        'Sedan': sum(1 for col in sedan_cols if col in centroid and \n",
    "                   any(term in str(centroid[col]).lower() for term in ['high', 'yes', '1', 'true'])),\n",
    "        'Truck/Pickup': sum(1 for col in truck_cols if col in centroid and \n",
    "                          any(term in str(centroid[col]).lower() for term in ['high', 'yes', '1', 'true'])),\n",
    "        'Minivan': sum(1 for col in minivan_cols if col in centroid and \n",
    "                      any(term in str(centroid[col]).lower() for term in ['high', 'yes', '1', 'true']))\n",
    "    }\n",
    "    \n",
    "    # Add top vehicle types to preferences\n",
    "    top_types = sorted(vehicle_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    for vtype, score in top_types:\n",
    "        if score > 0:\n",
    "            if 'Luxury vehicles' in preferences:\n",
    "                preferences.append(f\"Premium {vtype} models\")\n",
    "            else:\n",
    "                preferences.append(f\"{vtype} models\")\n",
    "            break\n",
    "    \n",
    "    # Check for alternative fuel preference\n",
    "    alt_fuel_cols = [col for col in vehicle_cols if any(term in col.lower() \n",
    "                                                     for term in ['hybrid', 'electric', 'alt', 'alternative'])]\n",
    "    if alt_fuel_cols:\n",
    "        alt_fuel_score = 0\n",
    "        for col in alt_fuel_cols:\n",
    "            if col in centroid:\n",
    "                val = str(centroid[col]).lower()\n",
    "                if any(term in val for term in ['high', 'yes', '1', 'true']):\n",
    "                    alt_fuel_score += 1\n",
    "        \n",
    "        if alt_fuel_score > 0:\n",
    "            preferences.append(\"Alternative fuel/hybrid vehicles\")\n",
    "    \n",
    "    # Add default preferences if none identified\n",
    "    if not preferences:\n",
    "        preferences.append(\"Standard vehicles\")\n",
    "        preferences.append(\"Mixed preferences\")\n",
    "    \n",
    "    # Add family-oriented preference if family_focus is high\n",
    "    # (This would come from key_features in a real implementation)\n",
    "    family_cols = [col for col in vehicle_cols if any(term in col.lower() \n",
    "                                                    for term in ['family', 'passenger'])]\n",
    "    if family_cols:\n",
    "        family_score = 0\n",
    "        for col in family_cols:\n",
    "            if col in centroid:\n",
    "                val = str(centroid[col]).lower()\n",
    "                if any(term in val for term in ['high', 'yes', '1', 'true']):\n",
    "                    family_score += 1\n",
    "        \n",
    "        if family_score > 0:\n",
    "            preferences.append(\"Family-oriented vehicles with versatile space\")\n",
    "    \n",
    "    return preferences[:4]  # Limit to top 4 preferences\n",
    "\n",
    "def create_buying_behavior_profile(cluster_records, centroid, column_categories):\n",
    "    # Create a buying behavior profile from cluster data\n",
    "    # Get propensity columns\n",
    "    propensity_cols = [col for col, category in column_categories.items() \n",
    "                     if category in [\"Propensity\", \"Financial\"]]\n",
    "    \n",
    "    if not propensity_cols:\n",
    "        # Return default behavior if no data\n",
    "        return [\"Standard purchasing process\", \"Mixed buying patterns\"]\n",
    "    \n",
    "    # Analyze centroid values to determine behaviors\n",
    "    behaviors = []\n",
    "    \n",
    "    # Check for research intensity\n",
    "    research_cols = [col for col in propensity_cols if any(term in col.lower() \n",
    "                                                        for term in ['research', 'compare', 'review'])]\n",
    "    if research_cols:\n",
    "        research_score = 0\n",
    "        for col in research_cols:\n",
    "            if col in centroid:\n",
    "                val = str(centroid[col]).lower()\n",
    "                if any(term in val for term in ['high', 'yes', '1', 'true']):\n",
    "                    research_score += 1\n",
    "        \n",
    "        if research_score > 0:\n",
    "            behaviors.append(\"Research-intensive purchase process\")\n",
    "    \n",
    "    # Check for price sensitivity\n",
    "    price_cols = [col for col in propensity_cols if any(term in col.lower() \n",
    "                                                     for term in ['price', 'value', 'budget'])]\n",
    "    if price_cols:\n",
    "        price_score = 0\n",
    "        for col in price_cols:\n",
    "            if col in centroid:\n",
    "                val = str(centroid[col]).lower()\n",
    "                if any(term in val for term in ['high', 'yes', '1', 'true']):\n",
    "                    price_score += 1\n",
    "        \n",
    "        if price_score > 0:\n",
    "            behaviors.append(\"Price-sensitive purchasing decisions\")\n",
    "        else:\n",
    "            behaviors.append(\"Quality-focused over price-sensitive\")\n",
    "    \n",
    "    # Check for financing preference\n",
    "    finance_cols = [col for col in propensity_cols if any(term in col.lower() \n",
    "                                                       for term in ['finance', 'loan', 'lease'])]\n",
    "    if finance_cols:\n",
    "        finance_score = 0\n",
    "        lease_score = 0\n",
    "        cash_score = 0\n",
    "        \n",
    "        for col in finance_cols:\n",
    "            if col in centroid:\n",
    "                val = str(centroid[col]).lower()\n",
    "                if 'lease' in col.lower() and any(term in val for term in ['high', 'yes', '1', 'true']):\n",
    "                    lease_score += 1\n",
    "                elif 'cash' in col.lower() and any(term in val for term in ['high', 'yes', '1', 'true']):\n",
    "                    cash_score += 1\n",
    "                elif any(term in val for term in ['high', 'yes', '1', 'true']):\n",
    "                    finance_score += 1\n",
    "        \n",
    "        if lease_score > finance_score and lease_score > cash_score:\n",
    "            behaviors.append(\"Prefers leasing over traditional financing\")\n",
    "        elif cash_score > finance_score and cash_score > lease_score:\n",
    "            behaviors.append(\"Prefer cash purchases when possible\")\n",
    "        elif finance_score > 0:\n",
    "            behaviors.append(\"Traditional financing approach\")\n",
    "    \n",
    "    # Check for tech influence\n",
    "    tech_cols = [col for col in propensity_cols if any(term in col.lower() \n",
    "                                                    for term in ['tech', 'feature', 'digital'])]\n",
    "    if tech_cols:\n",
    "        tech_score = 0\n",
    "        for col in tech_cols:\n",
    "            if col in centroid:\n",
    "                val = str(centroid[col]).lower()\n",
    "                if any(term in val for term in ['high', 'yes', '1', 'true']):\n",
    "                    tech_score += 1\n",
    "        \n",
    "        if tech_score > 0:\n",
    "            behaviors.append(\"Influenced by technology features\")\n",
    "    \n",
    "    # Check for brand influence\n",
    "    brand_cols = [col for col in propensity_cols if any(term in col.lower() \n",
    "                                                     for term in ['brand', 'loyal'])]\n",
    "    if brand_cols:\n",
    "        brand_score = 0\n",
    "        for col in brand_cols:\n",
    "            if col in centroid:\n",
    "                val = str(centroid[col]).lower()\n",
    "                if any(term in val for term in ['high', 'yes', '1', 'true']):\n",
    "                    brand_score += 1\n",
    "        \n",
    "        if brand_score > 0:\n",
    "            behaviors.append(\"Brand-loyal purchase decisions\")\n",
    "    # Add digital shopping behavior\n",
    "    digital_cols = [col for col in propensity_cols if any(term in col.lower() \n",
    "                                                       for term in ['online', 'digital', 'mobile'])]\n",
    "    if digital_cols:\n",
    "        digital_score = 0\n",
    "        for col in digital_cols:\n",
    "            if col in centroid:\n",
    "                val = str(centroid[col]).lower()\n",
    "                if any(term in val for term in ['high', 'yes', '1', 'true']):\n",
    "                    digital_score += 1\n",
    "        \n",
    "        if digital_score > 0:\n",
    "            behaviors.append(\"Digital-first research approach\")\n",
    "    \n",
    "    # Add default behaviors if none identified\n",
    "    if not behaviors:\n",
    "        behaviors.append(\"Standard purchasing process\")\n",
    "        behaviors.append(\"Mixed buying patterns\")\n",
    "    \n",
    "    return behaviors[:4]  # Limit to top 4 behaviors\n",
    "\n",
    "def generate_marketing_recommendations(key_features, demographics, vehicle_preferences, buying_behavior, size_pct):\n",
    "    # Generate marketing recommendations based on segment insights\n",
    "    # Determine primary orientation based on key features and other insights\n",
    "    primary_orientation = None\n",
    "    secondary_orientation = None\n",
    "    \n",
    "    # Evaluate orientations by combining key features\n",
    "    orientations = {\n",
    "        'luxury': key_features['luxury_orientation'],\n",
    "        'value': key_features['price_sensitivity'],\n",
    "        'technology': key_features['tech_adoption'],\n",
    "        'family': key_features['family_focus'],\n",
    "        'utility': key_features['utility_priority']\n",
    "    }\n",
    "    \n",
    "    # Get top two orientations\n",
    "    top_orientations = sorted(orientations.items(), key=lambda x: x[1], reverse=True)\n",
    "    primary_orientation = top_orientations[0][0]\n",
    "    if len(top_orientations) > 1 and top_orientations[1][1] > 20:\n",
    "        secondary_orientation = top_orientations[1][0]\n",
    "    \n",
    "    # Evaluate segment size to determine marketing approach\n",
    "    if size_pct > 20:\n",
    "        size_category = \"mass\"\n",
    "    elif size_pct > 10:\n",
    "        size_category = \"major\"\n",
    "    elif size_pct > 3:\n",
    "        size_category = \"niche\"\n",
    "    else:\n",
    "        size_category = \"micro\"\n",
    "    \n",
    "    # Generate marketing strategy components\n",
    "    messaging = []\n",
    "    channels = []\n",
    "    product_focus = []\n",
    "    customer_experience = []\n",
    "    \n",
    "    # Generate recommendations based on primary orientation\n",
    "    if primary_orientation == 'luxury':\n",
    "        messaging = [\n",
    "            \"Emphasize exclusivity and premium craftsmanship\",\n",
    "            \"Focus on performance specifications and driving experience\",\n",
    "            \"Highlight premium materials and attention to detail\",\n",
    "            \"Stress the prestige and status aspects of ownership\"\n",
    "        ]\n",
    "        channels = [\n",
    "            \"Luxury lifestyle publications and websites\",\n",
    "            \"Exclusive events and experiences\",\n",
    "            \"Personalized direct marketing\",\n",
    "            \"High-end digital platforms\"\n",
    "        ]\n",
    "        product_focus = [\n",
    "            \"Premium model variants with exclusive features\",\n",
    "            \"Specialized limited editions\",\n",
    "            \"Models with advanced technology packages\",\n",
    "            \"High-performance engine options\"\n",
    "        ]\n",
    "        customer_experience = [\n",
    "            \"White-glove concierge service\",\n",
    "            \"VIP test drive experiences\",\n",
    "            \"Personalized shopping journey\",\n",
    "            \"Premium ownership benefits\"\n",
    "        ]\n",
    "    elif primary_orientation == 'value':\n",
    "        messaging = [\n",
    "            \"Emphasize total value and cost of ownership\",\n",
    "            \"Focus on reliability and longevity\",\n",
    "            \"Highlight competitive pricing and incentives\",\n",
    "            \"Stress practical benefits and economic advantages\"\n",
    "        ]\n",
    "        channels = [\n",
    "            \"Mass media with targeted value messaging\",\n",
    "            \"Price comparison platforms\",\n",
    "            \"Digital platforms featuring deals and incentives\",\n",
    "            \"Local dealer marketing with price focus\"\n",
    "        ]\n",
    "        product_focus = [\n",
    "            \"Value-oriented trim packages\",\n",
    "            \"Efficient and economical models\",\n",
    "            \"Base models with essential feature sets\",\n",
    "            \"Special value editions\"\n",
    "        ]\n",
    "        customer_experience = [\n",
    "            \"Straightforward, transparent pricing\",\n",
    "            \"Streamlined purchase process\",\n",
    "            \"Value-focused financing options\",\n",
    "            \"No-pressure sales environment\"\n",
    "        ]\n",
    "    elif primary_orientation == 'technology':\n",
    "        messaging = [\n",
    "            \"Focus on innovation and cutting-edge technology\",\n",
    "            \"Emphasize connectivity and digital features\",\n",
    "            \"Highlight vehicle technology advancements\",\n",
    "            \"Position as forward-thinking and progressive\"\n",
    "        ]\n",
    "        channels = [\n",
    "            \"Technology and innovation publications\",\n",
    "            \"Digital and social media platforms\",\n",
    "            \"Tech-focused events and demonstrations\",\n",
    "            \"Online communities and forums\"\n",
    "        ]\n",
    "        product_focus = [\n",
    "            \"Models with advanced technology packages\",\n",
    "            \"Connectivity and infotainment features\",\n",
    "            \"Electric and hybrid vehicles\",\n",
    "            \"Advanced driver assistance systems\"\n",
    "        ]\n",
    "        customer_experience = [\n",
    "            \"Digital-first shopping experience\",\n",
    "            \"Tech-focused product demonstrations\",\n",
    "            \"Mobile app integration for ownership\",\n",
    "            \"Virtual reality showroom experiences\"\n",
    "        ]\n",
    "    elif primary_orientation == 'family':\n",
    "        messaging = [\n",
    "            \"Focus on safety features and ratings\",\n",
    "            \"Emphasize versatility and space efficiency\",\n",
    "            \"Highlight comfort for all passengers\",\n",
    "            \"Stress family-focused convenience features\"\n",
    "        ]\n",
    "        channels = [\n",
    "            \"Family lifestyle publications\",\n",
    "            \"Parenting websites and platforms\",\n",
    "            \"School and community partnerships\",\n",
    "            \"Family-oriented events and activities\"\n",
    "        ]\n",
    "        product_focus = [\n",
    "            \"Family-sized vehicles with versatile seating\",\n",
    "            \"Models with top safety ratings\",\n",
    "            \"Vehicles with entertainment systems\",\n",
    "            \"Features for convenience and comfort\"\n",
    "        ]\n",
    "        customer_experience = [\n",
    "            \"Family-friendly showrooms with kids' areas\",\n",
    "            \"Extended test drives for family evaluation\",\n",
    "            \"Family-focused financing options\",\n",
    "            \"Simplified buying process\"\n",
    "        ]\n",
    "    elif primary_orientation == 'utility':\n",
    "        messaging = [\n",
    "            \"Focus on capability and durability\",\n",
    "            \"Emphasize practical functionality\",\n",
    "            \"Highlight versatility and adaptability\",\n",
    "            \"Stress reliability and ruggedness\"\n",
    "        ]\n",
    "        channels = [\n",
    "            \"Industry and trade publications\",\n",
    "            \"Practical demonstration events\",\n",
    "            \"Work and lifestyle partnerships\",\n",
    "            \"Targeted social media for utility use cases\"\n",
    "        ]\n",
    "        product_focus = [\n",
    "            \"Versatile cargo and towing capability\",\n",
    "            \"Robust design and durability features\",\n",
    "            \"Models with practical customization options\",\n",
    "            \"Functional accessories and packages\"\n",
    "        ]\n",
    "        customer_experience = [\n",
    "            \"Practical demonstration of capabilities\",\n",
    "            \"Feature-focused sales approach\",\n",
    "            \"Worksite vehicle programs\",\n",
    "            \"Service packages that minimize downtime\"\n",
    "        ]\n",
    "    else:\n",
    "        # Generic/balanced approach as fallback\n",
    "        messaging = [\n",
    "            \"Balance features and value messaging\",\n",
    "            \"Focus on overall ownership benefits\",\n",
    "            \"Highlight versatility and adaptability\",\n",
    "            \"Emphasize quality and reliability\"\n",
    "        ]\n",
    "        channels = [\n",
    "            \"Broad market advertising\",\n",
    "            \"Digital and social media mix\",\n",
    "            \"Dealership marketing\",\n",
    "            \"Content marketing highlighting use cases\"\n",
    "        ]\n",
    "        product_focus = [\n",
    "            \"Mid-range models with balanced features\",\n",
    "            \"Popular configurations and packages\",\n",
    "            \"Versatile models with broad appeal\",\n",
    "            \"Core product lineup\"\n",
    "        ]\n",
    "        customer_experience = [\n",
    "            \"Streamlined and efficient buying process\",\n",
    "            \"Balanced approach to sales and service\",\n",
    "            \"Focus on customer satisfaction\",\n",
    "            \"Standard dealership experience with personal touches\"\n",
    "        ]\n",
    "    \n",
    "    # Modify recommendations based on secondary orientation\n",
    "    if secondary_orientation == 'luxury' and primary_orientation != 'luxury':\n",
    "        messaging[1] = \"Emphasize premium features within reach\"\n",
    "        product_focus[0] = \"Higher trim levels with premium touches\"\n",
    "    elif secondary_orientation == 'technology' and primary_orientation != 'technology':\n",
    "        messaging[2] = \"Highlight innovative technology features\"\n",
    "        product_focus[2] = \"Models with advanced technology options\"\n",
    "        customer_experience[1] = \"Technology-focused product demonstrations\"\n",
    "    elif secondary_orientation == 'family' and primary_orientation != 'family':\n",
    "        messaging[2] = \"Emphasize versatility for family needs\"\n",
    "        product_focus[1] = \"Family-friendly configurations and features\"\n",
    "    elif secondary_orientation == 'utility' and primary_orientation != 'utility':\n",
    "        messaging[2] = \"Highlight practical utility and versatility\"\n",
    "        product_focus[1] = \"Models with enhanced utility features\"\n",
    "    elif secondary_orientation == 'value' and primary_orientation != 'value':\n",
    "        messaging[1] = \"Focus on value proposition and total cost of ownership\"\n",
    "        channels[2] = \"Value-oriented promotional campaigns\"\n",
    "    \n",
    "    # Adjust for segment size\n",
    "    if size_category == \"micro\":\n",
    "        channels = [\n",
    "            \"Highly targeted digital marketing\",\n",
    "            \"Specialized publications and platforms\",\n",
    "            \"Niche community engagement\",\n",
    "            \"One-to-one personalized marketing\"\n",
    "        ]\n",
    "    elif size_category == \"niche\":\n",
    "        channels[0] = \"Targeted marketing to specific segments\"\n",
    "    elif size_category == \"mass\":\n",
    "        channels[0] = \"Broad market mass media campaigns\"\n",
    "    \n",
    "    # Adjust based on demographics\n",
    "    age = demographics.get('age', 'Mixed')\n",
    "    income = demographics.get('income', 'Mixed')\n",
    "    \n",
    "    if age == \"18-34\":\n",
    "        channels[1] = \"Social media and digital platforms for younger audiences\"\n",
    "        customer_experience[1] = \"Digital-first buying experience\"\n",
    "    elif age == \"55+\":\n",
    "        channels[1] = \"Traditional media and established platforms\"\n",
    "        customer_experience[2] = \"Relationship-based buying experience\"\n",
    "    \n",
    "    if \"High\" in income:\n",
    "        if primary_orientation != 'luxury':\n",
    "            product_focus[0] = \"Premium models and higher trim levels\"\n",
    "    elif \"Low\" in income:\n",
    "        if primary_orientation != 'value':\n",
    "            product_focus[2] = \"Value-oriented models with essential features\"\n",
    "    \n",
    "    # Adjust based on buying behavior\n",
    "    for behavior in buying_behavior:\n",
    "        if \"research\" in behavior.lower():\n",
    "            channels.append(\"Information-rich platforms and content marketing\")\n",
    "            customer_experience[1] = \"Detailed product information and comparison tools\"\n",
    "        elif \"price\" in behavior.lower() and primary_orientation != 'value':\n",
    "            messaging[1] = \"Emphasize value proposition and competitive pricing\"\n",
    "        elif \"leasing\" in behavior.lower():\n",
    "            product_focus.append(\"Models with attractive lease residuals\")\n",
    "            customer_experience[2] = \"Streamlined leasing programs\"\n",
    "        elif \"technology\" in behavior.lower() and primary_orientation != 'technology':\n",
    "            product_focus[2] = \"Models with appealing technology packages\"\n",
    "        elif \"brand\" in behavior.lower():\n",
    "            messaging[3] = \"Emphasize brand heritage and reputation\"\n",
    "            customer_experience[3] = \"Brand-focused ownership experience\"\n",
    "    \n",
    "    # Compile final marketing strategy\n",
    "    marketing_strategy = {\n",
    "        'messaging': list(set(messaging))[:4],  # Remove duplicates and limit to 4\n",
    "        'channels': list(set(channels))[:4],\n",
    "        'product_focus': list(set(product_focus))[:4],\n",
    "        'customer_experience': list(set(customer_experience))[:4]\n",
    "    }\n",
    "    \n",
    "    return marketing_strategy\n",
    "\n",
    "def generate_segment_visualizations(segment_insights, report_dir, k):\n",
    "    \"\"\"\n",
    "    Generate visualizations for the marketing report\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    segment_insights : dict\n",
    "        Dictionary with segment insights\n",
    "    report_dir : str\n",
    "        Directory to save visualizations\n",
    "    k : int\n",
    "        Number of clusters\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary mapping visualization types to file paths\n",
    "    \"\"\"\n",
    "    log_status(f\"Generating visualizations for {k} segments...\")\n",
    "    visualization_paths = {}\n",
    "    \n",
    "    # Create segment names and sizes for easier access\n",
    "    segment_names = {}\n",
    "    segment_sizes = {}\n",
    "    segment_colors = {}\n",
    "    \n",
    "    # Standard colors for visualizations\n",
    "    colors = ['#3366cc', '#dc3912', '#ff9900', '#109618', '#990099', '#0099c6', \n",
    "              '#dd4477', '#66aa00', '#b82e2e', '#316395', '#994499', '#22aa99',\n",
    "              '#aaaa11', '#6633cc', '#e67300', '#8b0707', '#651067', '#329262',\n",
    "              '#5574a6', '#3b3eac']\n",
    "    \n",
    "    # Ensure we have enough colors\n",
    "    while len(colors) < k:\n",
    "        colors.extend(colors[:k-len(colors)])\n",
    "    \n",
    "    # Prepare data for visualizations\n",
    "    for i in range(k):\n",
    "        if i in segment_insights['segments']:\n",
    "            segment = segment_insights['segments'][i]\n",
    "            segment_names[i] = segment['name']\n",
    "            segment_sizes[i] = segment['size_pct']\n",
    "            segment_colors[i] = colors[i]\n",
    "    \n",
    "    # 1. Segment Distribution Visualization\n",
    "    try:\n",
    "        log_status(\"Creating segment distribution chart...\")\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        \n",
    "        # Create bar chart of segment sizes\n",
    "        bars = plt.bar(\n",
    "            [segment_names[i] for i in range(k)],\n",
    "            [segment_sizes[i] for i in range(k)],\n",
    "            color=[segment_colors[i] for i in range(k)]\n",
    "        )\n",
    "        \n",
    "        # Add percentage labels above each bar\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            plt.text(bar.get_x() + bar.get_width()/2., height + 0.3,\n",
    "                   f'{height:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        # Add count labels inside each bar\n",
    "        total_records = 50000  # Placeholder - would use actual count in real implementation\n",
    "        for i, bar in enumerate(bars):\n",
    "            height = bar.get_height()\n",
    "            count = int(height * total_records / 100)\n",
    "            plt.text(bar.get_x() + bar.get_width()/2., height/2,\n",
    "                   f'{count:,}', ha='center', va='center', \n",
    "                   color='white', fontweight='bold')\n",
    "        \n",
    "        # Customize the plot\n",
    "        plt.title('Vehicle Customer Segment Distribution', fontsize=16, pad=20)\n",
    "        plt.ylabel('Percentage of Customers', fontsize=12)\n",
    "        plt.ylim(0, max(segment_sizes.values()) * 1.1)\n",
    "        \n",
    "        # Rotate x-axis labels for better readability\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        \n",
    "        # Adjust layout\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the figure\n",
    "        distribution_path = f\"{report_dir}/vehicle_segment_distribution_{k}.jpg\"\n",
    "        plt.savefig(distribution_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        visualization_paths['distribution'] = distribution_path\n",
    "        log_status(f\"Saved segment distribution chart to {distribution_path}\")\n",
    "    except Exception as e:\n",
    "        log_status(f\"Error creating distribution visualization: {str(e)}\")\n",
    "    \n",
    "    # 2. Segment Quadrant Map Visualization\n",
    "    try:\n",
    "        log_status(\"Creating segment quadrant map...\")\n",
    "        plt.figure(figsize=(14, 12))\n",
    "        \n",
    "        # Define segment positioning on the quadrant map\n",
    "        # Calculate based on luxury and tech scores from key_features\n",
    "        segment_positions = {}\n",
    "        \n",
    "        for i in range(k):\n",
    "            if i in segment_insights['segments']:\n",
    "                luxury_score = segment_insights['segments'][i]['key_features']['luxury_orientation']\n",
    "                tech_score = segment_insights['segments'][i]['key_features']['tech_adoption']\n",
    "                \n",
    "                # Scale to -5 to 5 range for both axes\n",
    "                x = (luxury_score - 50) / 10\n",
    "                y = (tech_score - 50) / 10\n",
    "                \n",
    "                # Add some randomness to prevent overlap\n",
    "                x += np.random.uniform(-0.5, 0.5)\n",
    "                y += np.random.uniform(-0.5, 0.5)\n",
    "                \n",
    "                segment_positions[i] = {\"x\": x, \"y\": y}\n",
    "        \n",
    "        # Plot the segments as bubbles\n",
    "        for i in range(k):\n",
    "            if i in segment_positions:\n",
    "                plt.scatter(\n",
    "                    segment_positions[i][\"x\"],\n",
    "                    segment_positions[i][\"y\"],\n",
    "                    s=segment_sizes[i] * 50,\n",
    "                    color=segment_colors[i],\n",
    "                    alpha=0.7,\n",
    "                    edgecolor='white',\n",
    "                    linewidth=1\n",
    "                )\n",
    "                # Add cluster number labels\n",
    "                plt.text(\n",
    "                    segment_positions[i][\"x\"],\n",
    "                    segment_positions[i][\"y\"],\n",
    "                    str(i),\n",
    "                    ha='center',\n",
    "                    va='center',\n",
    "                    fontweight='bold',\n",
    "                    color='white'\n",
    "                )\n",
    "                # Add segment name labels\n",
    "                plt.text(\n",
    "                    segment_positions[i][\"x\"],\n",
    "                    segment_positions[i][\"y\"] - 0.6,\n",
    "                    f\"{segment_names[i]}\\n({segment_sizes[i]:.1f}%)\",\n",
    "                    ha='center',\n",
    "                    va='top',\n",
    "                    fontsize=8,\n",
    "                    color='black'\n",
    "                )\n",
    "        \n",
    "        # Add quadrant labels\n",
    "        plt.text(3, 3, 'Tech-Forward Premium', ha='center', va='center', fontsize=12, fontweight='bold', color='#555555')\n",
    "        plt.text(-3, 3, 'Tech-Forward Value', ha='center', va='center', fontsize=12, fontweight='bold', color='#555555')\n",
    "        plt.text(3, -3, 'Traditional Premium', ha='center', va='center', fontsize=12, fontweight='bold', color='#555555')\n",
    "        plt.text(-3, -3, 'Traditional Value', ha='center', va='center', fontsize=12, fontweight='bold', color='#555555')\n",
    "        \n",
    "        # Add axes labels\n",
    "        plt.xlabel('Luxury Orientation', fontsize=14)\n",
    "        plt.ylabel('Innovation Orientation', fontsize=14)\n",
    "        \n",
    "        # Add title\n",
    "        plt.title('Vehicle Customer Segment Map', fontsize=16, pad=20)\n",
    "        \n",
    "        # Set axis limits\n",
    "        plt.xlim(-6, 6)\n",
    "        plt.ylim(-6, 6)\n",
    "        \n",
    "        # Add gridlines\n",
    "        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "        \n",
    "        # Add origin lines\n",
    "        plt.axhline(y=0, color='gray', linestyle='-', alpha=0.5)\n",
    "        plt.axvline(x=0, color='gray', linestyle='-', alpha=0.5)\n",
    "        \n",
    "        # Add axis descriptions\n",
    "        plt.text(6, 0, 'Premium/Luxury', ha='right', va='center', fontsize=10)\n",
    "        plt.text(-6, 0, 'Economy/Value', ha='left', va='center', fontsize=10)\n",
    "        plt.text(0, 6, 'Innovative/Tech-Forward', ha='center', va='top', fontsize=10)\n",
    "        plt.text(0, -6, 'Traditional', ha='center', va='bottom', fontsize=10)\n",
    "        \n",
    "        # Adjust layout\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the figure\n",
    "        quadrant_path = f\"{report_dir}/vehicle_segment_quadrant_map_{k}.jpg\"\n",
    "        plt.savefig(quadrant_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        visualization_paths['quadrant_map'] = quadrant_path\n",
    "        log_status(f\"Saved segment quadrant map to {quadrant_path}\")\n",
    "    except Exception as e:\n",
    "        log_status(f\"Error creating quadrant map visualization: {str(e)}\")\n",
    "    \n",
    "    # 3. Radar Charts for Segment Profiles\n",
    "    try:\n",
    "        log_status(\"Creating segment radar profiles...\")\n",
    "        \n",
    "        # Helper function for radar charts\n",
    "        def radar_factory(num_vars, frame='circle'):\n",
    "            \"\"\"Create a radar chart with `num_vars` axes.\"\"\"\n",
    "            # Calculate evenly-spaced axis angles\n",
    "            theta = np.linspace(0, 2*np.pi, num_vars, endpoint=False)\n",
    "            \n",
    "            # Create vertices for polygon plots\n",
    "            def unit_poly_verts(theta):\n",
    "                \"\"\"Return vertices of polygon for subplot axes.\"\"\"\n",
    "                x0, y0, r = [0.5] * 3\n",
    "                verts = [(r*np.cos(t) + x0, r*np.sin(t) + y0) for t in theta]\n",
    "                return verts\n",
    "            \n",
    "            # Class for creating a radar chart\n",
    "            class RadarAxes(PolarAxes):\n",
    "                name = 'radar'\n",
    "                \n",
    "                def __init__(self, *args, **kwargs):\n",
    "                    self.theta = theta\n",
    "                    super().__init__(*args, **kwargs)\n",
    "                    self.set_theta_zero_location('N')\n",
    "                    \n",
    "                def fill(self, *args, closed=True, **kwargs):\n",
    "                    return super().fill(self.theta, args[0], closed=closed, **kwargs)\n",
    "                    \n",
    "                def plot(self, *args, **kwargs):\n",
    "                    return super().plot(self.theta, args[0], **kwargs)\n",
    "                    \n",
    "                def set_varlabels(self, labels):\n",
    "                    self.set_thetagrids(np.degrees(self.theta), labels)\n",
    "                    \n",
    "                def _gen_axes_patch(self):\n",
    "                    if frame == 'circle':\n",
    "                        return Circle((0.5, 0.5), 0.5)\n",
    "                    elif frame == 'polygon':\n",
    "                        return RegularPolygon((0.5, 0.5), num_vars, radius=0.5, edgecolor=\"k\")\n",
    "                    else:\n",
    "                        raise ValueError(\"Unknown value for 'frame': %s\" % frame)\n",
    "                        \n",
    "                def _gen_axes_spines(self):\n",
    "                    if frame == 'circle':\n",
    "                        return super()._gen_axes_spines()\n",
    "                    elif frame == 'polygon':\n",
    "                        verts = unit_poly_verts(self.theta)\n",
    "                        verts.append(verts[0])\n",
    "                        path = Path(verts)\n",
    "                        spine = Spine(self, 'circle', path)\n",
    "                        spine.set_transform(Affine2D().scale(.5).translate(.5, .5) + self.transAxes)\n",
    "                        return {'polar': spine}\n",
    "                    else:\n",
    "                        raise ValueError(\"Unknown value for 'frame': %s\" % frame)\n",
    "            \n",
    "            register_projection(RadarAxes)\n",
    "            return theta\n",
    "        \n",
    "        # Define radar chart attributes\n",
    "        radar_attributes = ['Luxury Orientation', 'Price Sensitivity', 'Tech Adoption', \n",
    "                          'Brand Loyalty', 'Family Focus', 'Utility Priority']\n",
    "        n_attributes = len(radar_attributes)\n",
    "        theta = radar_factory(n_attributes, frame='polygon')\n",
    "        \n",
    "        # Calculate grid layout\n",
    "        grid_size = max(2, int(np.ceil(np.sqrt(k))))\n",
    "        \n",
    "        # Create figure for radar charts\n",
    "        fig, axes = plt.subplots(figsize=(16, 16), nrows=grid_size, ncols=grid_size, \n",
    "                                 subplot_kw=dict(projection='radar'))\n",
    "        fig.subplots_adjust(wspace=0.4, hspace=0.4, top=0.85, bottom=0.05)\n",
    "        \n",
    "        # Flatten the axes array for easier iteration\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        # Set plot limits for all axes\n",
    "        for ax in axes:\n",
    "            ax.set_ylim(0, 100)\n",
    "        \n",
    "        # Plot each segment on its own subplot\n",
    "        for i in range(min(k, len(axes))):\n",
    "            if i in segment_insights['segments']:\n",
    "                # Get the key attribute values in order\n",
    "                radar_data = [\n",
    "                    segment_insights['segments'][i]['key_features'].get('luxury_orientation', 50),\n",
    "                    segment_insights['segments'][i]['key_features'].get('price_sensitivity', 50),\n",
    "                    segment_insights['segments'][i]['key_features'].get('tech_adoption', 50),\n",
    "                    segment_insights['segments'][i]['key_features'].get('brand_loyalty', 50),\n",
    "                    segment_insights['segments'][i]['key_features'].get('family_focus', 50),\n",
    "                    segment_insights['segments'][i]['key_features'].get('utility_priority', 50)\n",
    "                ]\n",
    "                \n",
    "                # Plot the radar chart\n",
    "                ax = axes[i]\n",
    "                ax.plot(radar_data, color=segment_colors[i], linewidth=2.5)\n",
    "                ax.fill(radar_data, alpha=0.25, color=segment_colors[i])\n",
    "                ax.set_title(segment_names[i], size=11, y=1.1, color=segment_colors[i], fontweight='bold')\n",
    "                ax.set_varlabels(radar_attributes)\n",
    "                \n",
    "                # Customize grid lines\n",
    "                ax.set_rgrids([20, 40, 60, 80], labels=['20', '40', '60', '80'], angle=0, fontsize=7)\n",
    "                \n",
    "                # Rotate attribute labels for better readability\n",
    "                for label, angle in zip(ax.get_xticklabels(), theta):\n",
    "                    if angle in (0, np.pi):\n",
    "                        label.set_horizontalalignment('center')\n",
    "                    elif 0 < angle < np.pi:\n",
    "                        label.set_horizontalalignment('left')\n",
    "                    else:\n",
    "                        label.set_horizontalalignment('right')\n",
    "                    label.set_fontsize(8)\n",
    "        \n",
    "        # Hide empty subplots\n",
    "        for i in range(k, len(axes)):\n",
    "            axes[i].set_visible(False)\n",
    "        \n",
    "        # Add main title\n",
    "        fig.suptitle('Vehicle Customer Segment Profiles', fontsize=16, fontweight='bold', y=0.98)\n",
    "        fig.text(0.5, 0.93, 'Based on k-modes clustering analysis', \n",
    "                 horizontalalignment='center', fontsize=10)\n",
    "        \n",
    "        # Adjust layout\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.9])\n",
    "        \n",
    "        # Save the figure\n",
    "        radar_path = f\"{report_dir}/vehicle_segment_radar_profiles_{k}.jpg\"\n",
    "        plt.savefig(radar_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        visualization_paths['radar_profiles'] = radar_path\n",
    "        log_status(f\"Saved segment radar profiles to {radar_path}\")\n",
    "    except Exception as e:\n",
    "        log_status(f\"Error creating radar profiles visualization: {str(e)}\")\n",
    "    \n",
    "    # 4. Marketing Strategy Recommendations\n",
    "    try:\n",
    "        log_status(\"Creating segment strategy recommendations visual...\")\n",
    "        \n",
    "        # Function to create a strategy card for a segment\n",
    "        def create_strategy_card(segment_id, ax):\n",
    "            \"\"\"Create a marketing strategy card visualization for a segment\"\"\"\n",
    "            segment = segment_insights['segments'].get(segment_id)\n",
    "            if not segment:\n",
    "                ax.set_visible(False)\n",
    "                return\n",
    "            segment_name = segment['name']\n",
    "            data = segment\n",
    "            color = segment_colors.get(segment_id, '#333333')\n",
    "            \n",
    "            # Set title\n",
    "            ax.set_title(segment_name, fontsize=12, fontweight='bold', color=color, pad=10)\n",
    "            \n",
    "            # Remove axes\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            ax.spines['top'].set_visible(False)\n",
    "            ax.spines['right'].set_visible(False)\n",
    "            ax.spines['bottom'].set_visible(False)\n",
    "            ax.spines['left'].set_visible(False)\n",
    "            \n",
    "            # Background color\n",
    "            ax.set_facecolor('#f8f9fa')\n",
    "            \n",
    "            # Segment size\n",
    "            ax.text(0.02, 0.98, f\"Segment size: {data['size_pct']:.1f}%\", transform=ax.transAxes,\n",
    "                    fontsize=10, verticalalignment='top', horizontalalignment='left',\n",
    "                    bbox=dict(facecolor='white', alpha=0.7, boxstyle='round,pad=0.3'))\n",
    "            \n",
    "            # Segment demographics summary\n",
    "            demographics = f\"Demographics: {data['demographics'].get('age', 'Various')} • {data['demographics'].get('income', 'Various')}\"\n",
    "            ax.text(0.02, 0.91, demographics, transform=ax.transAxes, \n",
    "                    fontsize=9, verticalalignment='top', horizontalalignment='left',\n",
    "                    bbox=dict(facecolor='white', alpha=0.7, boxstyle='round,pad=0.3'))\n",
    "            \n",
    "            # Create text blocks for each strategy section\n",
    "            ypositions = [0.85, 0.60, 0.35, 0.10]\n",
    "            heights = [0.15, 0.15, 0.15, 0.15]\n",
    "            titles = ['Key Messaging', 'Marketing Channels', 'Product Focus', 'Customer Experience']\n",
    "            content_lists = [\n",
    "                data['marketing_strategy'].get('messaging', []),\n",
    "                data['marketing_strategy'].get('channels', []),\n",
    "                data['marketing_strategy'].get('product_focus', []),\n",
    "                data['marketing_strategy'].get('customer_experience', [])\n",
    "            ]\n",
    "            \n",
    "            for ypos, height, title, content in zip(ypositions, heights, titles, content_lists):\n",
    "                # Section background\n",
    "                rect = plt.Rectangle((0.05, ypos-height), 0.9, height, \n",
    "                                   fill=True, color='white', alpha=0.8,\n",
    "                                   transform=ax.transAxes, zorder=1,\n",
    "                                   linewidth=1, edgecolor='#dddddd')\n",
    "                ax.add_patch(rect)\n",
    "                \n",
    "                # Section title\n",
    "                ax.text(0.07, ypos-0.03, title, transform=ax.transAxes,\n",
    "                        fontsize=10, fontweight='bold', verticalalignment='top',\n",
    "                        horizontalalignment='left', zorder=2)\n",
    "                \n",
    "                # Section content as bullet points\n",
    "                for i, item in enumerate(content[:4]):  # Limit to 4 items\n",
    "                    ax.text(0.09, ypos-0.06-(i*0.025), f\"• {item}\", transform=ax.transAxes,\n",
    "                            fontsize=8, verticalalignment='top', horizontalalignment='left',\n",
    "                            zorder=2, wrap=True)\n",
    "        \n",
    "        # Create marketing strategy cards visualization\n",
    "        fig = plt.figure(figsize=(16, 20))\n",
    "        \n",
    "        # Calculate grid dimensions\n",
    "        grid_cols = min(3, k)\n",
    "        grid_rows = int(np.ceil(k / grid_cols))\n",
    "        \n",
    "        # Create a GridSpec\n",
    "        gs = GridSpec(grid_rows, grid_cols, figure=fig, hspace=0.4, wspace=0.3)\n",
    "        \n",
    "        # Title for the entire figure\n",
    "        fig.suptitle('Marketing Strategy Recommendations by Segment', fontsize=16, fontweight='bold', y=0.98)\n",
    "        fig.text(0.5, 0.955, 'Based on k-modes clustering analysis of customer vehicle preferences',\n",
    "                 horizontalalignment='center', fontsize=10)\n",
    "        \n",
    "        # Create a subplot and strategy card for each segment\n",
    "        for i in range(k):\n",
    "            if i in segment_insights['segments']:\n",
    "                # Calculate grid position\n",
    "                row = i // grid_cols\n",
    "                col = i % grid_cols\n",
    "                \n",
    "                # Create subplot\n",
    "                ax = fig.add_subplot(gs[row, col])\n",
    "                \n",
    "                # Create strategy card\n",
    "                create_strategy_card(i, ax)\n",
    "        \n",
    "        # Adjust layout\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.94])\n",
    "        \n",
    "        # Save the figure\n",
    "        strategy_path = f\"{report_dir}/vehicle_segment_marketing_strategies_{k}.jpg\"\n",
    "        plt.savefig(strategy_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        visualization_paths['marketing_strategies'] = strategy_path\n",
    "        log_status(f\"Saved marketing strategies visualization to {strategy_path}\")\n",
    "    except Exception as e:\n",
    "        log_status(f\"Error creating marketing strategies visualization: {str(e)}\")\n",
    "    \n",
    "    return visualization_paths\n",
    "\n",
    "def create_marketing_report_document(segment_insights, visualization_paths, report_dir, k):\n",
    "    \"\"\"\n",
    "    Create a comprehensive marketing report document based on clustering results\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    segment_insights : dict\n",
    "        Dictionary with segment insights\n",
    "    visualization_paths : dict\n",
    "        Dictionary with paths to visualizations\n",
    "    report_dir : str\n",
    "        Directory to save the report\n",
    "    k : int\n",
    "        Number of clusters\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        Path to the generated report\n",
    "    \"\"\"\n",
    "    log_status(\"Creating comprehensive marketing report document...\")\n",
    "    \n",
    "    # Create report file path\n",
    "    report_path = f\"{report_dir}/Vehicle_Customer_Segmentation_Report_{k}.md\"\n",
    "    \n",
    "    # Generate report content\n",
    "    report_content = f\"\"\"# Vehicle Customer Segmentation & Marketing Strategy\n",
    "## Based on K-modes Clustering Analysis\n",
    "\n",
    "*Prepared: {time.strftime('%B %d, %Y')}*\n",
    "\n",
    "---\n",
    "\n",
    "## Executive Summary\n",
    "\n",
    "This report presents the results of advanced k-modes clustering analysis performed on customer vehicle preference data. We identified {k} distinct customer segments with unique characteristics, behaviors, and purchase drivers. These segments provide a foundation for targeted marketing strategies that can enhance campaign effectiveness, improve customer experience, and optimize product development and positioning.\n",
    "\n",
    "The analysis reveals several key insights:\n",
    "\n",
    "1. The market shows distinct segmentation by luxury orientation, price sensitivity, and technology adoption\n",
    "2. Family-focused customers represent a substantial portion of the market with specific needs\n",
    "3. Technology adoption is a significant differentiator across segments\n",
    "4. Price sensitivity varies significantly and correlates strongly with preferred vehicle types\n",
    "5. Marketing channel effectiveness differs substantially between segments, suggesting the need for tailored approaches\n",
    "\n",
    "The {k} identified segments represent natural groupings based on multiple customer attributes including demographics, behaviors, preferences, and purchase patterns. Each segment has been analyzed to create actionable marketing personas with specific recommendations.\n",
    "\n",
    "---\n",
    "\n",
    "## Identified Customer Segments\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    # Add visualization reference if available\n",
    "    if 'distribution' in visualization_paths:\n",
    "        report_content += f\"![Vehicle Customer Segment Distribution]({os.path.basename(visualization_paths['distribution'])})\\n\\n\"\n",
    "    \n",
    "    report_content += f\"Our k-modes clustering analysis revealed {k} distinct customer segments:\\n\\n\"\n",
    "    \n",
    "    # Add segment summaries\n",
    "    for i in range(k):\n",
    "        if i in segment_insights['segments']:\n",
    "            segment = segment_insights['segments'][i]\n",
    "            report_content += f\"**{i+1}. {segment['name']} ({segment['size_pct']:.1f}%)**: \"\n",
    "            \n",
    "            # Create a brief description based on key features\n",
    "            key_features = segment['key_features']\n",
    "            vehicle_prefs = segment['vehicle_preferences']\n",
    "            \n",
    "            luxury_level = \"luxury\" if key_features['luxury_orientation'] > 70 else \\\n",
    "                          \"premium\" if key_features['luxury_orientation'] > 50 else \\\n",
    "                          \"mainstream\" if key_features['luxury_orientation'] > 30 else \"value-oriented\"\n",
    "                          \n",
    "            tech_level = \"tech-forward\" if key_features['tech_adoption'] > 70 else \\\n",
    "                         \"tech-savvy\" if key_features['tech_adoption'] > 50 else \\\n",
    "                         \"technology-conscious\" if key_features['tech_adoption'] > 30 else \"\"\n",
    "                         \n",
    "            price_level = \"price-insensitive\" if key_features['price_sensitivity'] < 30 else \\\n",
    "                         \"value-conscious\" if key_features['price_sensitivity'] > 70 else \"\"\n",
    "                         \n",
    "            family_focus = \"family-oriented\" if key_features['family_focus'] > 70 else \"\"\n",
    "            \n",
    "            # Combine descriptors into a brief segment description\n",
    "            descriptors = [d for d in [luxury_level, tech_level, price_level, family_focus] if d]\n",
    "            description = f\"{' '.join(descriptors).capitalize()} consumers \"\n",
    "            \n",
    "            # Add vehicle preference if available\n",
    "            if vehicle_prefs:\n",
    "                description += f\"seeking {vehicle_prefs[0].lower()}\"\n",
    "            else:\n",
    "                description += \"with distinctive automotive preferences\"\n",
    "                \n",
    "            report_content += f\"{description}\\n\\n\"\n",
    "    \n",
    "    # Market segment positioning section\n",
    "    report_content += \"---\\n\\n## Market Segment Positioning\\n\\n\"\n",
    "    \n",
    "    # Add visualization reference if available\n",
    "    if 'quadrant_map' in visualization_paths:\n",
    "        report_content += f\"![Vehicle Customer Segment Map]({os.path.basename(visualization_paths['quadrant_map'])})\\n\\n\"\n",
    "    \n",
    "    report_content += f\"\"\"The quadrant map above visualizes how our {k} segments position along two critical dimensions:\n",
    "- **Horizontal Axis**: Luxury Orientation (economy/value to premium/luxury)\n",
    "- **Vertical Axis**: Innovation Orientation (traditional to tech-forward)\n",
    "\n",
    "This positioning reveals four primary market categories:\n",
    "- **Tech-Forward Premium**: Segments focused on cutting-edge technology and premium features\n",
    "- **Tech-Forward Value**: Segments embracing technology at more accessible price points \n",
    "- **Traditional Premium**: Segments prioritizing luxury and established prestige\n",
    "- **Traditional Value**: Segments focused on practicality, reliability, and value\n",
    "\n",
    "Understanding this positioning is crucial for developing tailored marketing strategies and appropriate product positioning for each segment.\n",
    "\n",
    "---\n",
    "\n",
    "## Segment Profiles and Key Attributes\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    # Add visualization reference if available\n",
    "    if 'radar_profiles' in visualization_paths:\n",
    "        report_content += f\"![Vehicle Customer Segment Profiles]({os.path.basename(visualization_paths['radar_profiles'])})\\n\\n\"\n",
    "    \n",
    "    report_content += \"\"\"The radar charts illustrate the distinct profiles of each customer segment across six key attributes:\n",
    "- **Luxury Orientation**: Preference for premium features and prestigious brands\n",
    "- **Price Sensitivity**: Importance of cost in purchase decisions\n",
    "- **Tech Adoption**: Openness to new technologies and digital features\n",
    "- **Brand Loyalty**: Tendency to stick with preferred brands\n",
    "- **Family Focus**: Prioritization of family needs in vehicle decisions\n",
    "- **Utility Priority**: Emphasis on practical functionality and versatility\n",
    "\n",
    "These profiles highlight the unique combinations of priorities and preferences that define each segment, allowing for more targeted marketing approaches.\n",
    "\n",
    "---\n",
    "\n",
    "## Detailed Segment Analysis\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    # Add detailed segment profiles\n",
    "    for i in range(k):\n",
    "        if i in segment_insights['segments']:\n",
    "            segment = segment_insights['segments'][i]\n",
    "            \n",
    "            report_content += f\"### Segment {i}: {segment['name']} ({segment['size_pct']:.1f}%)\\n\\n\"\n",
    "            \n",
    "            # Demographics section\n",
    "            report_content += \"**Demographics:** \\n\"\n",
    "            for key, value in segment['demographics'].items():\n",
    "                if key not in ['gender_split', 'geography']:  # Keep it focused\n",
    "                    report_content += f\"- {key.replace('_', ' ').title()}: {value}\\n\"\n",
    "            report_content += \"\\n\"\n",
    "            \n",
    "            # Characteristics section\n",
    "            report_content += \"**Characteristics:**\\n\"\n",
    "            key_features = segment['key_features']\n",
    "            report_content += f\"- {'Very high' if key_features['luxury_orientation'] > 80 else 'High' if key_features['luxury_orientation'] > 60 else 'Medium' if key_features['luxury_orientation'] > 40 else 'Low'} luxury orientation ({key_features['luxury_orientation']}/100)\\n\"\n",
    "            report_content += f\"- {'Very high' if key_features['price_sensitivity'] > 80 else 'High' if key_features['price_sensitivity'] > 60 else 'Medium' if key_features['price_sensitivity'] > 40 else 'Low'} price sensitivity ({key_features['price_sensitivity']}/100)\\n\"\n",
    "            report_content += f\"- {'Very high' if key_features['tech_adoption'] > 80 else 'High' if key_features['tech_adoption'] > 60 else 'Medium' if key_features['tech_adoption'] > 40 else 'Low'} tech adoption ({key_features['tech_adoption']}/100)\\n\"\n",
    "            report_content += f\"- {'Very high' if key_features['brand_loyalty'] > 80 else 'High' if key_features['brand_loyalty'] > 60 else 'Medium' if key_features['brand_loyalty'] > 40 else 'Low'} brand loyalty ({key_features['brand_loyalty']}/100)\\n\"\n",
    "            if key_features['family_focus'] > 40:\n",
    "                report_content += f\"- {'Very high' if key_features['family_focus'] > 80 else 'High' if key_features['family_focus'] > 60 else 'Medium'} family focus ({key_features['family_focus']}/100)\\n\"\n",
    "            if key_features['utility_priority'] > 40:\n",
    "                report_content += f\"- {'Very high' if key_features['utility_priority'] > 80 else 'High' if key_features['utility_priority'] > 60 else 'Medium'} utility priority ({key_features['utility_priority']}/100)\\n\"\n",
    "            report_content += \"\\n\"\n",
    "            \n",
    "            # Vehicle preferences section\n",
    "            report_content += \"**Vehicle Preferences:**\\n\"\n",
    "            for pref in segment['vehicle_preferences']:\n",
    "                report_content += f\"- {pref}\\n\"\n",
    "            report_content += \"\\n\"\n",
    "            \n",
    "            # Buying behavior section\n",
    "            report_content += \"**Buying Behavior:**\\n\"\n",
    "            for behavior in segment['buying_behavior']:\n",
    "                report_content += f\"- {behavior}\\n\"\n",
    "            report_content += \"\\n\"\n",
    "            \n",
    "            # Marketing strategy section\n",
    "            report_content += \"**Marketing Strategy:**\\n\"\n",
    "            for message in segment['marketing_strategy']['messaging'][:3]:  # Limit to top 3\n",
    "                report_content += f\"- {message}\\n\"\n",
    "            for channel in segment['marketing_strategy']['channels'][:2]:  # Limit to top 2\n",
    "                report_content += f\"- Utilize {channel.lower()}\\n\"\n",
    "            report_content += \"\\n\"\n",
    "    \n",
    "    # Marketing recommendations section\n",
    "    report_content += \"---\\n\\n## Marketing Strategy Recommendations\\n\\n\"\n",
    "    \n",
    "    # Add visualization reference if available\n",
    "    if 'marketing_strategies' in visualization_paths:\n",
    "        report_content += f\"![Marketing Strategy Recommendations by Segment]({os.path.basename(visualization_paths['marketing_strategies'])})\\n\\n\"\n",
    "    \n",
    "    report_content += \"\"\"Each segment requires a tailored marketing approach to effectively engage its members. The visualization above provides specific recommendations for:\n",
    "\n",
    "1. **Key Messaging**: The most compelling message themes and content for each segment\n",
    "2. **Marketing Channels**: The most effective channels and media for reaching each segment\n",
    "3. **Product Focus**: The vehicle types and features to emphasize for each segment\n",
    "4. **Customer Experience**: The dealership and service experiences that will resonate with each segment\n",
    "\n",
    "---\n",
    "\n",
    "## Strategic Recommendations\n",
    "\n",
    "Based on the comprehensive segmentation analysis, we recommend the following strategies:\n",
    "\n",
    "### 1. Segment-Specific Product Development\n",
    "\n",
    "- **Luxury Tech Integration**: Develop high-end models with advanced technology to capture luxury-focused segments\n",
    "- **Family-Friendly Innovation**: Create products balancing practicality with modern features for family segments\n",
    "- **Value-Focused Technology**: Introduce affordable models with strategic tech features for value-conscious segments\n",
    "- **Sustainable Options**: Expand eco-friendly lineup to target environmentally-conscious segments\n",
    "\n",
    "### 2. Marketing Campaign Optimization\n",
    "\n",
    "- **Channel Alignment**: Match marketing channel mix to segment preferences\n",
    "- **Message Tailoring**: Customize messaging to address specific segment priorities and pain points\n",
    "- **Visual Language**: Develop distinct visual approaches for different segments across all touchpoints\n",
    "- **Timing Strategies**: Optimize campaign timing based on segment purchase cycles\n",
    "\n",
    "### 3. Customer Experience Enhancements\n",
    "\n",
    "- **Digital Transformation**: Accelerate digital experience development for tech-forward segments\n",
    "- **Showroom Evolution**: Create segment-specific zones or experiences within dealerships\n",
    "- **Service Differentiation**: Develop tiered service models aligned with segment expectations\n",
    "- **Community Building**: Foster segment-specific communities, especially for lifestyle segments\n",
    "\n",
    "### 4. Dealer Support Programs\n",
    "\n",
    "- **Segment Training**: Train sales staff on segment identification and appropriate approaches\n",
    "- **Inventory Mix**: Guide dealers on optimal inventory distribution based on local segment composition\n",
    "- **Pricing Strategy**: Develop segment-responsive pricing and promotion guidelines\n",
    "- **Performance Measurement**: Track conquest and retention by segment to refine strategies\n",
    "\n",
    "---\n",
    "\n",
    "## Implementation Roadmap\n",
    "\n",
    "**Phase 1: Foundation (Month 1-2)**\n",
    "- Finalize segment definitions and profiles\n",
    "- Develop segmentation scoring model\n",
    "- Create segment identification tools for dealers\n",
    "- Establish segment-based KPIs and measurement\n",
    "\n",
    "**Phase 2: Integration (Month 3-4)**\n",
    "- Incorporate segmentation into marketing planning\n",
    "- Initiate segment-specific creative development\n",
    "- Begin dealer training program rollout\n",
    "- Launch first targeted digital campaigns\n",
    "\n",
    "**Phase 3: Expansion (Month 5-6)**\n",
    "- Implement full multi-channel segment strategies\n",
    "- Complete dealer training and support materials\n",
    "- Introduce segment-specific customer journeys\n",
    "- Launch product development initiatives\n",
    "\n",
    "**Phase 4: Refinement (Month 7-12)**\n",
    "- Analyze segment performance and response\n",
    "- Refine targeting models based on initial results\n",
    "- Optimize channel mix and marketing spend\n",
    "- Develop next generation of segment-based strategies\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    report_content += f\"\"\"The {k}-segment model provides a comprehensive framework for understanding the diverse customer landscape. By implementing targeted strategies for each segment, we can achieve:\n",
    "\n",
    "- **Improved Marketing ROI**: More efficient allocation of marketing resources\n",
    "- **Enhanced Customer Acquisition**: More compelling, relevant messaging\n",
    "- **Increased Customer Retention**: Better alignment with customer needs and expectations\n",
    "- **Product Development Guidance**: Clear direction for future vehicle development\n",
    "\n",
    "This segmentation should be viewed as a living framework that will evolve as market conditions change and new data becomes available. Regular refinement of the model will ensure continued relevance and effectiveness.\n",
    "\n",
    "---\n",
    "\n",
    "*End of Report*\n",
    "\"\"\"\n",
    "    \n",
    "    # Write the report to a file\n",
    "    with open(report_path, 'w') as f:\n",
    "        f.write(report_content)\n",
    "    \n",
    "    log_status(f\"Marketing report document created at: {report_path}\")\n",
    "    \n",
    "    return report_path\n",
    "\n",
    "def extend_kmodes_with_marketing_report(kmodes_results):\n",
    "    \"\"\"\n",
    "    Extend k-modes clustering with marketing report generation.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    kmodes_results : dict\n",
    "        The results from k-modes clustering run\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        Path to the generated marketing report\n",
    "    \"\"\"\n",
    "    if not kmodes_results:\n",
    "        log_status(\"No k-modes clustering results available for marketing report generation\")\n",
    "        return None\n",
    "    \n",
    "    log_status(\"Generating comprehensive marketing segmentation report...\")\n",
    "    \n",
    "    try:\n",
    "        # Extract key information\n",
    "        cluster_data = kmodes_results.get('cluster_data')\n",
    "        centroids_df = kmodes_results.get('centroids_df')\n",
    "        num_clusters = kmodes_results.get('num_clusters')\n",
    "        \n",
    "        if not cluster_data is None and not centroids_df is None and num_clusters:\n",
    "            # Create report directory\n",
    "            report_dir = f\"/dbfs/FileStore/acxiom_clustering/kmodes_marketing_report_{num_clusters}\"\n",
    "            os.makedirs(report_dir, exist_ok=True)\n",
    "            \n",
    "            # Analyze clusters to generate marketing insights\n",
    "            segment_insights = analyze_kmodes_segments(cluster_data, centroids_df, num_clusters)\n",
    "            \n",
    "            # Generate visualizations\n",
    "            visualization_paths = generate_segment_visualizations(segment_insights, report_dir, num_clusters)\n",
    "            \n",
    "            # Create the comprehensive report\n",
    "            report_path = create_marketing_report_document(segment_insights, visualization_paths, report_dir, num_clusters)\n",
    "            \n",
    "            log_status(f\"✓ Marketing segmentation report successfully generated at: {report_path}\")\n",
    "            return report_path\n",
    "        else:\n",
    "            log_status(\"Missing required data in k-modes results for marketing report generation\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        log_status(f\"ERROR generating marketing report: {str(e)}\")\n",
    "        log_status(traceback.format_exc())\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "077cf84b-8cc8-49a0-bca9-dfbdf113eb1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# CELL 28: Run kmodes\n",
    "\n",
    "def run_kmodes_with_marketing_report(cluster_counts=None, sample_size=None):\n",
    "    \"\"\"\n",
    "    Run K-modes clustering with comprehensive marketing report generation\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    cluster_counts : list, optional\n",
    "        List of cluster counts to try (default [8, 10, 12])\n",
    "    sample_size : int, optional\n",
    "        Sample size to use for clustering\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    bool\n",
    "        True if successful, False otherwise\n",
    "    \"\"\"\n",
    "    # Use default values if not specified\n",
    "    if cluster_counts is None:\n",
    "        cluster_counts = [8, 10, 12]\n",
    "    \n",
    "    if sample_size is None:\n",
    "        sample_size = GLOBAL_CONFIG.get('clustering_sample_size', 50000)\n",
    "    \n",
    "    log_status(f\"===== EXECUTING K-MODES CLUSTERING WITH MARKETING REPORT =====\")\n",
    "    log_status(f\"Cluster counts: {cluster_counts}\")\n",
    "    log_status(f\"Sample size: {sample_size}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # First check if kmodes package is available\n",
    "        if not install_kmodes_if_needed():\n",
    "            log_status(\"ERROR: Required kmodes package not available\")\n",
    "            return False\n",
    "        \n",
    "        # Run k-modes with multiple cluster counts\n",
    "        results = run_kmodes_multiple(\n",
    "            cluster_counts=cluster_counts,\n",
    "            sample_size=sample_size\n",
    "        )\n",
    "        \n",
    "        if not results:\n",
    "            log_status(\"ERROR: K-modes clustering failed\")\n",
    "            return False\n",
    "        \n",
    "        # Generate segment names for best cluster solution (lowest cost)\n",
    "        cost_values = {k: result['cost'] for k, result in results.items()}\n",
    "        best_k = min(cost_values, key=cost_values.get)\n",
    "        \n",
    "        log_status(f\"Selected optimal clustering with k={best_k} clusters\")\n",
    "        \n",
    "        # Generate marketing report for best clustering\n",
    "        best_result = results[best_k]\n",
    "        \n",
    "        # Add segment names to best clustering result\n",
    "        segment_names = generate_kmodes_segment_names(best_result)\n",
    "        if segment_names:\n",
    "            log_status(f\"Generated {len(segment_names)} segment names for optimal clustering\")\n",
    "        \n",
    "        # Generate marketing report\n",
    "        report_path = generate_kmodes_marketing_report(best_result)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        total_time = end_time - start_time\n",
    "        \n",
    "        if report_path:\n",
    "            log_status(f\"✅ K-modes clustering and marketing report completed in {total_time:.2f} seconds\")\n",
    "            log_status(f\"Marketing report saved to: {report_path}\")\n",
    "            return True\n",
    "        else:\n",
    "            log_status(f\"❌ Marketing report generation failed after {total_time:.2f} seconds\")\n",
    "            return False\n",
    "        \n",
    "    except Exception as e:\n",
    "        log_status(f\"ERROR in k-modes clustering: {str(e)}\")\n",
    "        log_status(traceback.format_exc())\n",
    "        return False\n",
    "    \n",
    "\n",
    "# Example usage:\n",
    "# Run K-Prototypes with 8 clusters only and increased weight for categorical variables\n",
    "run_just_kprototypes(cluster_counts=[8], categorical_weight=0.8)\n",
    "\n",
    "# Run K-Prototypes with default settings (8, 10, 12 clusters)\n",
    "# run_just_kprototypes()\n",
    "\n",
    "# Run K-Prototypes with multiple cluster counts and a smaller sample size\n",
    "# run_just_kprototypes(cluster_counts=[5, 8, 10, 12, 15], sample_size=30000)\n",
    "\n",
    "#(cluster_counts=[4])\n",
    "#run_just_kmodes(cluster_counts=[5, 8, 10])\n",
    "#compare_kmodes_with_other_methods(sample_size=15000, num_clusters=8)\n",
    "#determine_optimal_k_for_kmodes_and_run(sample_size = GLOBAL_CONFIG.get('clustering_sample_size', 50000), max_k=15)\n",
    "\n",
    "#run_kmodes_with_marketing_report(cluster_counts=[4], sample_size = GLOBAL_CONFIG.get('clustering_sample_size', 50000))\n",
    "\n",
    "# Run kprototypes with multiple cluster counts\n",
    "\n",
    "#run_kprototypes_with_marketing_report(cluster_counts=[4, 8, 10], sample_size = GLOBAL_CONFIG.get('clustering_sample_size', 50000))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "d6fffb8a-6ca2-4bb1-9e44-4c0a452f4152",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cell 29: Invoke Kprototypes\n",
    "\n",
    "# Run with default settings (8, 10, 12 clusters)\n",
    "#run_kprototypes_with_marketing_report()\n",
    "\n",
    "# Run with specific cluster count\n",
    "run_kprototypes_with_marketing_report(cluster_counts=[8])\n",
    "\n",
    "# Run with multiple cluster counts and adjusted categorical weight\n",
    "#run_kprototypes_with_marketing_report(cluster_counts=[5, 8, 10], categorical_weight=0.7)\n",
    "\n",
    "# Run with global sample size\n",
    "#run_kprototypes_with_marketing_report(cluster_counts=[5, 8, 10], sample_size=GLOBAL_CONFIG.get('clustering_sample_size', 50000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e756ac0-0e00-4844-9898-f751ac3d03f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# CELL: Run MCA and K-means with Directory Creation\n",
    "\n",
    "def run_mca_kmeans_with_directory_creation():\n",
    "    \"\"\"\n",
    "    Run MCA analysis followed by k-means clustering with guaranteed directory creation\n",
    "    \"\"\"\n",
    "    log_status(\"===== STARTING MCA AND K-MEANS PIPELINE WITH DIRECTORY CREATION =====\")\n",
    "    \n",
    "    # Step 1: Extract data\n",
    "    log_status(\"Step 1: Extracting data...\")\n",
    "    extracted_data = direct_extract_acxiom_data(sample_size=GLOBAL_CONFIG['mca_sample_size'])\n",
    "    \n",
    "    if not extracted_data:\n",
    "        log_status(\"ERROR: Data extraction failed\")\n",
    "        return False\n",
    "    \n",
    "    # Step 2: Manually prepare data for MCA with fixed NaN handling\n",
    "    log_status(\"Step 2: Preparing data for MCA with correct NaN handling...\")\n",
    "    \n",
    "    # Get Spark DataFrame\n",
    "    spark_df = extracted_data['spark_df']\n",
    "    # Convert to pandas\n",
    "    df = spark_df.toPandas()\n",
    "    \n",
    "    # Get column definitions\n",
    "    id_column, vehicle_columns, propensity_columns, demographic_columns, lifestyle_columns, financial_columns = define_columns()\n",
    "    \n",
    "    # Create available columns lists\n",
    "    available_vehicle_cols = [col for col in vehicle_columns if col in df.columns]\n",
    "    available_propensity_cols = [col for col in propensity_columns if col in df.columns]\n",
    "    available_demographic_cols = [col for col in demographic_columns if col in df.columns]\n",
    "    available_lifestyle_cols = [col for col in lifestyle_columns if col in df.columns]\n",
    "    available_financial_cols = [col for col in financial_columns if col in df.columns]\n",
    "    \n",
    "    # Combine all available feature columns\n",
    "    available_feature_cols = (available_vehicle_cols + available_propensity_cols + \n",
    "                            available_demographic_cols + available_lifestyle_cols + \n",
    "                            available_financial_cols)\n",
    "    \n",
    "    # Create category mappings\n",
    "    column_categories = {}\n",
    "    for col in available_vehicle_cols:\n",
    "        column_categories[col] = \"Vehicle\"\n",
    "    for col in available_propensity_cols:\n",
    "        column_categories[col] = \"Propensity\"\n",
    "    for col in available_demographic_cols:\n",
    "        column_categories[col] = \"Demographic\"\n",
    "    for col in available_lifestyle_cols:\n",
    "        column_categories[col] = \"Lifestyle\"\n",
    "    for col in available_financial_cols:\n",
    "        column_categories[col] = \"Financial\"\n",
    "    \n",
    "    # Extract features\n",
    "    features = df[available_feature_cols].copy()\n",
    "    id_values = df[id_column].copy() if id_column in df.columns else None\n",
    "    \n",
    "    # Identify categorical columns\n",
    "    categorical_cols = []\n",
    "    for col in available_feature_cols:\n",
    "        try:\n",
    "            # Skip columns that have all NaN values\n",
    "            if features[col].isna().all():  # Correct check for all NaN\n",
    "                log_status(f\"Skipping column {col} with all NaN values\")\n",
    "                continue\n",
    "                \n",
    "            # Check if categorical by type or value count\n",
    "            is_categorical = (pd.api.types.is_object_dtype(features[col]) or \n",
    "                             isinstance(features[col].dtype, pd.CategoricalDtype) or\n",
    "                            (pd.api.types.is_numeric_dtype(features[col]) and features[col].nunique() <= 15))\n",
    "            \n",
    "            if is_categorical:\n",
    "                categorical_cols.append(col)\n",
    "                # Convert to string and fill NaNs\n",
    "                features[col] = features[col].fillna(\"missing\").astype(str)\n",
    "        except Exception as e:\n",
    "            log_status(f\"Warning: Error processing column {col}: {str(e)}\")\n",
    "            # Skip problematic column\n",
    "            continue\n",
    "    \n",
    "    # Check if we have enough categorical columns\n",
    "    if len(categorical_cols) < 3:\n",
    "        log_status(f\"ERROR: Not enough categorical columns for MCA (only {len(categorical_cols)})\")\n",
    "        return False\n",
    "    \n",
    "    # Create prepared data dictionary\n",
    "    prepared_data = {\n",
    "        'features': features,\n",
    "        'feature_cols': available_feature_cols,\n",
    "        'id_column': id_column,\n",
    "        'id_values': id_values,\n",
    "        'categorical_cols': categorical_cols,\n",
    "        'column_categories': column_categories\n",
    "    }\n",
    "    \n",
    "    log_status(f\"Successfully prepared {len(categorical_cols)} categorical columns for MCA\")\n",
    "    \n",
    "    # Step 3: Run MCA analysis\n",
    "    log_status(\"Step 3: Running MCA analysis...\")\n",
    "    \n",
    "    mca_analysis = robust_run_mca_analysis(prepared_data)\n",
    "    \n",
    "    if not mca_analysis:\n",
    "        log_status(\"ERROR: MCA analysis failed\")\n",
    "        return False\n",
    "    \n",
    "    log_status(f\"Successfully completed MCA analysis with {mca_analysis['n_dims']} dimensions\")\n",
    "    \n",
    "    # Step 4: Run k-means clustering with specified cluster counts\n",
    "    log_status(\"Step 4: Running k-means clustering...\")\n",
    "    \n",
    "    cluster_counts = [4, 8, 12]\n",
    "    clustering_results = {}\n",
    "    \n",
    "    # Create base output directory if it doesn't exist\n",
    "    output_prefix = GLOBAL_CONFIG['clustering_output_prefix']\n",
    "    output_base_dir = os.path.dirname(output_prefix)\n",
    "    \n",
    "    # Ensure base directory exists\n",
    "    try:\n",
    "        if not dbutils.fs.ls(output_base_dir):\n",
    "            log_status(f\"Creating base output directory: {output_base_dir}\")\n",
    "            dbutils.fs.mkdirs(output_base_dir)\n",
    "        else:\n",
    "            log_status(f\"Output directory already exists: {output_base_dir}\")\n",
    "    except Exception as e:\n",
    "        log_status(f\"WARNING: Could not create output directory {output_base_dir}: {str(e)}\")\n",
    "        log_status(\"Will attempt to continue anyway\")\n",
    "    \n",
    "    for k in cluster_counts:\n",
    "        log_status(f\"Running k-means with {k} clusters...\")\n",
    "        \n",
    "        # Select dimensions for clustering\n",
    "        n_dims = mca_analysis['n_dims']\n",
    "        dim_cols = [f'MCA_dim{i+1}' for i in range(n_dims)]\n",
    "        \n",
    "        # Get data for clustering\n",
    "        cluster_data = mca_analysis['mca_coords'][dim_cols].fillna(0)\n",
    "        \n",
    "        # Run k-means\n",
    "        from sklearn.cluster import KMeans\n",
    "        from sklearn.metrics import silhouette_score\n",
    "        \n",
    "        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "        cluster_labels = kmeans.fit_predict(cluster_data)\n",
    "        \n",
    "        # Calculate silhouette score if possible\n",
    "        try:\n",
    "            sil_score = silhouette_score(cluster_data, cluster_labels)\n",
    "            log_status(f\"K-means with k={k}: silhouette score = {sil_score:.4f}\")\n",
    "        except Exception as e:\n",
    "            log_status(f\"Warning: Unable to calculate silhouette score: {str(e)}\")\n",
    "            sil_score = None\n",
    "        \n",
    "        # Add cluster labels to MCA coordinates\n",
    "        result_df = mca_analysis['mca_coords'].copy()\n",
    "        result_df['kmeans_cluster'] = cluster_labels\n",
    "        \n",
    "        # Create centroids DataFrame\n",
    "        centers = pd.DataFrame(kmeans.cluster_centers_, columns=dim_cols)\n",
    "        centers['cluster_id'] = range(k)\n",
    "        \n",
    "        # Save results\n",
    "        result_path = f\"{output_prefix}_kmeans_{k}_clusters.parquet\"\n",
    "        centers_path = f\"{output_prefix}_kmeans_{k}_centers.csv\"\n",
    "        \n",
    "        # Create specific directories for this cluster count\n",
    "        kmeans_dir = os.path.dirname(result_path)\n",
    "        try:\n",
    "            if not dbutils.fs.ls(kmeans_dir):\n",
    "                log_status(f\"Creating k-means output directory: {kmeans_dir}\")\n",
    "                dbutils.fs.mkdirs(kmeans_dir)\n",
    "        except Exception as e:\n",
    "            log_status(f\"WARNING: Could not create k-means directory {kmeans_dir}: {str(e)}\")\n",
    "        \n",
    "        # Save files with explicit error handling\n",
    "        try:\n",
    "            result_df.to_parquet(f\"/dbfs{result_path}\")\n",
    "            log_status(f\"Saved k-means clustering results to {result_path}\")\n",
    "            \n",
    "            # Verify file exists after saving\n",
    "            if dbutils.fs.ls(result_path):\n",
    "                log_status(f\"Verified file exists: {result_path}\")\n",
    "            else:\n",
    "                log_status(f\"WARNING: File does not exist after saving: {result_path}\")\n",
    "        except Exception as e:\n",
    "            log_status(f\"ERROR saving clustering results: {str(e)}\")\n",
    "        \n",
    "        try:\n",
    "            centers.to_csv(f\"/dbfs{centers_path}\", index=False)\n",
    "            log_status(f\"Saved k-means centers to {centers_path}\")\n",
    "            \n",
    "            # Verify file exists after saving\n",
    "            if dbutils.fs.ls(centers_path):\n",
    "                log_status(f\"Verified file exists: {centers_path}\")\n",
    "            else:\n",
    "                log_status(f\"WARNING: File does not exist after saving: {centers_path}\")\n",
    "        except Exception as e:\n",
    "            log_status(f\"ERROR saving centroids: {str(e)}\")\n",
    "        \n",
    "        # Store results for marketing report\n",
    "        clustering_results[k] = {\n",
    "            'cluster_data': result_df,\n",
    "            'centroids_df': centers,\n",
    "            'num_clusters': k,\n",
    "            'silhouette_score': sil_score\n",
    "        }\n",
    "        \n",
    "        log_status(f\"Successfully completed k-means clustering with {k} clusters\")\n",
    "    \n",
    "    # Step 5: Generate marketing reports\n",
    "    log_status(\"Step 5: Generating marketing reports...\")\n",
    "    \n",
    "    for k, result in clustering_results.items():\n",
    "        log_status(f\"Generating marketing report for {k} clusters...\")\n",
    "        \n",
    "        try:\n",
    "            # Create report directory\n",
    "            report_dir = f\"{output_prefix}_kmeans_{k}_marketing_report\"\n",
    "            try:\n",
    "                if not dbutils.fs.ls(report_dir):\n",
    "                    log_status(f\"Creating marketing report directory: {report_dir}\")\n",
    "                    dbutils.fs.mkdirs(report_dir)\n",
    "            except Exception as dir_error:\n",
    "                log_status(f\"WARNING: Could not create report directory {report_dir}: {str(dir_error)}\")\n",
    "            \n",
    "            # Generate report\n",
    "            report_path = generate_kmodes_marketing_report(\n",
    "                result,\n",
    "                output_prefix=f\"{output_prefix}_kmeans_{k}\"\n",
    "            )\n",
    "            \n",
    "            if report_path:\n",
    "                log_status(f\"✅ Successfully generated marketing report at {report_path}\")\n",
    "                \n",
    "                # Verify report exists\n",
    "                if dbutils.fs.ls(report_path):\n",
    "                    log_status(f\"Verified report file exists: {report_path}\")\n",
    "                else:\n",
    "                    log_status(f\"WARNING: Report file does not exist after generation: {report_path}\")\n",
    "            else:\n",
    "                log_status(f\"❌ Failed to generate marketing report for {k} clusters\")\n",
    "        except Exception as e:\n",
    "            log_status(f\"ERROR generating marketing report for {k} clusters: {str(e)}\")\n",
    "            log_status(traceback.format_exc())\n",
    "    \n",
    "    log_status(\"===== MCA AND K-MEANS PIPELINE COMPLETED =====\")\n",
    "    return True\n",
    "\n",
    "# Execute the pipeline with directory creation\n",
    "run_mca_kmeans_with_directory_creation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "f7bc7446-e8c8-40d5-b211-e263dc35dd19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "################## Marketing Persona Reports ###################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d04bb218-eda3-4e6b-a03c-b5d97fa6436b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##### Viz Cell 1: Vehicle Customer Segment Distribution Report #####\n",
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Set the style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"colorblind\")\n",
    "\n",
    "# Segment data\n",
    "segments = {\n",
    "    'Premium Luxury Enthusiasts': 2.5,\n",
    "    'Ultra-Premium Specialty Buyers': 0.7,\n",
    "    'Technology-Forward Early Adopters': 3.0,\n",
    "    'Upscale Family Vehicle Buyers': 6.4,\n",
    "    'Mainstream Family Vehicle Owners': 25.3,\n",
    "    'Value-Oriented Vehicle Buyers': 24.1,\n",
    "    'Practical Utility Vehicle Owners': 12.3,\n",
    "    'Economy-Minded Traditional Buyers': 25.7\n",
    "}\n",
    "\n",
    "# Create a DataFrame for the segments\n",
    "df = pd.DataFrame({\n",
    "    'Segment': list(segments.keys()),\n",
    "    'Percentage': list(segments.values())\n",
    "})\n",
    "\n",
    "# Calculate the counts (assuming 10,000 total customers based on the clustering results)\n",
    "total_customers = 10000\n",
    "df['Count'] = (df['Percentage'] * total_customers / 100).astype(int)\n",
    "\n",
    "# Sort by percentage (optional)\n",
    "# df = df.sort_values('Percentage', ascending=False)\n",
    "\n",
    "# Create a color map for the segments\n",
    "colors = ['#3366cc', '#dc3912', '#ff9900', '#109618', \n",
    "          '#990099', '#0099c6', '#dd4477', '#66aa00']\n",
    "\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Create the bar chart\n",
    "bars = ax.bar(df['Segment'], df['Percentage'], color=colors)\n",
    "\n",
    "# Add percentage labels above each bar\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height + 0.3,\n",
    "            f'{height}%', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Add count labels inside each bar\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    count = int(height * total_customers / 100)\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height/2,\n",
    "            f'{count:,}', ha='center', va='center', \n",
    "            color='white', fontweight='bold')\n",
    "\n",
    "# Customize the plot\n",
    "ax.set_title('Vehicle Customer Segment Distribution', fontsize=16, pad=20)\n",
    "ax.set_ylabel('Percentage of Customers', fontsize=12)\n",
    "ax.set_ylim(0, max(df['Percentage']) * 1.1)  # Add some space for the labels\n",
    "\n",
    "# Rotate x-axis labels for better readability\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig('vehicle_segment_distribution.png', dpi=300, bbox_inches='tight')\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31c36107-650d-4de1-a860-af7524719175",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##### Viz Cell 2: Veicle Customer Segment Quadrant Map #####\n",
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Set the style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "# Define segment data\n",
    "# x: Luxury Orientation, y: Innovation Openness, size: percentage\n",
    "segments = [\n",
    "    {'name': 'Premium Luxury Enthusiasts', 'x': 5.5, 'y': 0, 'size': 2.5, 'cluster': 0, 'color': '#3366cc'},\n",
    "    {'name': 'Ultra-Premium Specialty Buyers', 'x': 4.5, 'y': 1.5, 'size': 0.7, 'cluster': 1, 'color': '#dc3912'},\n",
    "    {'name': 'Technology-Forward Early Adopters', 'x': 2.0, 'y': 5.5, 'size': 3.0, 'cluster': 2, 'color': '#ff9900'},\n",
    "    {'name': 'Upscale Family Vehicle Buyers', 'x': 3.0, 'y': 4.0, 'size': 6.4, 'cluster': 3, 'color': '#109618'},\n",
    "    {'name': 'Mainstream Family Vehicle Owners', 'x': -1.0, 'y': 1.5, 'size': 25.3, 'cluster': 4, 'color': '#990099'},\n",
    "    {'name': 'Value-Oriented Vehicle Buyers', 'x': -2.5, 'y': 0, 'size': 24.1, 'cluster': 5, 'color': '#0099c6'},\n",
    "    {'name': 'Practical Utility Vehicle Owners', 'x': -1.5, 'y': -2.5, 'size': 12.3, 'cluster': 6, 'color': '#dd4477'},\n",
    "    {'name': 'Economy-Minded Traditional Buyers', 'x': -3.0, 'y': -3.5, 'size': 25.7, 'cluster': 7, 'color': '#66aa00'}\n",
    "]\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(segments)\n",
    "\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "# Plot the segments as bubbles\n",
    "for _, segment in df.iterrows():\n",
    "    ax.scatter(\n",
    "        segment['x'], \n",
    "        segment['y'], \n",
    "        s=segment['size'] * 50,  # Adjust size multiplier as needed\n",
    "        color=segment['color'], \n",
    "        alpha=0.7, \n",
    "        edgecolor='white',\n",
    "        linewidth=1\n",
    "    )\n",
    "    # Add cluster number labels\n",
    "    ax.text(\n",
    "        segment['x'], \n",
    "        segment['y'], \n",
    "        str(segment['cluster']),\n",
    "        ha='center', \n",
    "        va='center', \n",
    "        fontweight='bold',\n",
    "        color='white'\n",
    "    )\n",
    "    # Add segment name and size labels\n",
    "    ax.text(\n",
    "        segment['x'], \n",
    "        segment['y'] - 0.6, \n",
    "        f\"{segment['name']}\\n({segment['size']}%)\",\n",
    "        ha='center', \n",
    "        va='top', \n",
    "        fontsize=8,\n",
    "        color='black'\n",
    "    )\n",
    "\n",
    "# Add quadrant labels\n",
    "ax.text(3, 3, 'Tech-Forward Premium', ha='center', va='center', fontsize=12, fontweight='bold', color='#555555')\n",
    "ax.text(-3, 3, 'Tech-Forward Value', ha='center', va='center', fontsize=12, fontweight='bold', color='#555555')\n",
    "ax.text(3, -3, 'Traditional Premium', ha='center', va='center', fontsize=12, fontweight='bold', color='#555555')\n",
    "ax.text(-3, -3, 'Traditional Value', ha='center', va='center', fontsize=12, fontweight='bold', color='#555555')\n",
    "\n",
    "# Add axes labels\n",
    "ax.set_xlabel('Luxury Orientation', fontsize=14)\n",
    "ax.set_ylabel('Innovation Openness', fontsize=14)\n",
    "\n",
    "# Add title\n",
    "ax.set_title('Vehicle Customer Segment Map', fontsize=16, pad=20)\n",
    "\n",
    "# Set axis limits\n",
    "ax.set_xlim(-6, 6)\n",
    "ax.set_ylim(-6, 6)\n",
    "\n",
    "# Add gridlines\n",
    "ax.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Add origin lines\n",
    "ax.axhline(y=0, color='gray', linestyle='-', alpha=0.5)\n",
    "ax.axvline(x=0, color='gray', linestyle='-', alpha=0.5)\n",
    "\n",
    "# Add axis descriptions\n",
    "ax.text(6, 0, 'Premium/Luxury', ha='right', va='center', fontsize=10)\n",
    "ax.text(-6, 0, 'Economy/Value', ha='left', va='center', fontsize=10)\n",
    "ax.text(0, 6, 'Innovative/Tech-Forward', ha='center', va='top', fontsize=10)\n",
    "ax.text(0, -6, 'Traditional', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# Add a legend for market segments\n",
    "legend_elements = [\n",
    "    plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='#3366cc', markersize=10, label='Premium Market (3.2%)'),\n",
    "    plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='#109618', markersize=10, label='Innovation-Focused (9.4%)'),\n",
    "    plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='#990099', markersize=10, label='Mainstream Market (49.4%)'),\n",
    "    plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='#66aa00', markersize=10, label='Practical/Economy (38.0%)')\n",
    "]\n",
    "ax.legend(handles=legend_elements, loc='lower right')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig('vehicle_segment_quadrant_map.png', dpi=300, bbox_inches='tight')\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51c76845-bc22-4dc2-a158-0cc0461e2490",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##### Viz Cell 3: Vehicle Customer Segment Radar Profiles #####\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from matplotlib.patches import Circle, RegularPolygon\n",
    "from matplotlib.path import Path\n",
    "from matplotlib.projections.polar import PolarAxes\n",
    "from matplotlib.projections import register_projection\n",
    "from matplotlib.spines import Spine\n",
    "from matplotlib.transforms import Affine2D\n",
    "\n",
    "# Define a function for creating a radar chart\n",
    "def radar_factory(num_vars, frame='circle'):\n",
    "    # Create a radar chart with `num_vars` axes\n",
    "    # Calculate evenly-spaced axis angles\n",
    "    theta = np.linspace(0, 2*np.pi, num_vars, endpoint=False)\n",
    "    \n",
    "    # Class for creating a radar chart\n",
    "    class RadarAxes(PolarAxes):\n",
    "        name = 'radar'\n",
    "        \n",
    "        def __init__(self, *args, **kwargs):\n",
    "            self.theta = theta\n",
    "            super().__init__(*args, **kwargs)\n",
    "            self.set_theta_zero_location('N')\n",
    "            \n",
    "        def fill(self, *args, closed=True, **kwargs):\n",
    "            return super().fill(self.theta, args[0], closed=closed, **kwargs)\n",
    "            \n",
    "        def plot(self, *args, **kwargs):\n",
    "            return super().plot(self.theta, args[0], **kwargs)\n",
    "            \n",
    "        def set_varlabels(self, labels):\n",
    "            self.set_thetagrids(np.degrees(self.theta), labels)\n",
    "            \n",
    "        def _gen_axes_patch(self):\n",
    "            if frame == 'circle':\n",
    "                return Circle((0.5, 0.5), 0.5)\n",
    "            elif frame == 'polygon':\n",
    "                return RegularPolygon((0.5, 0.5), num_vars, radius=0.5, edgecolor=\"k\")\n",
    "            else:\n",
    "                raise ValueError(\"Unknown value for 'frame': %s\" % frame)\n",
    "                \n",
    "        def _gen_axes_spines(self):\n",
    "            if frame == 'circle':\n",
    "                return super()._gen_axes_spines()\n",
    "            elif frame == 'polygon':\n",
    "                verts = unit_poly_verts(self.theta)\n",
    "                verts.append(verts[0])\n",
    "                path = Path(verts)\n",
    "                spine = Spine(self, 'circle', path)\n",
    "                spine.set_transform(Affine2D().scale(.5).translate(.5, .5) + self.transAxes)\n",
    "                return {'polar': spine}\n",
    "            else:\n",
    "                raise ValueError(\"Unknown value for 'frame': %s\" % frame)\n",
    "    \n",
    "    register_projection(RadarAxes)\n",
    "    return theta\n",
    "\n",
    "def unit_poly_verts(theta):\n",
    "    # Return vertices of polygon for subplot axes.\n",
    "    x0, y0, r = [0.5] * 3\n",
    "    verts = [(r*np.cos(t) + x0, r*np.sin(t) + y0) for t in theta]\n",
    "    return verts\n",
    "\n",
    "# Segment profiles based on key attributes\n",
    "segment_profiles = {\n",
    "    'Premium Luxury Enthusiasts': {\n",
    "        'color': '#3366cc',\n",
    "        'data': [\n",
    "            90,  # Luxury Orientation\n",
    "            20,  # Price Sensitivity\n",
    "            70,  # Tech Adoption\n",
    "            85,  # Brand Loyalty\n",
    "            40,  # Family Focus\n",
    "            30,  # Utility Priority\n",
    "        ]\n",
    "    },\n",
    "    'Ultra-Premium Specialty Buyers': {\n",
    "        'color': '#dc3912',\n",
    "        'data': [\n",
    "            95,  # Luxury Orientation\n",
    "            10,  # Price Sensitivity\n",
    "            80,  # Tech Adoption\n",
    "            90,  # Brand Loyalty\n",
    "            20,  # Family Focus\n",
    "            25,  # Utility Priority\n",
    "        ]\n",
    "    },\n",
    "    'Technology-Forward Early Adopters': {\n",
    "        'color': '#ff9900',\n",
    "        'data': [\n",
    "            65,  # Luxury Orientation\n",
    "            40,  # Price Sensitivity\n",
    "            95,  # Tech Adoption\n",
    "            40,  # Brand Loyalty\n",
    "            50,  # Family Focus\n",
    "            35,  # Utility Priority\n",
    "        ]\n",
    "    },\n",
    "    'Upscale Family Vehicle Buyers': {\n",
    "        'color': '#109618',\n",
    "        'data': [\n",
    "            75,  # Luxury Orientation\n",
    "            45,  # Price Sensitivity\n",
    "            70,  # Tech Adoption\n",
    "            65,  # Brand Loyalty\n",
    "            90,  # Family Focus\n",
    "            60,  # Utility Priority\n",
    "        ]\n",
    "    },\n",
    "    'Mainstream Family Vehicle Owners': {\n",
    "        'color': '#990099',\n",
    "        'data': [\n",
    "            45,  # Luxury Orientation\n",
    "            70,  # Price Sensitivity\n",
    "            55,  # Tech Adoption\n",
    "            60,  # Brand Loyalty\n",
    "            85,  # Family Focus\n",
    "            65,  # Utility Priority\n",
    "        ]\n",
    "    },\n",
    "    'Value-Oriented Vehicle Buyers': {\n",
    "        'color': '#0099c6',\n",
    "        'data': [\n",
    "            25,  # Luxury Orientation\n",
    "            85,  # Price Sensitivity\n",
    "            40,  # Tech Adoption\n",
    "            50,  # Brand Loyalty\n",
    "            60,  # Family Focus\n",
    "            55,  # Utility Priority\n",
    "        ]\n",
    "    },\n",
    "    'Practical Utility Vehicle Owners': {\n",
    "        'color': '#dd4477',\n",
    "        'data': [\n",
    "            35,  # Luxury Orientation\n",
    "            75,  # Price Sensitivity\n",
    "            30,  # Tech Adoption\n",
    "            65,  # Brand Loyalty\n",
    "            50,  # Family Focus\n",
    "            90,  # Utility Priority\n",
    "        ]\n",
    "    },\n",
    "    'Economy-Minded Traditional Buyers': {\n",
    "        'color': '#66aa00',\n",
    "        'data': [\n",
    "            15,  # Luxury Orientation\n",
    "            95,  # Price Sensitivity\n",
    "            20,  # Tech Adoption\n",
    "            70,  # Brand Loyalty\n",
    "            45,  # Family Focus\n",
    "            60,  # Utility Priority\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Define attribute labels\n",
    "attributes = ['Luxury Orientation', 'Price Sensitivity', 'Tech Adoption', \n",
    "              'Brand Loyalty', 'Family Focus', 'Utility Priority']\n",
    "\n",
    "# Create figure and radar chart factory\n",
    "n_attributes = len(attributes)\n",
    "theta = radar_factory(n_attributes, frame='polygon')\n",
    "\n",
    "# Create subplots for each segment (in a 3x3 grid)\n",
    "fig, axes = plt.subplots(figsize=(16, 14), nrows=3, ncols=3, \n",
    "                         subplot_kw=dict(projection='radar'))\n",
    "fig.subplots_adjust(wspace=0.4, hspace=0.4, top=0.85, bottom=0.05)\n",
    "\n",
    "# Flatten the axes array for easier iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Set plot limits for all axes\n",
    "for ax in axes:\n",
    "    ax.set_ylim(0, 100)\n",
    "\n",
    "# Plot each segment on its own subplot\n",
    "for i, (segment_name, profile) in enumerate(segment_profiles.items()):\n",
    "    # Skip if we have more segments than subplots\n",
    "    if i >= len(axes) - 1:  # Reserve the last subplot for the combined view\n",
    "        break\n",
    "        \n",
    "    # Plot the radar chart\n",
    "    ax = axes[i]\n",
    "    ax.plot(profile['data'], color=profile['color'], linewidth=2.5)\n",
    "    ax.fill(profile['data'], alpha=0.25, color=profile['color'])\n",
    "    ax.set_title(segment_name, size=11, y=1.1, color=profile['color'], fontweight='bold')\n",
    "    ax.set_varlabels(attributes)\n",
    "    \n",
    "    # Customize grid lines\n",
    "    ax.set_rgrids([20, 40, 60, 80], labels=['20', '40', '60', '80'], angle=0, fontsize=7)\n",
    "    \n",
    "    # Rotate attribute labels for better readability\n",
    "    for label, angle in zip(ax.get_xticklabels(), theta):\n",
    "        if angle in (0, np.pi):\n",
    "            label.set_horizontalalignment('center')\n",
    "        elif 0 < angle < np.pi:\n",
    "            label.set_horizontalalignment('left')\n",
    "        else:\n",
    "            label.set_horizontalalignment('right')\n",
    "        label.set_fontsize(8)\n",
    "\n",
    "# Create a combined view on the last subplot\n",
    "combined_ax = axes[-1]\n",
    "for segment_name, profile in segment_profiles.items():\n",
    "    combined_ax.plot(profile['data'], color=profile['color'], linewidth=1.5, \n",
    "                     label=segment_name, alpha=0.8)\n",
    "\n",
    "combined_ax.set_title('All Segments Comparison', size=12, y=1.1, fontweight='bold')\n",
    "combined_ax.set_varlabels(attributes)\n",
    "combined_ax.set_rgrids([20, 40, 60, 80], labels=['20', '40', '60', '80'], angle=0, fontsize=7)\n",
    "combined_ax.legend(loc='upper right', bbox_to_anchor=(1.8, 1.0), fontsize=8)\n",
    "\n",
    "# Add main title\n",
    "fig.suptitle('Vehicle Customer Segment Profiles', fontsize=16, fontweight='bold', y=0.98)\n",
    "fig.text(0.5, 0.93, 'Based on hierarchical clustering analysis', \n",
    "         horizontalalignment='center', fontsize=10)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.9])\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig('vehicle_segment_radar_profiles.png', dpi=300, bbox_inches='tight')\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3557ad76-25a1-44ad-9a97-c7182d9992f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##### Viz Cell 4: #####\n",
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "# Set the style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "# Define segment data with marketing strategies\n",
    "segments = {\n",
    "    'Premium Luxury Enthusiasts': {\n",
    "        'size': 2.5,\n",
    "        'color': '#3366cc',\n",
    "        'messaging': [\n",
    "            'Emphasize exclusivity and premium craftsmanship',\n",
    "            'Highlight prestigious brand heritage and status',\n",
    "            'Focus on cutting-edge technology and innovation',\n",
    "            'Showcase luxury materials and attention to detail'\n",
    "        ],\n",
    "        'channels': [\n",
    "            'Luxury lifestyle publications',\n",
    "            'Exclusive events and experiences',\n",
    "            'Personalized direct marketing',\n",
    "            'High-end digital platforms',\n",
    "            'Strategic partnerships with luxury brands'\n",
    "        ],\n",
    "        'product_focus': [\n",
    "            'Flagship luxury models',\n",
    "            'Premium features and options',\n",
    "            'Exclusive limited editions',\n",
    "            'Personalization programs'\n",
    "        ],\n",
    "        'customer_experience': [\n",
    "            'White-glove concierge service',\n",
    "            'Personalized shopping experience',\n",
    "            'VIP ownership benefits',\n",
    "            'Exclusive access to brand events'\n",
    "        ]\n",
    "    },\n",
    "    'Ultra-Premium Specialty Buyers': {\n",
    "        'size': 0.7,\n",
    "        'color': '#dc3912',\n",
    "        'messaging': [\n",
    "            'Focus on exclusivity and rarity',\n",
    "            'Emphasize bespoke customization options',\n",
    "            'Highlight engineering excellence and craftsmanship',\n",
    "            'Position as the pinnacle of automotive achievement'\n",
    "        ],\n",
    "        'channels': [\n",
    "            'Direct one-to-one outreach',\n",
    "            'Invitation-only events',\n",
    "            'Ultra-high-net-worth networks',\n",
    "            'Specialized luxury publications',\n",
    "            'Private showings and experiences'\n",
    "        ],\n",
    "        'product_focus': [\n",
    "            'Limited production models',\n",
    "            'Bespoke customization programs',\n",
    "            'Signature editions',\n",
    "            'Collector series vehicles'\n",
    "        ],\n",
    "        'customer_experience': [\n",
    "            'Completely personalized purchase journey',\n",
    "            'Factory visits and behind-the-scenes access',\n",
    "            'Dedicated relationship manager',\n",
    "            'Exclusive owner community events'\n",
    "        ]\n",
    "    },\n",
    "    'Technology-Forward Early Adopters': {\n",
    "        'size': 3.0,\n",
    "        'color': '#ff9900',\n",
    "        'messaging': [\n",
    "            'Focus on innovation and cutting-edge technology',\n",
    "            'Emphasize environmental benefits and sustainability',\n",
    "            'Highlight connectivity and integration features',\n",
    "            'Position as forward-thinking and progressive'\n",
    "        ],\n",
    "        'channels': [\n",
    "            'Technology publications and platforms',\n",
    "            'Digital and social media',\n",
    "            'Innovation conferences and events',\n",
    "            'Tech-focused partnerships and integrations',\n",
    "            'Online communities and forums'\n",
    "        ],\n",
    "        'product_focus': [\n",
    "            'Electric and alternative fuel vehicles',\n",
    "            'Models with advanced technology features',\n",
    "            'Connectivity and smart integration',\n",
    "            'Innovative design and materials'\n",
    "        ],\n",
    "        'customer_experience': [\n",
    "            'Digital-first shopping experience',\n",
    "            'Virtual reality product demonstrations',\n",
    "            'Tech-focused showroom experiences',\n",
    "            'Community of like-minded early adopters'\n",
    "        ]\n",
    "    },\n",
    "    'Upscale Family Vehicle Buyers': {\n",
    "        'size': 6.4,\n",
    "        'color': '#109618',\n",
    "        'messaging': [\n",
    "            'Balance luxury with practical family functionality',\n",
    "            'Emphasize safety features and technology',\n",
    "            'Highlight spacious comfort and premium materials',\n",
    "            'Focus on quality of experience for the whole family'\n",
    "        ],\n",
    "        'channels': [\n",
    "            'Upscale family lifestyle publications',\n",
    "            'Premium digital and social channels',\n",
    "            'Family-oriented events and sponsorships',\n",
    "            'Parenting communities and networks'\n",
    "        ],\n",
    "        'product_focus': [\n",
    "            'Premium SUVs and crossovers',\n",
    "            'Family-oriented luxury vehicles',\n",
    "            'Safety-focused premium models',\n",
    "            'Versatile luxury vehicles with ample space'\n",
    "        ],\n",
    "        'customer_experience': [\n",
    "            'Family-friendly premium showrooms',\n",
    "            'Child-friendly facilities during purchase and service',\n",
    "            'Family test drive experiences',\n",
    "            'Premium ownership benefits for the whole family'\n",
    "        ]\n",
    "    },\n",
    "    'Mainstream Family Vehicle Owners': {\n",
    "        'size': 25.3,\n",
    "        'color': '#990099',\n",
    "        'messaging': [\n",
    "            'Focus on value, reliability, and practicality',\n",
    "            'Emphasize family-friendly features and versatility',\n",
    "            'Highlight safety ratings and features',\n",
    "            'Show how vehicles fit into family lifestyle'\n",
    "        ],\n",
    "        'channels': [\n",
    "            'Mass market advertising',\n",
    "            'Family-focused digital platforms',\n",
    "            'Social media and content marketing',\n",
    "            'Partnerships with family brands',\n",
    "            'Community events and sponsorships'\n",
    "        ],\n",
    "        'product_focus': [\n",
    "            'Mid-size SUVs and crossovers',\n",
    "            'Family sedans and minivans',\n",
    "            'Models with strong safety ratings',\n",
    "            'Versatile and practical vehicles'\n",
    "        ],\n",
    "        'customer_experience': [\n",
    "            'Family-friendly showrooms',\n",
    "            'Straightforward purchase process',\n",
    "            'Transparent pricing and financing',\n",
    "            'Reliable service and support'\n",
    "        ]\n",
    "    },\n",
    "    'Value-Oriented Vehicle Buyers': {\n",
    "        'size': 24.1,\n",
    "        'color': '#0099c6',\n",
    "        'messaging': [\n",
    "            'Emphasize affordability and value proposition',\n",
    "            'Focus on fuel efficiency and low operating costs',\n",
    "            'Highlight reliability and practical features',\n",
    "            'Demonstrate strong return on investment'\n",
    "        ],\n",
    "        'channels': [\n",
    "            'Mass market advertising with value messaging',\n",
    "            'Price comparison platforms',\n",
    "            'Deal-focused digital marketing',\n",
    "            'Email campaigns with special offers',\n",
    "            'Search engine marketing for price-focused queries'\n",
    "        ],\n",
    "        'product_focus': [\n",
    "            'Economy and compact vehicles',\n",
    "            'Fuel-efficient models',\n",
    "            'Entry-level variants with good feature set',\n",
    "            'Models with strong warranty coverage'\n",
    "        ],\n",
    "        'customer_experience': [\n",
    "            'No-pressure sales environment',\n",
    "            'Transparent pricing',\n",
    "            'Simple buying process',\n",
    "            'Value-oriented service packages'\n",
    "        ]\n",
    "    },\n",
    "    'Practical Utility Vehicle Owners': {\n",
    "        'size': 12.3,\n",
    "        'color': '#dd4477',\n",
    "        'messaging': [\n",
    "            'Focus on capability, durability, and functionality',\n",
    "            'Highlight utility features and versatility',\n",
    "            'Emphasize reliability and toughness',\n",
    "            'Show practical applications and use cases'\n",
    "        ],\n",
    "        'channels': [\n",
    "            'Industry and trade publications',\n",
    "            'Specialized interest groups',\n",
    "            'Work and utility-focused events',\n",
    "            'Practical demonstration videos',\n",
    "            'Partnerships with related industries'\n",
    "        ],\n",
    "        'product_focus': [\n",
    "            'Pickup trucks and utility vans',\n",
    "            'Work-oriented SUVs',\n",
    "            'Models with towing and cargo capabilities',\n",
    "            'Vehicles with practical customization options'\n",
    "        ],\n",
    "        'customer_experience': [\n",
    "            'Practical demonstrations of capabilities',\n",
    "            'Straightforward purchase experience',\n",
    "            'Focus on specs and performance metrics',\n",
    "            'Service programs that minimize downtime'\n",
    "        ]\n",
    "    },\n",
    "    'Economy-Minded Traditional Buyers': {\n",
    "        'size': 25.7,\n",
    "        'color': '#66aa00',\n",
    "        'messaging': [\n",
    "            'Emphasize lowest total cost of ownership',\n",
    "            'Focus on reliability and longevity',\n",
    "            'Highlight fuel economy and efficiency',\n",
    "            'Show value of basic, no-frills transportation'\n",
    "        ],\n",
    "        'channels': [\n",
    "            'Budget-focused advertising',\n",
    "            'Local marketing and promotions',\n",
    "            'Dealership-level campaigns',\n",
    "            'Value-oriented digital platforms',\n",
    "            'Targeted offers and incentives'\n",
    "        ],\n",
    "        'product_focus': [\n",
    "            'Economy cars and compact models',\n",
    "            'Base trim levels with essential features',\n",
    "            'Most fuel-efficient options',\n",
    "            'Used vehicle programs with warranty'\n",
    "        ],\n",
    "        'customer_experience': [\n",
    "            'Quick, efficient buying process',\n",
    "            'Focus on affordability and payment options',\n",
    "            'Straightforward, no-frills approach',\n",
    "            'Economy service packages and maintenance'\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Function to create a strategy card for a segment\n",
    "def create_strategy_card(segment_name, data, ax):\n",
    "    # Set title\n",
    "    ax.set_title(segment_name, fontsize=12, fontweight='bold', color=data['color'], pad=10)\n",
    "    \n",
    "    # Remove axes\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    \n",
    "    # Background color\n",
    "    ax.set_facecolor('#f8f9fa')\n",
    "    \n",
    "    # Segment size\n",
    "    ax.text(0.02, 0.98, f\"Segment size: {data['size']}%\", transform=ax.transAxes,\n",
    "            fontsize=10, verticalalignment='top', horizontalalignment='left',\n",
    "            bbox=dict(facecolor='white', alpha=0.7, boxstyle='round,pad=0.3'))\n",
    "    \n",
    "    # Create text blocks for each strategy section\n",
    "    ypositions = [0.85, 0.60, 0.35, 0.10]\n",
    "    heights = [0.15, 0.15, 0.15, 0.15]\n",
    "    titles = ['Key Messaging', 'Marketing Channels', 'Product Focus', 'Customer Experience']\n",
    "    content_lists = [data['messaging'], data['channels'], data['product_focus'], data['customer_experience']]\n",
    "    \n",
    "    for ypos, height, title, content in zip(ypositions, heights, titles, content_lists):\n",
    "        # Section background\n",
    "        rect = plt.Rectangle((0.05, ypos-height), 0.9, height, \n",
    "                           fill=True, color='white', alpha=0.8,\n",
    "                           transform=ax.transAxes, zorder=1,\n",
    "                           linewidth=1, edgecolor='#dddddd')\n",
    "        ax.add_patch(rect)\n",
    "        \n",
    "        # Section title\n",
    "        ax.text(0.07, ypos-0.03, title, transform=ax.transAxes,\n",
    "                fontsize=10, fontweight='bold', verticalalignment='top',\n",
    "                horizontalalignment='left', zorder=2)\n",
    "        \n",
    "        # Section content as bullet points\n",
    "        for i, item in enumerate(content):\n",
    "            ax.text(0.09, ypos-0.06-(i*0.025), f\"• {item}\", transform=ax.transAxes,\n",
    "                    fontsize=8, verticalalignment='top', horizontalalignment='left',\n",
    "                    zorder=2, wrap=True)\n",
    "\n",
    "# Create the marketing strategies visualization\n",
    "def create_marketing_strategies_visualization():\n",
    "    # Create figure\n",
    "    fig = plt.figure(figsize=(16, 20))\n",
    "    \n",
    "    # Create a GridSpec\n",
    "    gs = GridSpec(4, 2, figure=fig, hspace=0.4, wspace=0.3)\n",
    "    \n",
    "    # Title for the entire figure\n",
    "    fig.suptitle('Marketing Strategy Recommendations by Segment', fontsize=16, fontweight='bold', y=0.98)\n",
    "    fig.text(0.5, 0.955, 'Based on hierarchical clustering analysis of customer vehicle preferences',\n",
    "             horizontalalignment='center', fontsize=10)\n",
    "    \n",
    "    # Create a subplot for each segment\n",
    "    segment_names = list(segments.keys())\n",
    "    \n",
    "    for i, segment_name in enumerate(segment_names):\n",
    "        # Calculate grid position\n",
    "        row = i // 2\n",
    "        col = i % 2\n",
    "        \n",
    "        # Create subplot\n",
    "        ax = fig.add_subplot(gs[row, col])\n",
    "        \n",
    "        # Create strategy card\n",
    "        create_strategy_card(segment_name, segments[segment_name], ax)\n",
    "    \n",
    "    # Adjust layout\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.94])\n",
    "    \n",
    "    # Return the figure\n",
    "    return fig\n",
    "\n",
    "# Create the visualization\n",
    "fig = create_marketing_strategies_visualization()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig('vehicle_segment_marketing_strategies.png', dpi=300, bbox_inches='tight')\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9fdf29b-4db6-47c1-b887-0b16736de858",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks permissions diagnostic script\n",
    "\n",
    "# Check current user\n",
    "print(\"Current User:\")\n",
    "print(spark.sql(\"SELECT current_user()\").collect()[0][0])\n",
    "\n",
    "# List available writable locations\n",
    "print(\"\\nPossible writable locations:\")\n",
    "print(\"1. /tmp directory:\")\n",
    "display(dbutils.fs.ls(\"/tmp\"))\n",
    "\n",
    "# print(\"\\n2. Your home directory:\")\n",
    "home_dir = f\"/user/{spark.sql('SELECT current_user()').collect()[0][0]}\"\n",
    "display(dbutils.fs.ls(home_dir))\n",
    "\n",
    "# Attempt to write to a temporary location\n",
    "try:\n",
    "    test_path = \"/tmp/permissions_test.txt\"\n",
    "    dbutils.fs.put(test_path, \"Permissions test\", overwrite=True)\n",
    "    print(f\"\\nSuccessfully wrote to {test_path}\")\n",
    "    \n",
    "    # Read back the file\n",
    "    print(\"File contents:\")\n",
    "    print(dbutils.fs.head(test_path))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nFailed to write to temporary location: {e}\")\n",
    "\n",
    "# Additional diagnostic information\n",
    "print(\"\\nCurrent Spark Configuration:\")\n",
    "for key, value in spark.conf.getAll().items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92b175c0-70fe-4890-9241-12b70be3ccd4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Explicit write test to /tmp\n",
    "\n",
    "# Generate a unique filename\n",
    "from datetime import datetime\n",
    "import uuid\n",
    "\n",
    "# Create a unique filename\n",
    "unique_filename = f\"/tmp/write_test_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{uuid.uuid4().hex[:8]}.txt\"\n",
    "\n",
    "try:\n",
    "    # Attempt to write the file\n",
    "    dbutils.fs.put(unique_filename, \"Databricks file write test successful!\", overwrite=True)\n",
    "    \n",
    "    # Confirm the file was written\n",
    "    print(f\"Successfully wrote file: {unique_filename}\")\n",
    "    \n",
    "    # Read back the contents\n",
    "    print(\"File contents:\")\n",
    "    print(dbutils.fs.head(unique_filename))\n",
    "    \n",
    "    # List the contents of /tmp to verify\n",
    "    print(\"\\nContents of /tmp after write:\")\n",
    "    display(dbutils.fs.ls(\"/tmp\"))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "950fc638-a7ce-4bcb-9b25-65fa031c46c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "############ Updated Training data set ###############\n",
    "\n",
    "# Import required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, count, coalesce, lit\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Define source tables\n",
    "SOURCE_FEATURES_TABLE = \"work.marsci.sw_onstar_enhanced_conversion_propensity_v2\"  # Your original table\n",
    "CROSSWALK_TABLE = \"work.marsci.vw_id_crosswalk\"  # ID crosswalk table\n",
    "DEMOGRAPHICS_TABLE = \"work.marsci.acxiom_full_demographic_test\"  # Demographic data table\n",
    "TARGET_MODEL_TABLE = \"work.marsci.sw_onstar_conversion_basic_model_features\"  # New table for model training\n",
    "\n",
    "# Load the data\n",
    "print(\"Loading data from source tables...\")\n",
    "df_features = spark.table(SOURCE_FEATURES_TABLE)\n",
    "df_crosswalk = spark.table(CROSSWALK_TABLE)\n",
    "df_demographics = spark.table(DEMOGRAPHICS_TABLE)\n",
    "\n",
    "print(f\"Loaded {df_features.count()} rows from features table\")\n",
    "print(f\"Loaded {df_crosswalk.count()} rows from crosswalk table\")\n",
    "print(f\"Loaded {df_demographics.count()} rows from demographics table\")\n",
    "\n",
    "# Define key identifiers needed for any model\n",
    "id_cols = ['FEATURE_DATE', 'ACCOUNT_NUMBER', 'ACCOUNT_ID', 'VIN_ID']\n",
    "target_col = ['pay_flg']\n",
    "\n",
    "# Define features available to Basic plan customers\n",
    "# These are features that Basic customers could potentially use or access\n",
    "basic_features = [\n",
    "    # Vehicle information\n",
    "    'VEHICLE_STATUS_CD', 'VEH_MAKE', 'VEH_MODEL', 'VEH_MANUF_YEAR', 'VEHICLE_WIFI_CAPABLE', 'ONSTAR_INSTALL_FLG',\n",
    "    \n",
    "    # Basic mobile app features - these are available to Basic customers\n",
    "    'LAST_MONTH_SUM_DAY_GETVEHICLEDATA_CNT',      # Check vehicle status\n",
    "    'LAST_MONTH_SUM_DAY_GETVEHICLELOCATION_CNT',  # See vehicle location\n",
    "    'LAST_MONTH_SUM_DAY_LOCKDOORS_CNT',           # Lock doors\n",
    "    'LAST_MONTH_SUM_DAY_UNLOCKDOORS_CNT',         # Unlock doors\n",
    "    'LAST_MONTH_SUM_DAY_REMOTESTART_CNT',         # Remote start\n",
    "    'LAST_MONTH_SUM_DAY_REMOTESTOP_CNT',          # Remote stop\n",
    "    'LAST_MONTH_SUM_DAY_LOCKTRUNK_CNT',           # Lock trunk\n",
    "    'LAST_MONTH_SUM_DAY_UNLOCKTRUNK_CNT',         # Unlock trunk\n",
    "    \n",
    "    # OnStar Button presses - basic users can press the blue button\n",
    "    'LAST_MONTH_SUM_DAY_BLUEBUTTONPRESS_CNT',\n",
    "    \n",
    "    # Subscription history features\n",
    "    'NUM_PREV_PAID_CORE_PLANS',\n",
    "    'NUM_PREV_PAID_NONCORE_PLANS',\n",
    "    'NUM_PREV_COMP_CORE_PLANS',\n",
    "    'NUM_PREV_COMP_NONCORE_PLANS',\n",
    "    'NUM_PREV_DRPO_PLANS',\n",
    "    'TOTAL_DAYS_WITH_ACTIVE_SUBSCRIPTION',\n",
    "    'TOTAL_DAYS_WITHOUT_ACTIVE_SUBSCRIPTION',\n",
    "    \n",
    "    # Include 3-month and 6-month app usage for trend analysis\n",
    "    'LAST_THREE_MONTHS_SUM_DAY_GETVEHICLEDATA_CNT',\n",
    "    'LAST_THREE_MONTHS_SUM_DAY_GETVEHICLELOCATION_CNT',\n",
    "    'LAST_THREE_MONTHS_SUM_DAY_LOCKDOORS_CNT',\n",
    "    'LAST_THREE_MONTHS_SUM_DAY_UNLOCKDOORS_CNT',\n",
    "    'LAST_THREE_MONTHS_SUM_DAY_REMOTESTART_CNT',\n",
    "    'LAST_THREE_MONTHS_SUM_DAY_REMOTESTOP_CNT',\n",
    "    'LAST_THREE_MONTHS_SUM_DAY_LOCKTRUNK_CNT',\n",
    "    'LAST_THREE_MONTHS_SUM_DAY_UNLOCKTRUNK_CNT',\n",
    "    'LAST_THREE_MONTHS_SUM_DAY_BLUEBUTTONPRESS_CNT',\n",
    "    \n",
    "    'LAST_SIX_MONTHS_SUM_DAY_GETVEHICLEDATA_CNT',\n",
    "    'LAST_SIX_MONTHS_SUM_DAY_GETVEHICLELOCATION_CNT',\n",
    "    'LAST_SIX_MONTHS_SUM_DAY_LOCKDOORS_CNT',\n",
    "    'LAST_SIX_MONTHS_SUM_DAY_UNLOCKDOORS_CNT',\n",
    "    'LAST_SIX_MONTHS_SUM_DAY_REMOTESTART_CNT',\n",
    "    'LAST_SIX_MONTHS_SUM_DAY_REMOTESTOP_CNT',\n",
    "    'LAST_SIX_MONTHS_SUM_DAY_LOCKTRUNK_CNT',\n",
    "    'LAST_SIX_MONTHS_SUM_DAY_UNLOCKTRUNK_CNT',\n",
    "    'LAST_SIX_MONTHS_SUM_DAY_BLUEBUTTONPRESS_CNT',\n",
    "]\n",
    "\n",
    "# Select important demographic features that may influence conversion\n",
    "demographic_cols = [\n",
    "    # Income and financial status\n",
    "    'ax_est_hh_income_prmr_plus_cd',\n",
    "    'ax_estimate_hh_income_prmr_cd',\n",
    "    'ax_home_market_value_prmr_cd',\n",
    "    'ax_net_worth_prmr_cd',\n",
    "    'ax_infobase_affordability_us',\n",
    "    'ax_econ_stblty_ind_financial',\n",
    "    \n",
    "    # Vehicle ownership and preferences\n",
    "    'ax_nbr_of_veh_owned_prmr',\n",
    "    'ax_veh_new_carbuyr_prmr_flg',\n",
    "    'ax_veh_lifestyle_ind_prmr_cd',\n",
    "    'ax_auto_enthusiast_ind',\n",
    "    'ax_auto_work_flg',\n",
    "    'ax_have_auto_loan',\n",
    "    'ax_prsnl_joint_auto_loan',\n",
    "    \n",
    "    # Household composition\n",
    "    'ax_age_2yr_incr_indiv1_plus_cd',\n",
    "    'ax_nbr_of_childrn_in_hh_prmr_plus',\n",
    "    'ax_num_adults_hh_prmr_plus',\n",
    "    'ax_household_size',\n",
    "    'ax_home_owner_renter_prmr_flg',\n",
    "    'ax_marital_status_in_hh',\n",
    "    'ax_presence_of_children_flg',\n",
    "    \n",
    "    # Technology adoption\n",
    "    'ax_attd_bhvr_prop_tech_adpt',\n",
    "    'ax_technology_flg',\n",
    "    \n",
    "    # Lifestyle indicators\n",
    "    'ax_entering_adulthood_flg',\n",
    "    'ax_broader_living_flg',\n",
    "    'ax_professional_living_flg',\n",
    "    'ax_common_living_flg',\n",
    "    'ax_sporty_living_flg',\n",
    "    'ax_sports_grouping_flg',\n",
    "    'ax_outdoors_grouping_flg',\n",
    "    'ax_spectr_auto_mocycl_rcng_flg',\n",
    "    'ax_nascar_flg',\n",
    "    'ax_environmental_issues_flg',\n",
    "    \n",
    "    # Luxury affinity\n",
    "    'ax_affnty_new_cadillac_fin',\n",
    "    'ax_prchs_new_lux_sedan_fin',\n",
    "    'ax_prchs_new_lux_suv_fin',\n",
    "    'ax_prchs_new_mid_lux_car_fin',\n",
    "    \n",
    "    # Credit behavior\n",
    "    'ax_heavy_transactors_ind',\n",
    "    'ax_carry_fwd_a_bal_cc',\n",
    "    'ax_price_snstv_pny_pnchr'\n",
    "]\n",
    "\n",
    "# Now construct the model features dataframe\n",
    "print(\"Creating model features dataset...\")\n",
    "\n",
    "# Filter out vehicles that are not eligible for Basic plan (model year < 2015)\n",
    "df_eligible = df_features.filter(col('VEH_MANUF_YEAR') >= 2015)\n",
    "\n",
    "# Select the columns we want for our model\n",
    "model_cols = id_cols + target_col + basic_features\n",
    "df_model = df_eligible.select(model_cols)\n",
    "\n",
    "# Join logic - create a pipeline to enrich our data with demographic information\n",
    "\n",
    "# Step 1: Join with ID crosswalk to get individual IDs\n",
    "print(\"Joining with ID crosswalk table...\")\n",
    "try:\n",
    "    # This will work if service data with INDIV_ID has already been integrated\n",
    "    if 'INDIV_ID' in df_model.columns:\n",
    "        df_model = df_model.join(\n",
    "            df_crosswalk.select('indiv_id', 'entity_realID_person_id', 'household_id', 'amperity_id'),\n",
    "            df_model['INDIV_ID'] == df_crosswalk['indiv_id'],\n",
    "            how='left'\n",
    "        )\n",
    "    else:\n",
    "        # Otherwise, try to join through account information first\n",
    "        print(\"INDIV_ID not available directly, using ID crosswalk to establish connections...\")\n",
    "        # For demonstration, assuming ACCOUNT_ID can be linked\n",
    "        # In actual implementation, you might need a different join strategy\n",
    "        \n",
    "        # Try to load service lane features to get INDIV_ID\n",
    "        try:\n",
    "            service_df = spark.table(\"work.marsci.service_lane_features\")\n",
    "            print(\"Successfully loaded service_lane_features table\")\n",
    "            df_model = df_model.join(\n",
    "                service_df.select('VIN_ID', 'FEATURE_DATE', 'INDIV_ID', 'HOUSEHOLD_ID'),\n",
    "                on=['VIN_ID', 'FEATURE_DATE'],\n",
    "                how='left'\n",
    "            )\n",
    "            # Then join with crosswalk\n",
    "            df_model = df_model.join(\n",
    "                df_crosswalk.select('indiv_id', 'entity_realID_person_id', 'household_id', 'amperity_id'),\n",
    "                df_model['INDIV_ID'] == df_crosswalk['indiv_id'],\n",
    "                how='left'\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Service lane features table not available: {str(e)}\")\n",
    "            print(\"Using a direct join strategy with the crosswalk table\")\n",
    "            # This is a fallback method if service_lane_features isn't available\n",
    "            df_model = df_model\n",
    "except Exception as e:\n",
    "    print(f\"Error during crosswalk join: {str(e)}\")\n",
    "\n",
    "# Step 2: Join with demographics using appropriate IDs\n",
    "print(\"Joining with demographic data...\")\n",
    "# Select the most recent demographic data for each individual\n",
    "df_demographics_latest = df_demographics.withColumn(\n",
    "    \"row_num\",\n",
    "    F.row_number().over(\n",
    "        F.window.partitionBy(\"indiv_id\").orderBy(F.desc(\"time_stamp\"))\n",
    "    )\n",
    ").filter(col(\"row_num\") == 1).drop(\"row_num\")\n",
    "\n",
    "# Now join with this demographics data\n",
    "try:\n",
    "    # Attempt to join on indiv_id if available\n",
    "    if 'indiv_id' in df_model.columns:\n",
    "        print(\"Joining demographics on indiv_id...\")\n",
    "        df_model = df_model.join(\n",
    "            df_demographics_latest.select(['indiv_id'] + demographic_cols),\n",
    "            df_model['indiv_id'] == df_demographics_latest['indiv_id'],\n",
    "            how='left'\n",
    "        )\n",
    "    # If that fails, try to join on amperity_id if available\n",
    "    elif 'amperity_id' in df_model.columns:\n",
    "        print(\"Joining demographics on amperity_id...\")\n",
    "        df_model = df_model.join(\n",
    "            df_demographics_latest.select(['amperity_id'] + demographic_cols),\n",
    "            df_model['amperity_id'] == df_demographics_latest['amperity_id'],\n",
    "            how='left'\n",
    "        )\n",
    "    else:\n",
    "        print(\"No common ID found for demographic join\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during demographic join: {str(e)}\")\n",
    "\n",
    "# Try to load loyalty data if available\n",
    "try:\n",
    "    loyalty_df = spark.table(\"work.marsci.loyalty_features\")\n",
    "    print(\"Successfully loaded loyalty_features table\")\n",
    "    df_model = df_model.join(\n",
    "        loyalty_df.select(['ACCOUNT_NUMBER', 'FEATURE_DATE', \n",
    "                          'MIN_LOYALTY_TIER', 'MAX_LOYALTY_TIER', \n",
    "                          'LOY_REDEEMED_FLAG', 'TOTAL_LOY_NUM_TRANSACTIONS']),\n",
    "        on=['ACCOUNT_NUMBER', 'FEATURE_DATE'],\n",
    "        how='left'\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Loyalty features table not available: {str(e)}\")\n",
    "\n",
    "# Add derived features that might be useful predictors\n",
    "print(\"Creating derived features...\")\n",
    "\n",
    "# Frequency of app usage (as a rate per month)\n",
    "df_model = df_model.withColumn(\n",
    "    'APP_USAGE_RATE_MONTHLY',\n",
    "    (\n",
    "        coalesce(col('LAST_MONTH_SUM_DAY_GETVEHICLEDATA_CNT'), lit(0)).cast('double') +\n",
    "        coalesce(col('LAST_MONTH_SUM_DAY_GETVEHICLELOCATION_CNT'), lit(0)).cast('double') +\n",
    "        coalesce(col('LAST_MONTH_SUM_DAY_LOCKDOORS_CNT'), lit(0)).cast('double') +\n",
    "        coalesce(col('LAST_MONTH_SUM_DAY_UNLOCKDOORS_CNT'), lit(0)).cast('double') +\n",
    "        coalesce(col('LAST_MONTH_SUM_DAY_REMOTESTART_CNT'), lit(0)).cast('double') +\n",
    "        coalesce(col('LAST_MONTH_SUM_DAY_REMOTESTOP_CNT'), lit(0)).cast('double') +\n",
    "        coalesce(col('LAST_MONTH_SUM_DAY_LOCKTRUNK_CNT'), lit(0)).cast('double') +\n",
    "        coalesce(col('LAST_MONTH_SUM_DAY_UNLOCKTRUNK_CNT'), lit(0)).cast('double')\n",
    "    )\n",
    ")\n",
    "\n",
    "# Usage trend (increasing or decreasing)\n",
    "df_model = df_model.withColumn(\n",
    "    'APP_USAGE_TREND',\n",
    "    (\n",
    "        (\n",
    "            coalesce(col('LAST_MONTH_SUM_DAY_GETVEHICLEDATA_CNT'), lit(0)).cast('double') +\n",
    "            coalesce(col('LAST_MONTH_SUM_DAY_GETVEHICLELOCATION_CNT'), lit(0)).cast('double') +\n",
    "            coalesce(col('LAST_MONTH_SUM_DAY_LOCKDOORS_CNT'), lit(0)).cast('double') +\n",
    "            coalesce(col('LAST_MONTH_SUM_DAY_UNLOCKDOORS_CNT'), lit(0)).cast('double') +\n",
    "            coalesce(col('LAST_MONTH_SUM_DAY_REMOTESTART_CNT'), lit(0)).cast('double') +\n",
    "            coalesce(col('LAST_MONTH_SUM_DAY_REMOTESTOP_CNT'), lit(0)).cast('double') +\n",
    "            coalesce(col('LAST_MONTH_SUM_DAY_LOCKTRUNK_CNT'), lit(0)).cast('double') +\n",
    "            coalesce(col('LAST_MONTH_SUM_DAY_UNLOCKTRUNK_CNT'), lit(0)).cast('double')\n",
    "        ) -\n",
    "        (\n",
    "            (\n",
    "                coalesce(col('LAST_THREE_MONTHS_SUM_DAY_GETVEHICLEDATA_CNT'), lit(0)).cast('double') +\n",
    "                coalesce(col('LAST_THREE_MONTHS_SUM_DAY_GETVEHICLELOCATION_CNT'), lit(0)).cast('double') +\n",
    "                coalesce(col('LAST_THREE_MONTHS_SUM_DAY_LOCKDOORS_CNT'), lit(0)).cast('double') +\n",
    "                coalesce(col('LAST_THREE_MONTHS_SUM_DAY_UNLOCKDOORS_CNT'), lit(0)).cast('double') +\n",
    "                coalesce(col('LAST_THREE_MONTHS_SUM_DAY_REMOTESTART_CNT'), lit(0)).cast('double') +\n",
    "                coalesce(col('LAST_THREE_MONTHS_SUM_DAY_REMOTESTOP_CNT'), lit(0)).cast('double') +\n",
    "                coalesce(col('LAST_THREE_MONTHS_SUM_DAY_LOCKTRUNK_CNT'), lit(0)).cast('double') +\n",
    "                coalesce(col('LAST_THREE_MONTHS_SUM_DAY_UNLOCKTRUNK_CNT'), lit(0)).cast('double')\n",
    "            ) / 3\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "# Flag for whether they use the app consistently\n",
    "df_model = df_model.withColumn(\n",
    "    'IS_ACTIVE_APP_USER',\n",
    "    when(col('APP_USAGE_RATE_MONTHLY') > 5, 1).otherwise(0)\n",
    ")\n",
    "\n",
    "# Flag for whether they press the blue button\n",
    "df_model = df_model.withColumn(\n",
    "    'IS_BLUEBUTTON_USER',\n",
    "    when(coalesce(col('LAST_THREE_MONTHS_SUM_DAY_BLUEBUTTONPRESS_CNT'), lit(0)) > 0, 1).otherwise(0)\n",
    ")\n",
    "\n",
    "# Add features derived from demographic data\n",
    "df_model = df_model.withColumn(\n",
    "    'IS_HIGH_INCOME',\n",
    "    when(col('ax_est_hh_income_prmr_plus_cd').isin(['I', 'J', 'K', 'L', 'M', 'N', 'P']), 1).otherwise(0)\n",
    ")\n",
    "\n",
    "df_model = df_model.withColumn(\n",
    "    'IS_LUXURY_VEHICLE_BUYER',\n",
    "    when(\n",
    "        (coalesce(col('ax_affnty_new_cadillac_fin'), lit(100)) < 30) |\n",
    "        (coalesce(col('ax_prchs_new_lux_sedan_fin'), lit(100)) < 30) |\n",
    "        (coalesce(col('ax_prchs_new_lux_suv_fin'), lit(100)) < 30) |\n",
    "        (coalesce(col('ax_prchs_new_mid_lux_car_fin'), lit(100)) < 30),\n",
    "        1\n",
    "    ).otherwise(0)\n",
    ")\n",
    "\n",
    "df_model = df_model.withColumn(\n",
    "    'IS_AUTO_ENTHUSIAST',\n",
    "    when(\n",
    "        (coalesce(col('ax_auto_enthusiast_ind'), lit(0)) == 1) |\n",
    "        (coalesce(col('ax_auto_work_flg'), lit(0)) == 1) |\n",
    "        (coalesce(col('ax_spectr_auto_mocycl_rcng_flg'), lit(0)) == 1) |\n",
    "        (coalesce(col('ax_nascar_flg'), lit(0)) == 1),\n",
    "        1\n",
    "    ).otherwise(0)\n",
    ")\n",
    "\n",
    "df_model = df_model.withColumn(\n",
    "    'IS_TECH_ADOPTER',\n",
    "    when(\n",
    "        (coalesce(col('ax_attd_bhvr_prop_tech_adpt'), lit(100)) < 30) |\n",
    "        (coalesce(col('ax_technology_flg'), lit(0)) == 1),\n",
    "        1\n",
    "    ).otherwise(0)\n",
    ")\n",
    "\n",
    "df_model = df_model.withColumn(\n",
    "    'HAS_FAMILY',\n",
    "    when(\n",
    "        (coalesce(col('ax_nbr_of_childrn_in_hh_prmr_plus'), lit(0)) > 0) |\n",
    "        (coalesce(col('ax_presence_of_children_flg'), lit(0)) == 1),\n",
    "        1\n",
    "    ).otherwise(0)\n",
    ")\n",
    "\n",
    "# Calculate missing values and display column statistics\n",
    "print(\"Checking missing values...\")\n",
    "total_records = df_model.count()\n",
    "null_counts = df_model.select([\n",
    "    count(when(col(c).isNull(), c)).alias(c) for c in df_model.columns\n",
    "])\n",
    "\n",
    "# Print column stats\n",
    "print(f\"Final dataset has {total_records} rows and {len(df_model.columns)} columns\")\n",
    "null_counts_pd = null_counts.toPandas().T\n",
    "null_counts_pd.columns = ['missing_count']\n",
    "null_counts_pd['missing_percentage'] = null_counts_pd['missing_count'] / total_records * 100\n",
    "print(null_counts_pd.sort_values('missing_percentage', ascending=False).head(10))\n",
    "\n",
    "# Calculate class distribution\n",
    "class_counts = df_model.groupBy(\"pay_flg\").count().toPandas()\n",
    "print(\"\\nTarget class distribution:\")\n",
    "print(class_counts)\n",
    "\n",
    "# Create the table\n",
    "print(f\"\\nCreating table: {TARGET_MODEL_TABLE}\")\n",
    "df_model.write.mode(\"overwrite\").format(\"delta\").saveAsTable(TARGET_MODEL_TABLE)\n",
    "\n",
    "print(f\"Successfully created {TARGET_MODEL_TABLE}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "DS Clustering - YG v1",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
